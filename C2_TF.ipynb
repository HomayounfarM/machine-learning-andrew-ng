{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "UZaUyq91Kn4u",
        "KVIsV85L0wUj",
        "NLWZPcD-16rl",
        "Oz_KHhZ-DetU",
        "uFbIY43n4HUD",
        "iJ1KOHMd-gDO",
        "TPrFTRGcCXk3",
        "h-0aj0uuk7TV"
      ],
      "authorship_tag": "ABX9TyP0CqFB/lockW6ivJzyMC70",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HomayounfarM/machine-learning-andrew-ng/blob/main/C2_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W1_Lab01_TF"
      ],
      "metadata": {
        "id": "mq53ARLRHHfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Tensorflow and Keras\n",
        "Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by FranÃ§ois Chollet that creates a simple, layer-centric interface to Tensorflow.\n",
        "The following example shows to use tersorflow and Keras to create a layer. The first layer includes three cells (units). The second layer infludes one cell.\n",
        "\n",
        "In the following example, we have two layers: the first layer includes three unit (cell) and the second layer includes one unit (cell).\n",
        "Our x can be an array with any length. then, w will be an array with the same length and the inner production of these two array plus a 'b' wil be assigned to the first cell of that layer."
      ],
      "metadata": {
        "id": "Eg9FXGzsBqLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7eVKTN4__8Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "#from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
        "#from tensorflow.keras.activations import sigmoid\n",
        "#import matplotlib.colors as colors\n",
        "\n",
        "x = np.array([[200, 1, 4, 6, 7, 8]], dtype = float)\n",
        "\n",
        "print(type(x))\n",
        "print(x.shape)\n",
        "print(x.dtype)\n",
        "\n",
        "layer_1 = tf.keras.layers.Dense(units = 3, activation = 'sigmoid')\n",
        "a1 = layer_1(x)\n",
        "print(a1.numpy())\n",
        "print(a1)\n",
        "\n",
        "layer_2 = Dense(units = 1, activation = 'sigmoid')\n",
        "a2 = layer_2(a1)\n",
        "\n",
        "print(a2.numpy())    # use .numpy() to convert the tensorflow to an np.array\n",
        "\n",
        "if a2 > 0.5:\n",
        "    yhat = 1\n",
        "else:\n",
        "    yhat = 0\n",
        "\n",
        "print(yhat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "X_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\n",
        "Y_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\n",
        "\n",
        "pos = Y_train == 1\n",
        "neg = Y_train == 0\n",
        "X_train[pos]\n",
        "\n",
        "pos = Y_train == 1\n",
        "neg = Y_train == 0\n",
        "\n",
        "\n",
        "dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC')\n",
        "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
        "ax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "ax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',\n",
        "              edgecolors=dlc[\"dlmagenta\"],lw=3)\n",
        "\n",
        "ax.set_ylim(-0.08,1.1)\n",
        "ax.set_ylabel('y', fontsize=12)\n",
        "ax.set_xlabel('x', fontsize=12)\n",
        "ax.set_title('one variable plot')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hDy7DrsbCIUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Neuron\n",
        "\n",
        "We can implement a 'logistic neuron' by adding a sigmoid activation. The function of the neuron is then described by (1).  \n",
        "\n",
        "$$ f_{\\mathbf{w},b}(x^{(i)}) = g(\\mathbf{w}x^{(i)} + b) \\tag{1}$$\n",
        "where $$g(x) = sigmoid(x)$$\n",
        "\n",
        "Let's set $w$ and $b$ to some known values and check the model.\n",
        "\n",
        "This section will create a Tensorflow Model that contains our logistic layer to demonstrate an alternate method of creating models. Tensorflow is most often used to create multi-layer models. The [Sequential](https://keras.io/guides/sequential_model/) model is a convenient means of constructing these models.\n",
        "We can implement a 'logistic'\n",
        "\n"
      ],
      "metadata": {
        "id": "8eajVEDoDH6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Rvy3-4v_Dhdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model.summary()` shows the layers and number of parameters in the model. There is only one layer in this model and that layer has only one unit. The unit has two parameters, $w$ and $b$.\n",
        "The first term in the 'Sequential function' determine the number of cells (units) in a layer. The second term, 'input_dim', is to determine the dimention of input to this layer.  "
      ],
      "metadata": {
        "id": "FS_AQ-2yD863"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "KvHWvdOsECw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W and b are randomly assigned. We need to train the model to find the best values for  and b"
      ],
      "metadata": {
        "id": "Yboq3i8WwBmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_layer = model.get_layer('L1')\n",
        "w,b = logistic_layer.get_weights()\n",
        "print(w,b)\n",
        "print(w.shape,b.shape)"
      ],
      "metadata": {
        "id": "EnXcZg4wED5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set the weight and bias to some known values."
      ],
      "metadata": {
        "id": "-7LVRJoREM2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I1uR3XLTEM1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_w = np.array([[2]])\n",
        "set_b = np.array([-4.5])\n",
        "# set_weights takes a list of numpy arrays\n",
        "logistic_layer.set_weights([set_w, set_b])\n",
        "print(logistic_layer.get_weights())"
      ],
      "metadata": {
        "id": "wME2GEXFEL92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare equation (2) to the layer output."
      ],
      "metadata": {
        "id": "GBh9fB-tEVYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = model.predict(X_train[0].reshape(1,1))\n",
        "print(a1)\n"
      ],
      "metadata": {
        "id": "oWU6UugGEWHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The followings are the functions used above"
      ],
      "metadata": {
        "id": "raaKGXU8FPOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoidnp(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    z : array_like\n",
        "        A scalar or numpy array of any size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "     g : array_like\n",
        "         sigmoid(z)\n",
        "    \"\"\"\n",
        "    z = np.clip( z, -500, 500 )           # protect against overflow\n",
        "    g = 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "VEqu_euWEsM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_logistic(X_train, Y_train, model, set_w, set_b, pos, neg):\n",
        "    fig,ax = plt.subplots(1,2,figsize=(16,4))\n",
        "\n",
        "    layerf= lambda x : model.predict(x)\n",
        "    plt_prob_1d(ax[0], layerf)\n",
        "\n",
        "    ax[0].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "    ax[0].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',\n",
        "                  edgecolors=dlc[\"dlblue\"],lw=3)\n",
        "\n",
        "    ax[0].set_ylim(-0.08,1.1)\n",
        "    ax[0].set_xlim(-0.5,5.5)\n",
        "    ax[0].set_ylabel('y', fontsize=16)\n",
        "    ax[0].set_xlabel('x', fontsize=16)\n",
        "    ax[0].set_title('Tensorflow Model', fontsize=20)\n",
        "    ax[0].legend(fontsize=16)\n",
        "\n",
        "    layerf= lambda x : sigmoidnp(np.dot(set_w,x.reshape(1,1)) + set_b)\n",
        "    plt_prob_1d(ax[1], layerf)\n",
        "\n",
        "    ax[1].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "    ax[1].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',\n",
        "                  edgecolors=dlc[\"dlblue\"],lw=3)\n",
        "\n",
        "    ax[1].set_ylim(-0.08,1.1)\n",
        "    ax[1].set_xlim(-0.5,5.5)\n",
        "    ax[1].set_ylabel('y', fontsize=16)\n",
        "    ax[1].set_xlabel('x', fontsize=16)\n",
        "    ax[1].set_title('Numpy Model', fontsize=20)\n",
        "    ax[1].legend(fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BuSuJLwYEyoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_prob_1d(ax,fwb):\n",
        "    \"\"\" plots a decision boundary but include shading to indicate the probability \"\"\"\n",
        "    #setup useful ranges and common linspaces\n",
        "    x_space  = np.linspace(0, 5 , 50)\n",
        "    y_space  = np.linspace(0, 1 , 50)\n",
        "\n",
        "    # get probability for x range, extend to y\n",
        "    z = np.zeros((len(x_space),len(y_space)))\n",
        "    for i in range(len(x_space)):\n",
        "        x = np.array([[x_space[i]]])\n",
        "        z[:,i] = fwb(x)\n",
        "\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "    new_cmap = truncate_colormap(cmap, 0.0, 0.5)\n",
        "    pcm = ax.pcolormesh(x_space, y_space, z,\n",
        "                   norm=cm.colors.Normalize(vmin=0, vmax=1),\n",
        "                   cmap=new_cmap, shading='nearest', alpha = 0.9)\n",
        "    ax.figure.colorbar(pcm, ax=ax)\n",
        "\n",
        "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
        "    \"\"\" truncates color map \"\"\"\n",
        "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
        "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
        "        cmap(np.linspace(minval, maxval, n)))\n",
        "    return new_cmap"
      ],
      "metadata": {
        "id": "e4Cxz4odE1fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W1_Lab02_TF"
      ],
      "metadata": {
        "id": "pupmi3GAHWJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataSet"
      ],
      "metadata": {
        "id": "UZaUyq91Kn4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = load_coffee_data();\n",
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "id": "1UDAe7B8HkrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize Data\n",
        "Fitting the weights to the data (back-propagation, covered in next week's lectures) will proceed more quickly if the data is normalized. This is the same procedure you used in Course 1 where features in the data are each normalized to have a similar range.\n",
        "The procedure below uses a Keras [normalization layer](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/). It has the following steps:\n",
        "- create a \"Normalization Layer\". Note, as applied here, this is not a layer in your model.\n",
        "- 'adapt' the data. This learns the mean and variance of the data set and saves the values internally.\n",
        "- normalize the data.  \n",
        "It is important to apply normalization to any future data that utilizes the learned model."
      ],
      "metadata": {
        "id": "FF3ACvgLMpX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\")\n",
        "print(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\")\n",
        "norm_l = tf.keras.layers.Normalization(axis=-1)    # create a \"Normalization Layer\"\n",
        "norm_l.adapt(X)  # learns mean, variance\n",
        "Xn = norm_l(X)    # normalize the data.\n",
        "print(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\")\n",
        "print(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\")"
      ],
      "metadata": {
        "id": "Vrn_POzZL61V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tile/copy our data to increase the training set size and reduce the number of training epochs."
      ],
      "metadata": {
        "id": "sDvnh2OsM93l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xt = np.tile(Xn,(1000,1))\n",
        "Yt= np.tile(Y,(1000,1))\n",
        "print(Xt.shape, Yt.shape)"
      ],
      "metadata": {
        "id": "oOEQ1ke7NCg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Model"
      ],
      "metadata": {
        "id": "hqUMM7S-PpBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build the \"Coffee Roasting Network\". There are two layers with sigmoid activations as shown below:"
      ],
      "metadata": {
        "id": "l7QYMVyKPvjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(2,)),\n",
        "        Dense(3, activation='sigmoid', name = 'layer1'),\n",
        "        Dense(1, activation='sigmoid', name = 'layer2')\n",
        "     ]\n",
        ")"
      ],
      "metadata": {
        "id": "fvo2onrTP0sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer has six w and three b. The second layer has three w and one b.\n"
      ],
      "metadata": {
        "id": "74x2AdOCQcR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Note 1:** The `tf.keras.Input(shape=(2,)),` specifies the expected shape of the input. This allows Tensorflow to size the weights and bias parameters at this point.  This is useful when exploring Tensorflow models. This statement can be omitted in practice and Tensorflow will size the network parameters when the input data is specified in the `model.fit` statement.  \n",
        ">**Note 2:** Including the sigmoid activation in the final layer is not considered best practice. It would instead be accounted for in the loss which improves numerical stability. This will be described in more detail in a later lab.\n",
        "\n",
        "The `model.summary()` provides a description of the network:"
      ],
      "metadata": {
        "id": "Sc_hdtFKRAtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Cjs9GECeQDVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine the weights and biases Tensorflow has instantiated.  The weights $W$ should be of size (number of features in input, number of units in the layer) while the bias $b$ size should match the number of units in the layer:\n",
        "- In the first layer with 3 units, we expect W to have a size of (2,3) and $b$ should have 3 elements.\n",
        "- In the second layer with 1 unit, we expect W to have a size of (3,1) and $b$ should have 1 element."
      ],
      "metadata": {
        "id": "kFQr__rfS1ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
        "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)"
      ],
      "metadata": {
        "id": "p8OftCbsS8oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following statements will be described in detail later. For now:\n",
        "- The `model.compile` statement defines a loss function and specifies a compile optimization.\n",
        "- The `model.fit` statement runs gradient descent and fits the weights to the data."
      ],
      "metadata": {
        "id": "0z8-t2emTDui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    Xt,Yt,\n",
        "    epochs=10,\n",
        ")"
      ],
      "metadata": {
        "id": "gNa8v4EdS8Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Updated Weights\n",
        "After fitting, the weights have been updated:"
      ],
      "metadata": {
        "id": "zBsAmqw2VMdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
        "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
      ],
      "metadata": {
        "id": "0jFNEyHYVPFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by creating input data. The model is expecting one or more examples where examples are in the rows of matrix. In this case, we have two features so the matrix will be (m,2) where m is the number of examples.\n",
        "Recall, we have normalized the input features so we must normalize our test data as well.   \n",
        "To make a prediction, you apply the `predict` method."
      ],
      "metadata": {
        "id": "pdICQ-bZXhmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array([\n",
        "    [200,13.9],  # postive example\n",
        "    [200,17]])   # negative example\n",
        "X_testn = norm_l(X_test)\n",
        "predictions = model.predict(X_testn)\n",
        "print(\"predictions = \\n\", predictions)"
      ],
      "metadata": {
        "id": "k1ao7C3CXknh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Epochs and batches\n",
        "In the `compile` statement above, the number of `epochs` was set to 10. This specifies that the entire data set should be applied during training 10 times.  During training, you see output describing the progress of training that looks like this:\n",
        "```\n",
        "Epoch 1/10\n",
        "6250/6250 [==============================] - 6s 910us/step - loss: 0.1782\n",
        "```\n",
        "The first line, `Epoch 1/10`, describes which epoch the model is currently running. For efficiency, the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line `6250/6250 [====` is describing which batch has been executed."
      ],
      "metadata": {
        "id": "OtR3H_CEX61v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert the probabilities to a decision, we apply a threshold:"
      ],
      "metadata": {
        "id": "atV6VDDWYAgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = np.zeros_like(predictions)\n",
        "for i in range(len(predictions)):\n",
        "    if predictions[i] >= 0.5:\n",
        "        yhat[i] = 1\n",
        "    else:\n",
        "        yhat[i] = 0\n",
        "print(f\"decisions = \\n{yhat}\")"
      ],
      "metadata": {
        "id": "fTtz2kPRYHin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be accomplished more succinctly:"
      ],
      "metadata": {
        "id": "IfavVteuYK9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = (predictions >= 0.5).astype(int)\n",
        "print(f\"decisions = \\n{yhat}\")"
      ],
      "metadata": {
        "id": "z2kbGpBkYQ1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###These are the functions we use above:"
      ],
      "metadata": {
        "id": "RbqgWk6mI76Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_coffee_data():\n",
        "    \"\"\" Creates a coffee roasting data set.\n",
        "        roasting duration: 12-15 minutes is best\n",
        "        temperature range: 175-260C is best\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(2)\n",
        "    X = rng.random(400).reshape(-1,2)\n",
        "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
        "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
        "    Y = np.zeros(len(X))\n",
        "\n",
        "    i=0\n",
        "    for t,d in X:\n",
        "        y = -3/(260-175)*t + 21\n",
        "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
        "            Y[i] = 1\n",
        "        else:\n",
        "            Y[i] = 0\n",
        "        i += 1\n",
        "\n",
        "    return (X, Y.reshape(-1,1))"
      ],
      "metadata": {
        "id": "Vi_j45jrI7iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Y0hpWLwI6iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W1_Lab03_TF"
      ],
      "metadata": {
        "id": "iIyAtOK90QDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lab: Neural Networks for Handwritten Digit Recognition, BinaryÂ¶\n",
        "In this exercise, you will use a neural network to recognize the hand-written digits zero and one."
      ],
      "metadata": {
        "id": "KVIsV85L0wUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from autils import *\n",
        "#%matplotlib inline\n",
        "\n",
        "#import logging\n",
        "#logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "#tf.autograph.set_verbosity(0)"
      ],
      "metadata": {
        "id": "CDd2Pblm0iEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "UD7IDFqZ_4Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem Statement\n",
        "In this exercise, you will use a neural network to recognize two handwritten digits, zero and one. This is a binary classification task. Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks. You will extend this network to recognize all 10 digits (0-9) in a future assignment.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "You will start by loading the dataset for this task.\n",
        "- The `load_data()` function shown below loads the data into variables `X` and `y`\n",
        "\n",
        "\n",
        "- The data set contains 1000 training examples of handwritten digits $^1$, here limited to zero and one.  \n",
        "\n",
        "    - Each training example is a 20-pixel x 20-pixel grayscale image of the digit.\n",
        "        - Each pixel is represented by a floating-point number indicating the grayscale intensity at that location.\n",
        "        - The 20 by 20 grid of pixels is âunrolledâ into a 400-dimensional vector.\n",
        "        - Each training example becomes a single row in our data matrix `X`.\n",
        "        - This gives us a 1000 x 400 matrix `X` where every row is a training example of a handwritten digit image.\n",
        "\n",
        "$$X =\n",
        "\\left(\\begin{array}{cc}\n",
        "--- (x^{(1)}) --- \\\\\n",
        "--- (x^{(2)}) --- \\\\\n",
        "\\vdots \\\\\n",
        "--- (x^{(m)}) ---\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "- The second part of the training set is a 1000 x 1 dimensional vector `y` that contains labels for the training set\n",
        "    - `y = 0` if the image is of the digit `0`, `y = 1` if the image is of the digit `1`.\n",
        "\n",
        "$^1$<sub> This is a subset of the MNIST handwritten digit dataset (http://yann.lecun.com/exdb/mnist/)</sub>"
      ],
      "metadata": {
        "id": "NLWZPcD-16rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "link = 'https://drive.google.com/file/d/1iR3DzNzotfp6y4MWjr2MJJWIvaE527xT/view?usp=share_link'\n",
        "link1 = 'https://drive.google.com/file/d/1PTtDlUkJ3TatNpaSiIb2t98uIs3B4_Da/view?usp=share_link'\n",
        "\n",
        "# to get the id part of the file\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('data.csv')\n",
        "df = pd.read_csv('data.csv', index_col=None)\n",
        "df = df.drop('Unnamed: 0', axis=1)\n",
        "X = df.to_numpy()\n",
        "\n",
        "id1 = link1.split(\"/\")[-2]\n",
        "downloaded1 = drive.CreateFile({'id':id1})\n",
        "downloaded1.GetContentFile('data1.csv')\n",
        "df1 = pd.read_csv('data1.csv', index_col=None)\n",
        "df1 = df1.drop('Unnamed: 0', axis=1)\n",
        "y = df1.to_numpy()"
      ],
      "metadata": {
        "id": "Rg-Du-Ro0iBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing the Data\n",
        "\n",
        "You will begin by visualizing a subset of the training set.\n",
        "- In the cell below, the code randomly selects 64 rows from `X`, maps each row back to a 20 pixel by 20 pixel grayscale image and displays the images together.\n",
        "- The label for each image is displayed above the image"
      ],
      "metadata": {
        "id": "HwyQLc50_tS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "# You do not need to modify anything in this cell\n",
        "\n",
        "m, n = X.shape\n",
        "\n",
        "fig, axes = plt.subplots(8,8, figsize=(8,8))\n",
        "fig.tight_layout(pad=0.1)\n",
        "\n",
        "for i,ax in enumerate(axes.flat):\n",
        "    # Select random indices\n",
        "    random_index = np.random.randint(m)\n",
        "\n",
        "    # Select rows corresponding to the random indices and\n",
        "    # reshape the image\n",
        "    X_random_reshaped = X[random_index].reshape((20,20)).T\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(X_random_reshaped, cmap='Greys')\n",
        "\n",
        "    # Display the label above the image\n",
        "    ax.set_title(y[random_index,0])\n",
        "    ax.set_axis_off()"
      ],
      "metadata": {
        "id": "8g3mFpfy_C4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model representation\n",
        "\n",
        "The neural network you will use in this assignment is shown in the figure below.\n",
        "- This has three dense layers with sigmoid activations.\n",
        "    - Recall that our inputs are pixel values of digit images.\n",
        "    - Since the images are of size 20 Ã 20, this gives us 400 inputs  \n",
        "    \n",
        "![picture](https://drive.google.com/uc?export=view&id=1UfjXiLEWsYCl8fvizkflk3SXxey2y7Le)\n",
        "\n"
      ],
      "metadata": {
        "id": "Oz_KHhZ-DetU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The parameters have dimensions that are sized for a neural network with $25$ units in layer 1, $15$ units in layer 2 and $1$ output unit in layer 3.\n",
        "\n",
        "    - Recall that the dimensions of these parameters are determined as follows:\n",
        "        - If network has $s_{in}$ units in a layer and $s_{out}$ units in the next layer, then\n",
        "            - $W$ will be of dimension $s_{in} \\times s_{out}$.\n",
        "            - $b$ will a vector with $s_{out}$ elements\n",
        "  \n",
        "    - Therefore, the shapes of `W`, and `b`,  are\n",
        "        - layer1: The shape of `W1` is (400, 25) and the shape of `b1` is (25,)\n",
        "        - layer2: The shape of `W2` is (25, 15) and the shape of `b2` is: (15,)\n",
        "        - layer3: The shape of `W3` is (15, 1) and the shape of `b3` is: (1,)\n",
        ">**Note:** The bias vector `b` could be represented as a 1-D (n,) or 2-D (1,n) array. Tensorflow utilizes a 1-D representation and this lab will maintain that convention."
      ],
      "metadata": {
        "id": "UZ_GmVxuIQi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Model Implementation"
      ],
      "metadata": {
        "id": "CsFI4KmIKUXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow models are built layer by layer. A layer's input dimensions ($s_{in}$ above) are calculated for you. You specify a layer's *output dimensions* and this determines the next layer's input dimension. The input dimension of the first layer is derived from the size of the input data specified in the `model.fit` statement below.\n",
        ">**Note:** It is also possible to add an input layer that specifies the input dimension of the first layer. For example:  \n",
        "`tf.keras.Input(shape=(400,)),    #specify input shape`  \n",
        "We will include that here to illuminate some model sizing."
      ],
      "metadata": {
        "id": "0ax8vqcLKY92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(400,)),    #specify input size\n",
        "        ### START CODE HERE ###\n",
        "        tf.keras.layers.Dense(25, input_dim=400,  activation = 'sigmoid', name='L1'),\n",
        "        tf.keras.layers.Dense(15, input_dim=25,  activation = 'sigmoid', name='L2'),\n",
        "        tf.keras.layers.Dense(1, input_dim=15,  activation = 'sigmoid', name='L3')\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "    ], name = \"my_model\"\n",
        ")"
      ],
      "metadata": {
        "id": "qYzzYbkL4JhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "YIReF-Ol4JeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can examine details of the model by first extracting the layers with model.layers and then extracting the weights with layerx.get_weights() as shown below."
      ],
      "metadata": {
        "id": "O-3ikM3W_CWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[layer1, layer2, layer3] = model.layers\n",
        "#### Examine Weights shapes\n",
        "W1,b1 = layer1.get_weights()\n",
        "W2,b2 = layer2.get_weights()\n",
        "W3,b3 = layer3.get_weights()\n",
        "print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
        "print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\n",
        "print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")"
      ],
      "metadata": {
        "id": "DlxLGAcA4JbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "xx.get_weights returns a NumPy array. One can also access the weights directly in their tensor form. Note the shape of the tensors in the final layer."
      ],
      "metadata": {
        "id": "vitR-MlKArJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layers[2].weights)"
      ],
      "metadata": {
        "id": "0PMOzPkE4JYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will define a loss function and run gradient descent to fit the weights of the model to the training data. This will be explained in more detail later."
      ],
      "metadata": {
        "id": "8kkilKO5A-ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X,y,\n",
        "    epochs=20\n",
        ")"
      ],
      "metadata": {
        "id": "mk4AmlQX4JV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the coefficients after trainig thwe model\n",
        "print(model.layers[2].weights)"
      ],
      "metadata": {
        "id": "1FnhlFVvBrim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the model on an example to make a prediction, use Keras predict. The input to predict is an array so the single example is reshaped to be two dimensional."
      ],
      "metadata": {
        "id": "eDOUAIatCQ8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(X[0].reshape(1,400))  # a zero\n",
        "print(f\" predicting a zero: {prediction}\")\n",
        "prediction = model.predict(X[500].reshape(1,400))  # a one\n",
        "print(f\" predicting a one:  {prediction}\")"
      ],
      "metadata": {
        "id": "WsR0us93Brfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the model is interpreted as a probability. In the first example above, the input is a zero. The model predicts the probability that the input is a one is nearly zero. In the second example, the input is a one. The model predicts the probability that the input is a one is nearly one. As in the case of logistic regression, the probability is compared to a threshold to make a final prediction."
      ],
      "metadata": {
        "id": "967HllniCarv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if prediction >= 0.5:\n",
        "    yhat = 1\n",
        "else:\n",
        "    yhat = 0\n",
        "print(f\"prediction after threshold: {yhat}\")"
      ],
      "metadata": {
        "id": "dpWQlRB4Brcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the predictions vs the labels for a random sample of 64 digits. This takes a moment to run."
      ],
      "metadata": {
        "id": "Fyt8q6ynC5ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "# You do not need to modify anything in this cell\n",
        "\n",
        "m, n = X.shape\n",
        "\n",
        "fig, axes = plt.subplots(8,8, figsize=(8,8))\n",
        "fig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]\n",
        "\n",
        "for i,ax in enumerate(axes.flat):\n",
        "    # Select random indices\n",
        "    random_index = np.random.randint(m)\n",
        "\n",
        "    # Select rows corresponding to the random indices and\n",
        "    # reshape the image\n",
        "    X_random_reshaped = X[random_index].reshape((20,20)).T\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(X_random_reshaped, cmap='gray')\n",
        "\n",
        "    # Predict using the Neural Network\n",
        "    prediction = model.predict(X[random_index].reshape(1,400))\n",
        "    if prediction >= 0.5:\n",
        "        yhat = 1\n",
        "    else:\n",
        "        yhat = 0\n",
        "\n",
        "    # Display the label above the image\n",
        "    ax.set_title(f\"{y[random_index,0]},{yhat}\")\n",
        "    ax.set_axis_off()\n",
        "fig.suptitle(\"Label, yhat\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hWb2iF4NBrZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLaEyzJXBrWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kg97oXlpBrT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###These are the functions we use above:"
      ],
      "metadata": {
        "id": "uFbIY43n4HUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "    X = np.load(\"data/X.npy\")\n",
        "    y = np.load(\"data/y.npy\")\n",
        "    X = X[0:1000]\n",
        "    y = y[0:1000]\n",
        "    return X, y\n",
        "\n",
        "def load_weights():\n",
        "    w1 = np.load(\"data/w1.npy\")\n",
        "    b1 = np.load(\"data/b1.npy\")\n",
        "    w2 = np.load(\"data/w2.npy\")\n",
        "    b2 = np.load(\"data/b2.npy\")\n",
        "    return w1, b1, w2, b2\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1. / (1. + np.exp(-x))"
      ],
      "metadata": {
        "id": "q9JOmyu70hwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W2_Lab04_TF (Multiclass)"
      ],
      "metadata": {
        "id": "fNHHRj4GjzZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Lab - Multi-class Classification"
      ],
      "metadata": {
        "id": "uSWbifRxkzuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib widget\n",
        "from sklearn.datasets import make_blobs\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n"
      ],
      "metadata": {
        "id": "CC-DPvaukq8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare and visualize our data\n",
        "We will use Scikit-Learn make_blobs function to make a training data set with 4 categories as shown in the plot below."
      ],
      "metadata": {
        "id": "lLiEQyZIlKSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make 4-class dataset for classification\n",
        "classes = 4\n",
        "m = 100\n",
        "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
        "std = 1.0\n",
        "X_train, y_train = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=30)"
      ],
      "metadata": {
        "id": "DjTM9IyilJxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_mc(X_train,y_train,classes, centers, std=std)"
      ],
      "metadata": {
        "id": "fnrk3vs8lY8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each dot represents a training example. The axis (x0,x1) are the inputs and the color represents the class the example is associated with. Once trained, the model will be presented with a new example, (x0,x1), and will predict the class.\n",
        "\n",
        "While generated, this data set is representative of many real-world classification problems. There are several input features (x0,...,xn) and several output categories. The model is trained to use the input features to predict the correct output category."
      ],
      "metadata": {
        "id": "aVpaYnmynw9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "This lab will use a 2-layer network as shown. Unlike the binary classification networks, this network has four outputs, one for each class. Given an input example, the output with the highest value is the predicted class of the input.\n",
        "Below is an example of how to construct this network in Tensorflow. Notice the output layer uses a linear rather than a softmax activation. While it is possible to include the softmax in the output layer, it is more numerically stable if linear outputs are passed to the loss function during training. If the model is used to predict probabilities, the softmax can be applied at that point.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=12PG4uzZM4dr40M7WQ2jt1gehJhjzAccz)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F1S2mEuqTteM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(2, activation = 'relu',   name = \"L1\"),\n",
        "        Dense(4, activation = 'linear', name = \"L2\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "6qYPtSkjl131"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statements below compile and train the network. Setting from_logits=True as an argument to the loss function specifies that the output activation was linear rather than a softmax."
      ],
      "metadata": {
        "id": "MnsCBHNFVWuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(0.01),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=200\n",
        ")"
      ],
      "metadata": {
        "id": "LZCms9Xkl109"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can see that we have two X's, so, in the first layer, we have two w's and one b for each unit. We have two unit in the first layer. So, we have 6 parameters there. In the secon layer, we have 4 units, therefor, we have 12 parameters."
      ],
      "metadata": {
        "id": "sOZM6ZDhT_wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "aQD3UN-cT-nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the model trained, we can see how the model has classified the training data."
      ],
      "metadata": {
        "id": "oYq3A1tN5SSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt_cat_mc(X_train, y_train, model, classes)"
      ],
      "metadata": {
        "id": "cTYUoQdyl1yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, the decision boundaries show how the model has partitioned the input space. This very simple model has had no trouble classifying the training data. How did it accomplish this? Let's look at the network in more detail.\n",
        "\n",
        "Below, we will pull the trained weights from the model and use that to plot the function of each of the network units. Further down, there is a more detailed explanation of the results. You don't need to know these details to successfully use neural networks, but it may be helpful to gain more intuition about how the layers combine to solve a classification problem."
      ],
      "metadata": {
        "id": "qaEiS5mSVg6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gather the trained parameters from the first layer\n",
        "l1 = model.get_layer(\"L1\")\n",
        "W1,b1 = l1.get_weights()"
      ],
      "metadata": {
        "id": "HfupL5FNl1vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the function of the first layer\n",
        "plt_layer_relu(X_train, y_train.reshape(-1,), W1, b1, classes)"
      ],
      "metadata": {
        "id": "w9OWbbBEVqVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "Layer 1\n",
        "These plots show the function of Units 0 and 1 in the first layer of the network. The inputs are ( ð¥0,ð¥1\n",
        " ) on the axis. The output of the unit is represented by the color of the background. This is indicated by the color bar on the right of each graph. Notice that since these units are using a ReLu, the outputs do not necessarily fall between 0 and 1 and in this case are greater than 20 at their peaks. The contour lines in this graph show the transition point between the output,  ð[1]ð\n",
        "  being zero and non-zero. Recall the graph for a ReLu : The contour line in the graph is the inflection point in the ReLu.\n",
        "\n",
        "Unit 0 has separated classes 0 and 1 from classes 2 and 3. Points to the left of the line (classes 0 and 1) will output zero, while points to the right will output a value greater than zero.\n",
        "Unit 1 has separated classes 0 and 2 from classes 1 and 3. Points above the line (classes 0 and 2 ) will output a zero, while points below will output a value greater than zero. Let's see how this works out in the next layer!"
      ],
      "metadata": {
        "id": "6mkghqoAPLEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gather the trained parameters from the output layer\n",
        "l2 = model.get_layer(\"L2\")\n",
        "W2, b2 = l2.get_weights()\n",
        "# create the 'new features', the training examples after L1 transformation\n",
        "Xl2 = np.maximum(0, np.dot(X_train,W1) + b1)\n",
        "\n",
        "plt_output_layer_linear(Xl2, y_train.reshape(-1,), W2, b2, classes,\n",
        "                        x0_rng = (-0.25,np.amax(Xl2[:,0])), x1_rng = (-0.25,np.amax(Xl2[:,1])))"
      ],
      "metadata": {
        "id": "MoWyDpHUXyYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer 2, the output layer\n",
        "The dots in these graphs are the training examples translated by the first layer. One way to think of this is the first layer has created a new set of features for evaluation by the 2nd layer. The axes in these plots are the outputs of the previous layer  ð[1]0\n",
        "  and  ð[1]1\n",
        " . As predicted above, classes 0 and 1 (blue and green) have  ð[1]0=0\n",
        "  while classes 0 and 2 (blue and orange) have  ð[1]1=0\n",
        " .\n",
        "Once again, the intensity of the background color indicates the highest values.\n",
        "Unit 0 will produce its maximum value for values near (0,0), where class 0 (blue) has been mapped.\n",
        "Unit 1 produces its highest values in the upper left corner selecting class 1 (green).\n",
        "Unit 2 targets the lower right corner where class 2 (orange) resides.\n",
        "Unit 3 produces its highest values in the upper right selecting our final class (purple).\n",
        "\n",
        "One other aspect that is not obvious from the graphs is that the values have been coordinated between the units. It is not sufficient for a unit to produce a maximum value for the class it is selecting for, it must also be the highest value of all the units for points in that class. This is done by the implied softmax function that is part of the loss function (SparseCategoricalCrossEntropy). Unlike other activation functions, the softmax works across all the outputs."
      ],
      "metadata": {
        "id": "p55UHkkEPRWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the functions we use above:"
      ],
      "metadata": {
        "id": "VsY0G-9vl0tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import warnings\n",
        "from matplotlib import cm\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
        "import matplotlib.colors as colors\n",
        "from matplotlib import cm\n",
        "\n",
        "dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC')\n",
        "dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC'\n",
        "dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\n",
        "\n",
        "dkcolors = plt.cm.Paired((1,3,7,9,5,11))\n",
        "ltcolors = plt.cm.Paired((0,2,6,8,4,10))\n",
        "dkcolors_map = mpl.colors.ListedColormap(dkcolors)\n",
        "ltcolors_map = mpl.colors.ListedColormap(ltcolors)\n",
        "\n",
        "def plt_mc(X_train,y_train,classes, centers, std):\n",
        "    css = np.unique(y_train)\n",
        "    fig,ax = plt.subplots(1,1,figsize=(3,3))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "    plt_mc_data(ax, X_train,y_train,classes, map=dkcolors_map, legend=True, size=50, equal_xy = False)\n",
        "    ax.set_title(\"Multiclass Data\")\n",
        "    ax.set_xlabel(\"x0\")\n",
        "    ax.set_ylabel(\"x1\")\n",
        "    #for c in css:\n",
        "    #    circ = plt.Circle(centers[c], 2*std, color=dkcolors_map(c), clip_on=False, fill=False, lw=0.5)\n",
        "    #    ax.add_patch(circ)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plt_mc_data(ax, X, y, classes,  class_labels=None, map=plt.cm.Paired,\n",
        "                legend=False, size=50, m='o', equal_xy = False):\n",
        "    \"\"\" Plot multiclass data. Note, if equal_xy is True, setting ylim on the plot may not work \"\"\"\n",
        "    for i in range(classes):\n",
        "        idx = np.where(y == i)\n",
        "        col = len(idx[0])*[i]\n",
        "        label = class_labels[i] if class_labels else \"c{}\".format(i)\n",
        "        # this didn't work on coursera but did in local version\n",
        "        #ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n",
        "        #            c=col, vmin=0, vmax=map.N, cmap=map,\n",
        "        #            s=size, label=label)\n",
        "        ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n",
        "                    color=map(col), vmin=0, vmax=map.N,\n",
        "                    s=size, label=label)\n",
        "    if legend: ax.legend()\n",
        "    if equal_xy: ax.axis(\"equal\")\n",
        "\n",
        "def plt_cat_mc(X_train, y_train, model, classes):\n",
        "    #make a model for plotting routines to call\n",
        "    model_predict = lambda Xl: np.argmax(model.predict(Xl),axis=1)\n",
        "\n",
        "    fig,ax = plt.subplots(1,1, figsize=(3,3))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "\n",
        "    #add the original data to the decison boundary\n",
        "    plt_mc_data(ax, X_train,y_train, classes, map=dkcolors_map, legend=True)\n",
        "    #plot the decison boundary.\n",
        "    plot_cat_decision_boundary_mc(ax, X_train, model_predict, vector=True)\n",
        "    ax.set_title(\"model decision boundary\")\n",
        "\n",
        "    plt.xlabel(r'$x_0$');\n",
        "    plt.ylabel(r\"$x_1$\");\n",
        "    plt.show()\n",
        "\n",
        "def plot_cat_decision_boundary_mc(ax, X, predict , class_labels=None, legend=False, vector=True):\n",
        "\n",
        "    # create a mesh to points to plot\n",
        "    x_min, x_max = X[:, 0].min()- 0.5, X[:, 0].max()+0.5\n",
        "    y_min, y_max = X[:, 1].min()- 0.5, X[:, 1].max()+0.5\n",
        "    h = max(x_max-x_min, y_max-y_min)/100\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    #print(\"points\", points.shape)\n",
        "    #print(\"xx.shape\", xx.shape)\n",
        "\n",
        "    #make predictions for each point in mesh\n",
        "    if vector:\n",
        "        Z = predict(points)\n",
        "    else:\n",
        "        Z = np.zeros((len(points),))\n",
        "        for i in range(len(points)):\n",
        "            Z[i] = predict(points[i].reshape(1,2))\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    #contour plot highlights boundaries between values - classes in this case\n",
        "    ax.contour(xx, yy, Z, linewidths=1)\n",
        "    #ax.axis('tight')\n",
        "\n",
        "def plt_layer_relu(X, Y, W1, b1, classes):\n",
        "    nunits = (W1.shape[1])\n",
        "    Y = Y.reshape(-1,)\n",
        "    fig,ax = plt.subplots(1,W1.shape[1], figsize=(7,2.5))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "\n",
        "    for i in range(nunits):\n",
        "        layerf= lambda x : np.maximum(0,(np.dot(x,W1[:,i]) + b1[i]))\n",
        "        plt_prob_z(ax[i], layerf)\n",
        "        plt_mc_data(ax[i], X, Y, classes, map=dkcolors_map,legend=True, size=50, m='o')\n",
        "        ax[i].set_title(f\"Layer 1 Unit {i}\")\n",
        "        ax[i].set_ylabel(r\"$x_1$\",size=10)\n",
        "        ax[i].set_xlabel(r\"$x_0$\",size=10)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plt_prob_z(ax,fwb, x0_rng=(-8,8), x1_rng=(-5,4)):\n",
        "    \"\"\" plots a decision boundary but include shading to indicate the probability\n",
        "        and adds a conouter to show where z=0\n",
        "    \"\"\"\n",
        "    #setup useful ranges and common linspaces\n",
        "    x0_space  = np.linspace(x0_rng[0], x0_rng[1], 40)\n",
        "    x1_space  = np.linspace(x1_rng[0], x1_rng[1], 40)\n",
        "\n",
        "    # get probability for x0,x1 ranges\n",
        "    tmp_x0,tmp_x1 = np.meshgrid(x0_space,x1_space)\n",
        "    z = np.zeros_like(tmp_x0)\n",
        "    c = np.zeros_like(tmp_x0)\n",
        "    for i in range(tmp_x0.shape[0]):\n",
        "        for j in range(tmp_x1.shape[1]):\n",
        "            x = np.array([[tmp_x0[i,j],tmp_x1[i,j]]])\n",
        "            z[i,j] = fwb(x)\n",
        "            c[i,j] = 0. if z[i,j] == 0 else 1.\n",
        "    with warnings.catch_warnings():  # suppress no contour warning\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        #ax.contour(tmp_x0, tmp_x1, c, colors='b', linewidths=1)\n",
        "        ax.contour(tmp_x0, tmp_x1, c, linewidths=1)\n",
        "\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "    new_cmap = truncate_colormap(cmap, 0.0, 0.7)\n",
        "\n",
        "    pcm = ax.pcolormesh(tmp_x0, tmp_x1, z,\n",
        "                   norm=cm.colors.Normalize(vmin=np.amin(z), vmax=np.amax(z)),\n",
        "                   cmap=new_cmap, shading='nearest', alpha = 0.9)\n",
        "    ax.figure.colorbar(pcm, ax=ax)\n",
        "\n",
        "def plt_output_layer_linear(X, Y, W, b, classes, x0_rng=None, x1_rng=None):\n",
        "    nunits = (W.shape[1])\n",
        "    Y = Y.reshape(-1,)\n",
        "    fig,ax = plt.subplots(2,int(nunits/2), figsize=(7,5))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "    for i,axi in enumerate(ax.flat):\n",
        "        layerf = lambda x : np.dot(x,W[:,i]) + b[i]\n",
        "        plt_prob_z(axi, layerf, x0_rng=x0_rng, x1_rng=x1_rng)\n",
        "        plt_mc_data(axi, X, Y, classes, map=dkcolors_map,legend=True, size=50, m='o')\n",
        "        axi.set_ylabel(r\"$a^{[1]}_1$\",size=9)\n",
        "        axi.set_xlabel(r\"$a^{[1]}_0$\",size=9)\n",
        "        axi.set_xlim(x0_rng)\n",
        "        axi.set_ylim(x1_rng)\n",
        "        axi.set_title(f\"Linear Output Unit {i}\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
        "    \"\"\" truncates color map \"\"\"\n",
        "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
        "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
        "        cmap(np.linspace(minval, maxval, n)))\n",
        "    return new_cmap"
      ],
      "metadata": {
        "id": "LXiSvLiqmNEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W2_Lab05_TF (Derivatives)\n"
      ],
      "metadata": {
        "id": "iJ1KOHMd-gDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Lab - Derivatives\n",
        "This lab will give you a more intuitive understanding of derivatives. It will show you a simple way of calculating derivatives arithmetically. It will also introduce you to a handy Python library that allows you to calculate derivatives symbolically."
      ],
      "metadata": {
        "id": "oVTl_zMO_CDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import symbols, diff"
      ],
      "metadata": {
        "id": "chPV75y1_FGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Informal definition of derivatives\n",
        "The formal definition of derivatives can be a bit daunting with limits and values 'going to zero'. The idea is really much simpler.\n",
        "\n",
        "The derivative of a function describes how the output of a function changes when there is a small change in an input variable.\n",
        "\n",
        "Let's use the cost function $J(w)$ as an example. The cost $J$ is the output and $w$ is the input variable.  \n",
        "Let's give a 'small change' a name *epsilon* or $\\epsilon$. We use these Greek letters because it is traditional in mathematics to use *epsilon*($\\epsilon$) or *delta* ($\\Delta$) to represent a small value. You can think of it as representing 0.001 or some other small value.  \n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\text{if } w \\uparrow \\epsilon \\text{ causes }J(w) \\uparrow \\text{by }k \\times \\epsilon \\text{ then}  \\\\\n",
        "\\frac{\\partial J(w)}{\\partial w} = k \\tag{1}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "This just says if you change the input to the function $J(w)$ by a little bit and the output changes by $k$ times that little bit, then the derivative of $J(w)$ is equal to $k$.\n",
        "\n",
        "Let's try this out.  Let's look at the derivative of the function $J(w) = w^2$ at the point $w=3$ and $\\epsilon = 0.001$"
      ],
      "metadata": {
        "id": "VdviazUA_OEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J = (3)**2\n",
        "J_epsilon = (3 + 0.001)**2\n",
        "k = (J_epsilon - J)/0.001    # difference divided by epsilon\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k:0.6f} \")"
      ],
      "metadata": {
        "id": "6spHLMt7_uAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have increased the input value a little bit (0.001), causing the output to change from 9 to 9.006001, an increase of 6 times the input increase. Referencing (1) above, this says that $k=6$, so $\\frac{\\partial J(w)}{\\partial w} \\approx 6$. If you are familiar with calculus, you know, written symbolically,  $\\frac{\\partial J(w)}{\\partial w} = 2 w$. With $w=3$ this is 6. Our calculation above is not exactly 6 because to be exactly correct $\\epsilon$ would need to be [infinitesimally small](https://www.dictionary.com/browse/infinitesimally) or really, really small. That is why we use the symbols $\\approx$ or ~= rather than =. Let's see what happens if we make $\\epsilon$ smaller."
      ],
      "metadata": {
        "id": "uQM0ZbTz_5Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J = (3)**2\n",
        "J_epsilon = (3 + 0.000000001)**2\n",
        "k = (J_epsilon - J)/0.000000001\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")"
      ],
      "metadata": {
        "id": "8saN6_5U_8BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value gets close to exactly 6 as we reduce the size of $\\epsilon$. Feel free to try reducing the value further."
      ],
      "metadata": {
        "id": "-Wn4wGslAD-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding symbolic derivatives\n",
        "In backprop it is useful to know the derivative of simple functions at any input value. Put another way, we would like to know the 'symbolic' derivative rather than the 'arithmetic' derivative. An example of a symbolic derivative is,  $\\frac{\\partial J(w)}{\\partial w} = 2 w$, the derivative of $J(w) = w^2$ above.  With the symbolic derivative you can find the value of the derivative at any input value $w$.  \n",
        "\n",
        "If you have taken a calculus course, you are familiar with the many [differentiation rules](https://en.wikipedia.org/wiki/Differentiation_rules#Power_laws,_polynomials,_quotients,_and_reciprocals) that mathematicians have developed to solve for a derivative given an expression. Well, it turns out this process has been automated with symbolic differentiation programs. An example of this in python is the [SymPy](https://www.sympy.org/en/index.html) library. Let's take a look at how to use this."
      ],
      "metadata": {
        "id": "IxMSSvViAV28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $J = w^2$\n",
        "Define the python variables and their symbolic names."
      ],
      "metadata": {
        "id": "sNPbbMDgAY5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J, w = symbols('J, w')"
      ],
      "metadata": {
        "id": "GNFuwH25AgAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define and print the expression. Note SymPy produces a latex string which generates a nicely readable equation."
      ],
      "metadata": {
        "id": "kE1YgFDdAjOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use SymPy's `diff` to differentiate the expression for $J$ with respect to $w$. Note the result matches our earlier example."
      ],
      "metadata": {
        "id": "SzoAWaOqAq4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J=w**2\n",
        "J"
      ],
      "metadata": {
        "id": "aGNqNuoBAl_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw = diff(J,w)\n",
        "dJ_dw"
      ],
      "metadata": {
        "id": "zHXUjgr3AyaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the derivative at a few points by 'substituting' numeric values for the symbolic values. In the first example, $w$ is replaced by $2$."
      ],
      "metadata": {
        "id": "-Q-rJXmaAwpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw.subs([(w,2)])    # derivative at the point w = 2"
      ],
      "metadata": {
        "id": "DGg36QmFBAO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw.subs([(w,3)])    # derivative at the point w = 3"
      ],
      "metadata": {
        "id": "HCRwo5ClBB2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw.subs([(w,-3)])    # derivative at the point w = -3"
      ],
      "metadata": {
        "id": "sF-RAckWBIDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
        "    \n",
        "```python\n",
        "J= 1/w**2\n",
        "dJ_dw = diff(J,w)\n",
        "dJ_dw.subs([(w,4)])\n",
        "```\n",
        "  \n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "IZy8tOxEBu5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W2_Lab06_TF (Backprop)\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "TPrFTRGcCXk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Lab: Back propagation using a computation graph\n",
        "Working through this lab will give you insight into a key algorithm used by most machine learning frameworks. Gradient descent requires the derivative of the cost with respect to each parameter in the network.  Neural networks can have millions or even billions of parameters. The *back propagation* algorithm is used to compute those derivatives. *Computation graphs* are used to simplify the operation. Let's dig into this below."
      ],
      "metadata": {
        "id": "dikdGfBGCYNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import *\n",
        "import numpy as np\n",
        "import re\n",
        "#%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.widgets import TextBox\n",
        "from matplotlib.widgets import Button\n",
        "import ipywidgets as widgets\n",
        "#from lab_utils_backprop import *"
      ],
      "metadata": {
        "id": "S2lhkBcQCpaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computation Graph\n",
        "A computation graph simplifies the computation of complex derivatives by breaking them into smaller steps. Let's see how this works.\n",
        "\n",
        "Let's calculate the derivative of this slightly complex expression, $J = (2+3w)^2$. We would like to find the derivative of $J$ with respect to $w$ or $\\frac{\\partial J}{\\partial w}$."
      ],
      "metadata": {
        "id": "32AecCcEDBBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Propagation   \n",
        "Let's calculate the values in the forward direction.\n",
        "\n",
        ">Just a note about this section. It uses global variables and reuses them as the calculation progresses. If you run cells out of order, you may get funny results. If you do, go back to this point and run them in order."
      ],
      "metadata": {
        "id": "rG7AGPZUINUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = 3\n",
        "a = 2+3*w\n",
        "J = a**2\n",
        "print(f\"a = {a}, J = {J}\")"
      ],
      "metadata": {
        "id": "2BIDdOV3IP-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backprop\n",
        "<img align=\"left\" src=\"./images/C2_W2_BP_network0_j.PNG\"     style=\" width:100px; padding: 10px 20px; \" > Backprop is the algorithm we use to calculate derivatives. As described in the lectures, backprop starts at the right and moves to the left. The first node to consider is $J = a^2 $ and the first step is to find $\\frac{\\partial J}{\\partial a}$\n"
      ],
      "metadata": {
        "id": "QOesZqwgIVcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\frac{\\partial J}{\\partial a}$\n",
        "#### Arithmetically\n",
        "Find $\\frac{\\partial J}{\\partial a}$ by finding how $J$ changes as a result of a little change in $a$. This is described in detail in the derivatives optional lab."
      ],
      "metadata": {
        "id": "txOVSwUnIbqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_epsilon = a + 0.001       # a epsilon\n",
        "J_epsilon = a_epsilon**2    # J_epsilon\n",
        "k = (J_epsilon - J)/0.001   # difference divided by epsilon\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_da ~= k = {k} \")"
      ],
      "metadata": {
        "id": "OOphDG0pIhNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\frac{\\partial J}{\\partial a}$ is 22 which is $2\\times a$. Our result is not exactly $2 \\times a$ because our epsilon value is not infinitesimally small.\n",
        "#### Symbolically\n",
        "Now, let's use SymPy to calculate derivatives symbolically as we did in the derivatives optional lab. We will prefix the name of the variable with an 's' to indicate this is a *symbolic* variable."
      ],
      "metadata": {
        "id": "me3h0qhCIxjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sw,sJ,sa = symbols('w,J,a')\n",
        "sJ = sa**2\n",
        "sJ"
      ],
      "metadata": {
        "id": "cH0lLJj2Iwxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sJ.subs([(sa,a)])"
      ],
      "metadata": {
        "id": "EKCuxg_KI3Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_da = diff(sJ, sa)\n",
        "dJ_da"
      ],
      "metadata": {
        "id": "jI4ZULWZI5Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, $\\frac{\\partial J}{\\partial a} = 2a$. When $a=11$, $\\frac{\\partial J}{\\partial a} = 22$. This matches our arithmetic calculation above.\n",
        "If you have not already done so, you can go back to the diagram above and fill in the value for $\\frac{\\partial J}{\\partial a}$."
      ],
      "metadata": {
        "id": "pvqd3_vNJAOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\frac{\\partial J}{\\partial w}$\n",
        "<img align=\"left\" src=\"./images/C2_W2_BP_network0_a.PNG\"     style=\" width:100px; padding: 10px 20px; \" >  Moving from right to left, the next value we would like to compute is $\\frac{\\partial J}{\\partial w}$. To do this, we first need to calculate $\\frac{\\partial a}{\\partial w}$ which describes how the output of this node, $a=2+3w$, changes when the input $w$ changes a little bit."
      ],
      "metadata": {
        "id": "flGxQBulJExW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Arithmetically\n",
        "Find $\\frac{\\partial a}{\\partial w}$ by finding how $a$ changes as a result of a little change in $w$."
      ],
      "metadata": {
        "id": "06E5SRxkJ4bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_epsilon = w + 0.001       # a  plus a small value, epsilon\n",
        "a_epsilon = 2 + 3*w_epsilon\n",
        "k = (a_epsilon - a)/0.001   # difference divided by epsilon\n",
        "print(f\"a = {a}, a_epsilon = {a_epsilon}, da_dw ~= k = {k} \")"
      ],
      "metadata": {
        "id": "gSNoQXohJ6q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculated arithmetically,  $\\frac{\\partial a}{\\partial w} \\approx 3$. Let's try it with SymPy."
      ],
      "metadata": {
        "id": "XUEBp1cKJ-Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sa = 2 + 3*sw\n",
        "sa"
      ],
      "metadata": {
        "id": "JWbpZkbpKBMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "da_dw = diff(sa,sw)\n",
        "da_dw"
      ],
      "metadata": {
        "id": "TZ2jbxtVKDHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">The next step is the interesting part:\n",
        "> - We know that a small change in $w$ will cause $a$ to change by 3 times that amount.\n",
        "> - We know that a small change in $a$ will cause $J$ to change by $2\\times a$ times that amount. (a=11 in this example)    \n",
        " so, putting these together,\n",
        "> - We  know that a small change in $w$ will cause $J$ to change by $3 \\times 2\\times a$ times that amount.\n",
        ">\n",
        "> These cascading changes go by the name of *the chain rule*.  It can be written like this:\n",
        " $$\\frac{\\partial J}{\\partial w} = \\frac{\\partial a}{\\partial w} \\frac{\\partial J}{\\partial a} $$\n",
        "\n",
        "It's worth spending some time thinking this through if it is not clear. This is a key take-away.\n",
        "\n",
        " Let's try calculating it:"
      ],
      "metadata": {
        "id": "CjUGgM1BJwyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw = da_dw * dJ_da\n",
        "dJ_dw"
      ],
      "metadata": {
        "id": "j5HCh5vbKQ5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And  a  is 11 in this example so  âJâw=66 . We can check this arithmetically:"
      ],
      "metadata": {
        "id": "nTeFFa3EKbab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_epsilon = w + 0.001\n",
        "a_epsilon = 2 + 3*w_epsilon\n",
        "J_epsilon = a_epsilon**2\n",
        "k = (J_epsilon - J)/0.001   # difference divided by epsilon\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")"
      ],
      "metadata": {
        "id": "j5mNGPczKXZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK! You can now fill the values for  $\\frac{\\partial a}{\\partial w}$ and $\\frac{\\partial J}{\\partial w}$ in  the diagram if you have not already done so."
      ],
      "metadata": {
        "id": "clRDtK6ZKfpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R2lQ0J7ukivP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W3_Lab07 (Model_Evaluation_and_Selection)\n",
        "\n",
        "Quantifying a learning algorithm's performance and comparing different models are some of the common tasks when applying machine learning to real world applications. In this lab, you will practice doing these using the tips shared in class. Specifically, you will:\n",
        "\n",
        "* split datasets into training, cross validation, and test sets\n",
        "* evaluate regression and classification models\n",
        "* add polynomial features to improve the performance of a linear regression model\n",
        "* compare several neural network architectures\n"
      ],
      "metadata": {
        "id": "STSmhw4qkyD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Lab Setup\n",
        "\n",
        "First, you will import the packages needed for the tasks in this lab. We also included some commands to make the outputs later more readable by reducing verbosity and suppressing non-critical warnings."
      ],
      "metadata": {
        "id": "rvH07KwNeQ7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Lab Setup"
      ],
      "metadata": {
        "id": "gcKqtN38lc9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for array computations and loading data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# for building linear regression models and preparing data\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# for building and training neural networks\n",
        "import tensorflow as tf\n",
        "\n",
        "# custom functions\n",
        "#import utils\n",
        "\n",
        "# reduce display precision on numpy arrays\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# suppress warnings\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.autograph.set_verbosity(0)"
      ],
      "metadata": {
        "id": "9gfIxFCHkxNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression\n",
        "\n",
        "First, you will be tasked to develop a model for a regression problem. You are given the dataset below consisting of 50 examples of an input feature `x` and its corresponding target `y`."
      ],
      "metadata": {
        "id": "OpY81Qozmhf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "url = '/content/drive/My Drive/Colab Notebooks/Data/c2_w3/data_w3_ex1.csv'\n",
        "df = pd.read_csv(url, header=None)\n",
        "df.columns = ['X', 'y']\n",
        "df.head(5)\n",
        "df = df.to_numpy()"
      ],
      "metadata": {
        "id": "52Rhe4u6juZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the inputs and outputs into separate arrays\n",
        "\n",
        "x = df[:,0]\n",
        "y = df[:,1]\n",
        "\n",
        "# Convert 1-D arrays into 2-D because the commands later will require it\n",
        "x = np.expand_dims(x, axis=1)\n",
        "y = np.expand_dims(y, axis=1)\n",
        "\n",
        "print(f\"the shape of the inputs x is: {x.shape}\")\n",
        "print(f\"the shape of the targets y is: {y.shape}\")"
      ],
      "metadata": {
        "id": "jfl0vDbvmyA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can plot the dataset to get an idea of how the target behaves with respect to the input."
      ],
      "metadata": {
        "id": "P_duWd9voT58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the entire dataset\n",
        "plot_dataset(x=x, y=y, title=\"input vs. target\")"
      ],
      "metadata": {
        "id": "pqH2v6OdoadS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the dataset into training, cross validation, and test sets\n",
        "\n",
        "In previous labs, you might have used the entire dataset to train your models. In practice however, it is best to hold out a portion of your data to measure how well your model generalizes to new examples. This will let you know if the model has overfit to your training set.\n",
        "\n",
        "As mentioned in the lecture, it is common to split your data into three parts:\n",
        "\n",
        "* ***training set*** - used to train the model\n",
        "* ***cross validation set (also called validation, development, or dev set)*** - used to evaluate the different model configurations you are choosing from. For example, you can use this to make a decision on what polynomial features to add to your dataset.\n",
        "* ***test set*** - used to give a fair estimate of your chosen model's performance against new examples. This should not be used to make decisions while you are still developing the models.\n",
        "\n",
        "Scikit-learn provides a [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split your data into the parts mentioned above. In the code cell below, you will split the entire dataset into 60% training, 20% cross validation, and 20% test."
      ],
      "metadata": {
        "id": "MqOi1HTVpQ_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
        "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
        "\n",
        "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
        "\n",
        "# Delete temporary variables\n",
        "del x_, y_\n",
        "\n",
        "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
        "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
        "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
        "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
        "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
        "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "2dfXeP1epU1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can plot the dataset again below to see which points were used as training, cross validation, or test data."
      ],
      "metadata": {
        "id": "zS8QXAmGp2RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title=\"input vs. target\")"
      ],
      "metadata": {
        "id": "kfGrVxYap5Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit a linear model\n",
        "\n",
        "Now that you have split the data, one of the first things you can try is to fit a linear model. You will do that in the next sections below."
      ],
      "metadata": {
        "id": "iPG8O4HDqDj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous course of this specialization, you saw that it is usually a good idea to perform feature scaling to help your model converge faster. This is especially true if your input features have widely different ranges of values. Later in this lab, you will be adding polynomial terms so your input features will indeed have different ranges. For example, $x$ runs from around 1600 to 3600, while $x^2$ will run from 2.56 million to 12.96 million.\n",
        "\n",
        "You will only use $x$ for this first model but it's good to practice feature scaling now so you can apply it later. For that, you will use the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) class from scikit-learn. This computes the z-score of your inputs. As a refresher, the z-score is given by the equation:\n",
        "\n",
        "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
        "\n",
        "where $\\mu$ is the mean of the feature values and $\\sigma$ is the standard deviation. The code below shows how to prepare the training set using the said class. You can plot the results again to inspect if it still follows the same pattern as before. The new graph should have a reduced range of values for `x`."
      ],
      "metadata": {
        "id": "QBVOXp1gqXsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the class\n",
        "scaler_linear = StandardScaler()\n",
        "\n",
        "# Compute the mean and standard deviation of the training set then transform it\n",
        "X_train_scaled = scaler_linear.fit_transform(x_train)\n",
        "\n",
        "print(f\"Computed mean of the training set: {scaler_linear.mean_.squeeze():.2f}\")\n",
        "print(f\"Computed standard deviation of the training set: {scaler_linear.scale_.squeeze():.2f}\")\n",
        "\n",
        "# Plot the results\n",
        "plot_dataset(x=X_train_scaled, y=y_train, title=\"scaled input vs. target\")"
      ],
      "metadata": {
        "id": "_cGlSs9LqCzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model\n",
        "\n",
        "Next, you will create and train a regression model. For this lab, you will use the [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class but take note that there are other [linear regressors](https://scikit-learn.org/stable/modules/classes.html#classical-linear-regressors) which you can also use."
      ],
      "metadata": {
        "id": "8N1KVMLisf-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the class\n",
        "linear_model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "linear_model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "SmenicKRspUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the Model\n",
        "\n",
        "To evaluate the performance of your model, you will measure the error for the training and cross validation sets. For the training error, recall the equation for calculating the mean squared error (MSE):\n",
        "\n",
        "$$J_{train}(\\vec{w}, b) = \\frac{1}{2m_{train}}\\left[\\sum_{i=1}^{m_{train}}(f_{\\vec{w},b}(\\vec{x}_{train}^{(i)}) - y_{train}^{(i)})^2\\right]$$\n",
        "\n",
        "Scikit-learn also has a built-in [`mean_squared_error()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function that you can use. Take note though that [as per the documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error), scikit-learn's implementation only divides by `m` and not `2*m`, where `m` is the number of examples. As mentioned in Course 1 of this Specialization (cost function lectures), dividing by `2m` is a convention we will follow but the calculations should still work whether or not you include it. Thus, to match the equation above, you can use the scikit-learn function then divide by 2 as shown below. We also included a for-loop implementation so you can check that it's equal.\n",
        "\n",
        "Another thing to take note: Since you trained the model on scaled values (i.e. using the z-score), you should also feed in the scaled training set instead of its raw values."
      ],
      "metadata": {
        "id": "Y0iB6H9ftRoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed the scaled training set and get the predictions\n",
        "yhat = linear_model.predict(X_train_scaled)\n",
        "\n",
        "# Use scikit-learn's utility function and divide by 2\n",
        "print(f\"training MSE (using sklearn function): {mean_squared_error(y_train, yhat) / 2}\")\n",
        "\n",
        "# for-loop implementation\n",
        "total_squared_error = 0\n",
        "\n",
        "for i in range(len(yhat)):\n",
        "  squared_error_i  = (yhat[i] - y_train[i])**2\n",
        "  total_squared_error += squared_error_i\n",
        "\n",
        "mse = total_squared_error / (2*len(yhat))\n",
        "\n",
        "print(f\"training MSE (for-loop implementation): {mse.squeeze()}\")"
      ],
      "metadata": {
        "id": "WfVyxZZ7tWe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then compute the MSE for the cross validation set with basically the same equation:\n",
        "\n",
        "$$J_{cv}(\\vec{w}, b) = \\frac{1}{2m_{cv}}\\left[\\sum_{i=1}^{m_{cv}}(f_{\\vec{w},b}(\\vec{x}_{cv}^{(i)}) - y_{cv}^{(i)})^2\\right]$$\n",
        "\n",
        "As with the training set, you will also want to scale the cross validation set. An *important* thing to note when using the z-score is you have to use the mean and standard deviation of the **training set** when scaling the cross validation set. This is to ensure that your input features are transformed as expected by the model. One way to gain intuition is with this scenario:\n",
        "\n",
        "* Say that your training set has an input feature equal to `500` which is scaled down to `0.5` using the z-score.\n",
        "* After training, your model is able to accurately map this scaled input `x=0.5` to the target output `y=300`.\n",
        "* Now let's say that you deployed this model and one of your users fed it a sample equal to `500`.\n",
        "* If you get this input sample's z-score using any other values of the mean and standard deviation, then it might not be scaled to `0.5` and your model will most likely make a wrong prediction (i.e. not equal to `y=300`).\n",
        "\n",
        "You will scale the cross validation set below by using the same `StandardScaler` you used earlier but only calling its [`transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.transform) method instead of [`fit_transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform)."
      ],
      "metadata": {
        "id": "c0QKPZ6PyUYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the cross validation set using the mean and standard deviation of the training set\n",
        "X_cv_scaled = scaler_linear.transform(x_cv)\n",
        "\n",
        "print(f\"Mean used to scale the CV set: {scaler_linear.mean_.squeeze():.2f}\")\n",
        "print(f\"Standard deviation used to scale the CV set: {scaler_linear.scale_.squeeze():.2f}\")\n",
        "\n",
        "# Feed the scaled cross validation set\n",
        "yhat = linear_model.predict(X_cv_scaled)\n",
        "\n",
        "# Use scikit-learn's utility function and divide by 2\n",
        "print(f\"Cross validation MSE: {mean_squared_error(y_cv, yhat) / 2}\")"
      ],
      "metadata": {
        "id": "Kd03hxuHyXnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Polynomial Features\n",
        "\n",
        "From the graphs earlier, you may have noticed that the target `y` rises more sharply at smaller values of `x` compared to higher ones. A straight line might not be the best choice because the target `y` seems to flatten out as `x` increases. Now that you have these values of the training and cross validation MSE from the linear model, you can try adding polynomial features to see if you can get a better performance. The code will mostly be the same but with a few extra preprocessing steps. Let's see that below."
      ],
      "metadata": {
        "id": "JoWSpE7_yhYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the additional features\n",
        "\n",
        "First, you will generate the polynomial features from your training set. The code below demonstrates how to do this using the [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class. It will create a new input feature which has the squared values of the input `x` (i.e. degree=2)."
      ],
      "metadata": {
        "id": "qa1jFfEcypv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the class to make polynomial features\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "# Compute the number of features and transform the training set\n",
        "X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "# Preview the first 5 elements of the new training set. Left column is `x` and right column is `x^2`\n",
        "# Note: The `e+<number>` in the output denotes how many places the decimal point should\n",
        "# be moved. For example, `3.24e+03` is equal to `3240`\n",
        "print(X_train_mapped[:5])"
      ],
      "metadata": {
        "id": "Eus1utvZywtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will then scale the inputs as before to narrow down the range of values."
      ],
      "metadata": {
        "id": "3Jw4diC3y_Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the class\n",
        "scaler_poly = StandardScaler()\n",
        "\n",
        "# Compute the mean and standard deviation of the training set then transform it\n",
        "X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "\n",
        "# Preview the first 5 elements of the scaled training set.\n",
        "print(X_train_mapped_scaled[:5])"
      ],
      "metadata": {
        "id": "Kcg0MckNzHcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then proceed to train the model. After that, you will measure the model's performance against the cross validation set. Like before, you should make sure to perform the same transformations as you did in the training set. You will add the same number of polynomial features then scale the range of values."
      ],
      "metadata": {
        "id": "jSz0fBdEzPa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the class\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_mapped_scaled, y_train )\n",
        "\n",
        "# Compute the training MSE\n",
        "yhat = model.predict(X_train_mapped_scaled)\n",
        "print(f\"Training MSE: {mean_squared_error(y_train, yhat) / 2}\")\n",
        "\n",
        "# Add the polynomial features to the cross validation set\n",
        "X_cv_mapped = poly.transform(x_cv)\n",
        "\n",
        "# Scale the cross validation set using the mean and standard deviation of the training set\n",
        "X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "# Compute the cross validation MSE\n",
        "yhat = model.predict(X_cv_mapped_scaled)\n",
        "print(f\"Cross validation MSE: {mean_squared_error(y_cv, yhat) / 2}\")"
      ],
      "metadata": {
        "id": "gfel3KqyzR3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll notice that the MSEs are significantly better for both the training and cross validation set when you added the 2nd order polynomial. You may want to introduce more polynomial terms and see which one gives the best performance. As shown in class, you can have 10 different models like this:\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1hwZ5FoBJO-FFf3DmWBXI4SUBdqkp1o-o)\n",
        "\n",
        "You can create a loop that contains all the steps in the previous code cells. Here is one implementation that adds polynomial features up to degree=10. We'll plot it at the end to make it easier to compare the results for each model."
      ],
      "metadata": {
        "id": "AQxF3AmrzepP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists containing the lists, models, and scalers\n",
        "train_mses = []\n",
        "cv_mses = []\n",
        "models = []\n",
        "scalers = []\n",
        "\n",
        "# Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "for degree in range(1,11):\n",
        "\n",
        "    # Add polynomial features to the training set\n",
        "    poly = PolynomialFeatures(degree, include_bias=False)\n",
        "    X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "    # Scale the training set\n",
        "    scaler_poly = StandardScaler()\n",
        "    X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "    scalers.append(scaler_poly)\n",
        "\n",
        "    # Create and train the model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_mapped_scaled, y_train )\n",
        "    models.append(model)\n",
        "\n",
        "    # Compute the training MSE\n",
        "    yhat = model.predict(X_train_mapped_scaled)\n",
        "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "    train_mses.append(train_mse)\n",
        "\n",
        "    # Add polynomial features and scale the cross validation set\n",
        "    poly = PolynomialFeatures(degree, include_bias=False)\n",
        "    X_cv_mapped = poly.fit_transform(x_cv)\n",
        "    X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "    # Compute the cross validation MSE\n",
        "    yhat = model.predict(X_cv_mapped_scaled)\n",
        "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "    cv_mses.append(cv_mse)\n",
        "\n",
        "# Plot the results\n",
        "degrees=range(1,11)\n",
        "plot_train_cv_mses(degrees, train_mses, cv_mses, title=\"degree of polynomial vs. train and CV MSEs\")"
      ],
      "metadata": {
        "id": "zikfw33800Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing the best model\n",
        "\n",
        "When selecting a model, you want to choose one that performs well both on the training and cross validation set. It implies that it is able to learn the patterns from your training set without overfitting. If you used the defaults in this lab, you will notice a sharp drop in cross validation error from the models with degree=1 to degree=2. This is followed by a relatively flat line up to degree=5. After that, however, the cross validation error is generally getting worse as you add more polynomial features. Given these, you can decide to use the model with the lowest `cv_mse` as the one best suited for your application."
      ],
      "metadata": {
        "id": "pWXH40af1Fki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model with the lowest CV MSE (add 1 because list indices start at 0)\n",
        "# This also corresponds to the degree of the polynomial added\n",
        "degree = np.argmin(cv_mses) + 1\n",
        "print(f\"Lowest CV MSE is found in the model with degree={degree}\")"
      ],
      "metadata": {
        "id": "sXenKQF81SnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then publish the generalization error by computing the test set's MSE. As usual, you should transform this data the same way you did with the training and cross validation sets."
      ],
      "metadata": {
        "id": "pQQ6D6zL1kDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add polynomial features to the test set\n",
        "poly = PolynomialFeatures(degree, include_bias=False)\n",
        "X_test_mapped = poly.fit_transform(x_test)\n",
        "\n",
        "# Scale the test set\n",
        "X_test_mapped_scaled = scalers[degree-1].transform(X_test_mapped)\n",
        "\n",
        "# Compute the test MSE\n",
        "yhat = models[degree-1].predict(X_test_mapped_scaled)\n",
        "test_mse = mean_squared_error(y_test, yhat) / 2\n",
        "\n",
        "print(f\"Training MSE: {train_mses[degree-1]:.2f}\")\n",
        "print(f\"Cross Validation MSE: {cv_mses[degree-1]:.2f}\")\n",
        "print(f\"Test MSE: {test_mse:.2f}\")"
      ],
      "metadata": {
        "id": "vkuvaomj1nAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks\n",
        "\n",
        "The same model selection process can also be used when choosing between different neural network architectures. In this section, you will create the models shown below and apply it to the same regression task above.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1my8evNUeqV-rC16apoFwnkMuSDSgSmKy)\n"
      ],
      "metadata": {
        "id": "EQRlYl8j2BZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the Data\n",
        "\n",
        "You will use the same training, cross validation, and test sets you generated in the previous section. From earlier lectures in this course, you may have known that neural networks can learn non-linear relationships so you can opt to skip adding polynomial features. The code is still included below in case you want to try later and see what effect it will have on your results. The default `degree` is set to `1` to indicate that it will just use `x_train`, `x_cv`, and `x_test` as is (i.e. without any additional polynomial features)."
      ],
      "metadata": {
        "id": "inLQUy1o2lZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add polynomial features\n",
        "degree = 4\n",
        "poly = PolynomialFeatures(degree, include_bias=False)\n",
        "X_train_mapped = poly.fit_transform(x_train)\n",
        "X_cv_mapped = poly.transform(x_cv)\n",
        "X_test_mapped = poly.transform(x_test)"
      ],
      "metadata": {
        "id": "0Wehefk12AtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you will scale the input features to help gradient descent converge faster. Again, notice that you are using the mean and standard deviation computed from the training set by just using `transform()` in the cross validation and test sets instead of `fit_transform()`."
      ],
      "metadata": {
        "id": "e_u2rmp4427-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features using the z-score\n",
        "scaler = StandardScaler()\n",
        "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
        "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
        "X_test_mapped_scaled = scaler.transform(X_test_mapped)"
      ],
      "metadata": {
        "id": "GfeARtbx48i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_mapped_scaled.shape"
      ],
      "metadata": {
        "id": "7bVYYcCI-GMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and train the models\n",
        "\n",
        "You will then create the neural network architectures shown earlier. The code is provided in the `build_models()` function in case you want to inspect or modify it. You will use that in the loop below then proceed to train the models. For each model, you will also record the training and cross validation errors."
      ],
      "metadata": {
        "id": "hJs4YHFz5BD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists that will contain the errors for each model\n",
        "nn_train_mses = []\n",
        "nn_cv_mses = []\n",
        "\n",
        "# Build the models\n",
        "nn_models = build_models()\n",
        "\n",
        "# Loop over the the models\n",
        "for model in nn_models:\n",
        "\n",
        "    # Setup the loss and optimizer\n",
        "    model.compile(\n",
        "    loss='mse',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
        "    )\n",
        "\n",
        "    print(f\"Training {model.name}...\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        X_train_mapped_scaled, y_train,\n",
        "        epochs=300,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    print(\"Done!\\n\")\n",
        "\n",
        "\n",
        "    # Record the training MSEs\n",
        "    yhat = model.predict(X_train_mapped_scaled)\n",
        "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "    nn_train_mses.append(train_mse)\n",
        "\n",
        "    # Record the cross validation MSEs\n",
        "    yhat = model.predict(X_cv_mapped_scaled)\n",
        "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "    nn_cv_mses.append(cv_mse)\n",
        "\n",
        "# print models summary\n",
        "print(\"Summary of the models:\")\n",
        "for model in nn_models:\n",
        "  print(model.summary())\n",
        "\n",
        "# print results\n",
        "print(\"RESULTS:\")\n",
        "for model_num in range(len(nn_train_mses)):\n",
        "    print(\n",
        "        f\"Model {model_num+1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" +\n",
        "        f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "TGOCn3la557d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the recorded errors, you can decide which is the best model for your application. Look at the results above and see if you agree with the selected `model_num` below. Finally, you will compute the test error to estimate how well it generalizes to new examples."
      ],
      "metadata": {
        "id": "HnLmMLwzBR2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the model with the lowest CV MSE\n",
        "model_num = 3\n",
        "\n",
        "# Compute the test MSE\n",
        "yhat = nn_models[model_num-1].predict(X_test_mapped_scaled)\n",
        "test_mse = mean_squared_error(y_test, yhat) / 2\n",
        "\n",
        "print(f\"Selected Model: {model_num}\")\n",
        "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
        "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
        "print(f\"Test MSE: {test_mse:.2f}\")"
      ],
      "metadata": {
        "id": "yu7bcuP6BVBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification\n",
        "\n",
        "In this last part of the lab, you will practice model evaluation and selection on a classification task. The process will be similar, with the main difference being the computation of the errors. You will see that in the following sections."
      ],
      "metadata": {
        "id": "ABsa986QHBNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Dataset\n",
        "\n",
        "First, you will load a dataset for a binary classification task. It has 200 examples of two input features (`x1` and `x2`), and a target `y` of either `0` or `1`."
      ],
      "metadata": {
        "id": "fNr1aCc9HFzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from a text file\n",
        "url = '/content/drive/My Drive/Colab Notebooks/Data/c2_w3/data_w3_ex2.csv'\n",
        "data = np.loadtxt(url, delimiter=',')\n",
        "print(type(data))\n",
        "# Split the inputs and outputs into separate arrays\n",
        "x_bc = data[:,:-1]\n",
        "y_bc = data[:,-1]\n",
        "\n",
        "# Convert y into 2-D because the commands later will require it (x is already 2-D)\n",
        "y_bc = np.expand_dims(y_bc, axis=1)\n",
        "\n",
        "print(f\"the shape of the inputs x is: {x_bc.shape}\")\n",
        "print(f\"the shape of the targets y is: {y_bc.shape}\")"
      ],
      "metadata": {
        "id": "6Y8YNhRWHLL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can plot the dataset to examine how the examples are separated."
      ],
      "metadata": {
        "id": "Puel1Q2UIVrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_bc_dataset(x=x_bc, y=y_bc, title=\"x1 vs. x2\")"
      ],
      "metadata": {
        "id": "zEK3_OfKIYTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split and prepare the dataset\n",
        "\n",
        "Next, you will generate the training, cross validation, and test sets. You will use the same 60/20/20 proportions as before. You will also scale the features as you did in the previous section."
      ],
      "metadata": {
        "id": "m4_y3Pb8Iikh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
        "x_bc_train, x_, y_bc_train, y_ = train_test_split(x_bc, y_bc, test_size=0.40, random_state=1)\n",
        "\n",
        "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "x_bc_cv, x_bc_test, y_bc_cv, y_bc_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
        "\n",
        "# Delete temporary variables\n",
        "del x_, y_\n",
        "\n",
        "print(f\"the shape of the training set (input) is: {x_bc_train.shape}\")\n",
        "print(f\"the shape of the training set (target) is: {y_bc_train.shape}\\n\")\n",
        "print(f\"the shape of the cross validation set (input) is: {x_bc_cv.shape}\")\n",
        "print(f\"the shape of the cross validation set (target) is: {y_bc_cv.shape}\\n\")\n",
        "print(f\"the shape of the test set (input) is: {x_bc_test.shape}\")\n",
        "print(f\"the shape of the test set (target) is: {y_bc_test.shape}\")"
      ],
      "metadata": {
        "id": "_h5tJti6IlHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features\n",
        "\n",
        "# Initialize the class\n",
        "scaler_linear = StandardScaler()\n",
        "\n",
        "# Compute the mean and standard deviation of the training set then transform it\n",
        "x_bc_train_scaled = scaler_linear.fit_transform(x_bc_train)\n",
        "x_bc_cv_scaled = scaler_linear.transform(x_bc_cv)\n",
        "x_bc_test_scaled = scaler_linear.transform(x_bc_test)"
      ],
      "metadata": {
        "id": "wJcBLXeyIpAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the error for classification models\n",
        "\n",
        "In the previous sections on regression models, you used the mean squared error to measure how well your model is doing. For classification, you can get a similar metric by getting the fraction of the data that the model has misclassified. For example, if your model made wrong predictions for 2 samples out of 5, then you will report an error of `40%` or `0.4`. The code below demonstrates this using a for-loop and also with Numpy's [`mean()`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) function."
      ],
      "metadata": {
        "id": "2a4VxxTaIsX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample model output\n",
        "probabilities = np.array([0.2, 0.6, 0.7, 0.3, 0.8])\n",
        "\n",
        "# Apply a threshold to the model output. If greater than 0.5, set to 1. Else 0.\n",
        "predictions = np.where(probabilities >= 0.5, 1, 0)\n",
        "\n",
        "# Ground truth labels\n",
        "ground_truth = np.array([1, 1, 1, 1, 1])\n",
        "\n",
        "# Initialize counter for misclassified data\n",
        "misclassified = 0\n",
        "\n",
        "# Get number of predictions\n",
        "num_predictions = len(predictions)\n",
        "\n",
        "# Loop over each prediction\n",
        "for i in range(num_predictions):\n",
        "\n",
        "    # Check if it matches the ground truth\n",
        "    if predictions[i] != ground_truth[i]:\n",
        "\n",
        "        # Add one to the counter if the prediction is wrong\n",
        "        misclassified += 1\n",
        "\n",
        "# Compute the fraction of the data that the model misclassified\n",
        "fraction_error = misclassified/num_predictions\n",
        "\n",
        "print(f\"probabilities: {probabilities}\")\n",
        "print(f\"predictions with threshold=0.5: {predictions}\")\n",
        "print(f\"targets: {ground_truth}\")\n",
        "print(f\"fraction of misclassified data (for-loop): {fraction_error}\")\n",
        "print(f\"fraction of misclassified data (with np.mean()): {np.mean(predictions != ground_truth)}\")"
      ],
      "metadata": {
        "id": "HgPCGnCzJKRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and train the model\n",
        "\n",
        "You will use the same neural network architectures in the previous section so you can call the `build_models()` function again to create new instances of these models.\n",
        "\n",
        "You will follow the recommended approach mentioned last week where you use a `linear` activation for the output layer (instead of `sigmoid`) then set `from_logits=True` when declaring the loss function of the model. You will use the [binary crossentropy loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) because this is a binary classification problem.\n",
        "\n",
        "After training, you will use a [sigmoid function](https://www.tensorflow.org/api_docs/python/tf/math/sigmoid) to convert the model outputs into probabilities. From there, you can set a threshold and get the fraction of misclassified examples from the training and cross validation sets.\n",
        "\n",
        "You can see all these in the code cell below."
      ],
      "metadata": {
        "id": "5Ci1Lh7ZJRO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists that will contain the errors for each model\n",
        "nn_train_error = []\n",
        "nn_cv_error = []\n",
        "\n",
        "# Build the models\n",
        "models_bc = build_models()\n",
        "\n",
        "# Loop over each model\n",
        "for model in models_bc:\n",
        "\n",
        "    # Setup the loss and optimizer\n",
        "    model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    )\n",
        "\n",
        "    print(f\"Training {model.name}...\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        x_bc_train_scaled, y_bc_train,\n",
        "        epochs=200,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    print(\"Done!\\n\")\n",
        "\n",
        "    # Set the threshold for classification\n",
        "    threshold = 0.5\n",
        "\n",
        "    # Record the fraction of misclassified examples for the training set\n",
        "    yhat = model.predict(x_bc_train_scaled)\n",
        "    yhat = tf.math.sigmoid(yhat)\n",
        "    yhat = np.where(yhat >= threshold, 1, 0)\n",
        "    train_error = np.mean(yhat != y_bc_train)\n",
        "    nn_train_error.append(train_error)\n",
        "\n",
        "    # Record the fraction of misclassified examples for the cross validation set\n",
        "    yhat = model.predict(x_bc_cv_scaled)\n",
        "    yhat = tf.math.sigmoid(yhat)\n",
        "    yhat = np.where(yhat >= threshold, 1, 0)\n",
        "    cv_error = np.mean(yhat != y_bc_cv)\n",
        "    nn_cv_error.append(cv_error)\n",
        "\n",
        "# Print the result\n",
        "for model_num in range(len(nn_train_error)):\n",
        "    print(\n",
        "        f\"Model {model_num+1}: Training Set Classification Error: {nn_train_error[model_num]:.5f}, \" +\n",
        "        f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "M7V7RyxkJrs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output above, you can choose which one performed best. If there is a tie on the cross validation set error, then you can pick the one with the lower training set error. Finally, you can compute the test error to report the model's generalization error."
      ],
      "metadata": {
        "id": "kQ7rPKbRJ6RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the model with the lowest error\n",
        "model_num = 3\n",
        "\n",
        "# Compute the test error\n",
        "yhat = models_bc[model_num-1].predict(x_bc_test_scaled)\n",
        "yhat = tf.math.sigmoid(yhat)\n",
        "yhat = np.where(yhat >= threshold, 1, 0)\n",
        "nn_test_error = np.mean(yhat != y_bc_test)\n",
        "\n",
        "print(f\"Selected Model: {model_num}\")\n",
        "print(f\"Training Set Classification Error: {nn_train_error[model_num-1]:.4f}\")\n",
        "print(f\"CV Set Classification Error: {nn_cv_error[model_num-1]:.4f}\")\n",
        "print(f\"Test Set Classification Error: {nn_test_error:.4f}\")"
      ],
      "metadata": {
        "id": "VsMlV9tTKA-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the functions we use above:"
      ],
      "metadata": {
        "id": "bG-7WxiMl8hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "#plt.style.use('./deeplearning.mplstyle')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def plot_dataset(x, y, title):\n",
        "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
        "    plt.rcParams[\"lines.markersize\"] = 12\n",
        "    plt.scatter(x, y, marker='x', c='r');\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "    plt.show()\n",
        "\n",
        "def plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title):\n",
        "    plt.scatter(x_train, y_train, marker='x', c='r', label='training');\n",
        "    plt.scatter(x_cv, y_cv, marker='o', c='b', label='cross validation');\n",
        "    plt.scatter(x_test, y_test, marker='^', c='g', label='test');\n",
        "    plt.title(\"input vs. target\")\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_train_cv_mses(degrees, train_mses, cv_mses, title):\n",
        "    degrees = range(1,11)\n",
        "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_bc_dataset(x, y, title):\n",
        "    for i in range(len(y)):\n",
        "        marker = 'x' if y[i] == 1 else 'o'\n",
        "        c = 'r' if y[i] == 1 else 'b'\n",
        "        plt.scatter(x[i,0], x[i,1], marker=marker, c=c);\n",
        "    plt.title(\"x1 vs x2\")\n",
        "    plt.xlabel(\"x1\");\n",
        "    plt.ylabel(\"x2\");\n",
        "    y_0 = mlines.Line2D([], [], color='r', marker='x', markersize=12, linestyle='None', label='y=1')\n",
        "    y_1 = mlines.Line2D([], [], color='b', marker='o', markersize=12, linestyle='None', label='y=0')\n",
        "    plt.title(title)\n",
        "    plt.legend(handles=[y_0, y_1])\n",
        "    plt.show()\n",
        "\n",
        "def build_models():\n",
        "\n",
        "    tf.random.set_seed(20)\n",
        "\n",
        "    model_1 = Sequential(\n",
        "        [\n",
        "            Dense(25, activation = 'relu'),\n",
        "            Dense(15, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_1'\n",
        "    )\n",
        "\n",
        "    model_2 = Sequential(\n",
        "        [\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_2'\n",
        "    )\n",
        "\n",
        "    model_3 = Sequential(\n",
        "        [\n",
        "            Dense(32, activation = 'relu'),\n",
        "            Dense(16, activation = 'relu'),\n",
        "            Dense(8, activation = 'relu'),\n",
        "            Dense(4, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_3'\n",
        "    )\n",
        "\n",
        "    model_list = [model_1, model_2, model_3]\n",
        "\n",
        "    return model_list\n",
        "\n",
        "\n",
        "# Not used in the lab. You can call this for the binary\n",
        "# classification problem if you set `from_logits=False`\n",
        "# when declaring the loss. With this, you will also not need\n",
        "# to call the `tf.math.sigmoid()` function in the loop\n",
        "# because the model output is already a probability\n",
        "def build_bc_models():\n",
        "\n",
        "    tf.random.set_seed(20)\n",
        "\n",
        "    model_1_bc = Sequential(\n",
        "        [\n",
        "            Dense(25, activation = 'relu'),\n",
        "            Dense(15, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_1_bc'\n",
        "    )\n",
        "\n",
        "    model_2_bc = Sequential(\n",
        "        [\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_2_bc'\n",
        "    )\n",
        "\n",
        "    model_3_bc = Sequential(\n",
        "        [\n",
        "            Dense(32, activation = 'relu'),\n",
        "            Dense(16, activation = 'relu'),\n",
        "            Dense(8, activation = 'relu'),\n",
        "            Dense(4, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_3_bc'\n",
        "    )\n",
        "\n",
        "    models_bc = [model_1_bc, model_2_bc, model_3_bc]\n",
        "\n",
        "    return models_bc\n",
        "\n",
        "\n",
        "def prepare_dataset(filename):\n",
        "\n",
        "    data = np.loadtxt(filename, delimiter=\",\")\n",
        "\n",
        "    x = data[:,:-1]\n",
        "    y = data[:,-1]\n",
        "\n",
        "    # Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
        "    x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=80)\n",
        "\n",
        "    # Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "    x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=80)\n",
        "\n",
        "    return x_train, y_train, x_cv, y_cv, x_test, y_test\n",
        "\n",
        "def train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "    degrees = range(1,max_degree+1)\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for degree in degrees:\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model.fit(X_train_mapped_scaled, y_train )\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv)\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
        "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
        "    plt.xticks(degrees)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for reg_param in reg_params:\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model = Ridge(alpha=reg_param)\n",
        "        model.fit(X_train_mapped_scaled, y_train)\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv)\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    reg_params = [str(x) for x in reg_params]\n",
        "    plt.plot(reg_params, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(reg_params, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(reg_params, np.repeat(baseline, len(reg_params)), linestyle='--', label='baseline')\n",
        "    plt.title(\"lambda vs. train and CV MSEs\")\n",
        "    plt.xlabel(\"lambda\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_diff_datasets(model, files, max_degree=10, baseline=None):\n",
        "\n",
        "    for file in files:\n",
        "\n",
        "        x_train, y_train, x_cv, y_cv, x_test, y_test = prepare_dataset(file['filename'])\n",
        "\n",
        "        train_mses = []\n",
        "        cv_mses = []\n",
        "        models = []\n",
        "        scalers = []\n",
        "        degrees = range(1,max_degree+1)\n",
        "\n",
        "        # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "        for degree in degrees:\n",
        "\n",
        "            # Add polynomial features to the training set\n",
        "            poly = PolynomialFeatures(degree, include_bias=False)\n",
        "            X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "            # Scale the training set\n",
        "            scaler_poly = StandardScaler()\n",
        "            X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "            scalers.append(scaler_poly)\n",
        "\n",
        "            # Create and train the model\n",
        "            model.fit(X_train_mapped_scaled, y_train )\n",
        "            models.append(model)\n",
        "\n",
        "            # Compute the training MSE\n",
        "            yhat = model.predict(X_train_mapped_scaled)\n",
        "            train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "            train_mses.append(train_mse)\n",
        "\n",
        "            # Add polynomial features and scale the cross-validation set\n",
        "            poly = PolynomialFeatures(degree, include_bias=False)\n",
        "            X_cv_mapped = poly.fit_transform(x_cv)\n",
        "            X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "            # Compute the cross-validation MSE\n",
        "            yhat = model.predict(X_cv_mapped_scaled)\n",
        "            cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "            cv_mses.append(cv_mse)\n",
        "\n",
        "        # Plot the results\n",
        "        plt.plot(degrees, train_mses, marker='o', c='r', linestyle=file['linestyle'], label=f\"{file['label']} training MSEs\");\n",
        "        plt.plot(degrees, cv_mses, marker='o', c='b', linestyle=file['linestyle'], label=f\"{file['label']} CV MSEs\");\n",
        "\n",
        "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
        "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
        "    plt.xticks(degrees)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_learning_curve(model, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "    num_samples_train_and_cv = []\n",
        "    percents = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for percent in percents:\n",
        "\n",
        "        num_samples_train = round(len(x_train) * (percent/100.0))\n",
        "        num_samples_cv = round(len(x_cv) * (percent/100.0))\n",
        "        num_samples_train_and_cv.append(num_samples_train + num_samples_cv)\n",
        "\n",
        "        x_train_sub = x_train[:num_samples_train]\n",
        "        y_train_sub = y_train[:num_samples_train]\n",
        "        x_cv_sub = x_cv[:num_samples_cv]\n",
        "        y_cv_sub = y_cv[:num_samples_cv]\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train_sub)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model.fit(X_train_mapped_scaled, y_train_sub)\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train_sub, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv_sub)\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv_sub, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    plt.plot(num_samples_train_and_cv, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(num_samples_train_and_cv, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(num_samples_train_and_cv, np.repeat(baseline, len(percents)), linestyle='--', label='baseline')\n",
        "    plt.title(\"number of examples vs. train and CV MSEs\")\n",
        "    plt.xlabel(\"total number of training and cv examples\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6nFAV60Nl9Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "h-0aj0uuk7TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.array([[1,2]])\n",
        "Y = np.array([[2,3]]).T\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print('*'*10)\n",
        "Z = np.ones([1,2])\n",
        "print(Z*X)\n",
        "print(Z*Y)\n",
        "print(Z@Y)\n",
        "np.dot(Z,Y)\n"
      ],
      "metadata": {
        "id": "gbodwHvs83T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1,2]])\n",
        "w = np.array([[1,2,3], [4,5,6]])\n",
        "b = np.array([1,2,3])\n",
        "x @ w + b    # np.matmul(x,w)+b does the same"
      ],
      "metadata": {
        "id": "BrXkhyOqlFr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Implementation of matplotlib function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "dx, dy = 0.015, 0.05\n",
        "y, x = np.mgrid[slice(-4, 4 + dy, dy),\n",
        "                slice(-4, 4 + dx, dx)]\n",
        "z = (1 - x / 3. + x ** 5 + y ** 5) * np.exp(-x ** 2 - y ** 2)\n",
        "z = z[:-1, :-1]\n",
        "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
        "\n",
        "plt.imshow(y, cmap ='Reds')\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XA5QKjFcm3UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'a':list(np.random.randint(0,10,100))})\n",
        "arr = df.to_numpy()\n",
        "arr.reshape((5,20))"
      ],
      "metadata": {
        "id": "JzYBwDk-gaqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'a':list(np.random.randint(0,255,4))})\n",
        "arr = df.to_numpy()\n",
        "arr = arr.reshape((2,2))\n",
        "plt.imshow(arr, cmap = 'Greys')"
      ],
      "metadata": {
        "id": "zvvkdy9xcbYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(np.random.randint(0,255,400)/100)"
      ],
      "metadata": {
        "id": "6JwBsfvPBBKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "givenString = \"TutorialsPoint\"\n",
        "my_list = []\n",
        "your_list = []\n",
        "\n",
        "for i in range(0, 20):\n",
        "  my_list.append(random.randint(0, 9))\n",
        "\n",
        "print(my_list)\n",
        "\n",
        "for item in my_list:\n",
        "  if item == 0 or item == 1:\n",
        "    print(\"yes\")\n",
        "  elif item == 2 or item == 3:\n",
        "    print(\"No\")\n",
        "  else:\n",
        "    print(\"NA\")"
      ],
      "metadata": {
        "id": "hNm2VHC2IN8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# input string\n",
        "givenString = \"ABCDEFGHIJ\"\n",
        "\n",
        "# generating 3 random items from the String using random.sample() method\n",
        "randomItems = random.sample(givenString, 10)\n",
        "print(randomItems)\n",
        "\n",
        "for item in randomItems:\n",
        "  if item == 'A' or item == 'B':\n",
        "    print(\"yes\")\n",
        "  elif item == 'C' or item == 'D':\n",
        "    print(\"No\")\n",
        "  else:\n",
        "    print(\"NA\")"
      ],
      "metadata": {
        "id": "8z5hWXHEPrxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saple_id = {'a1b':1, 'b1b':2, 'c1b':3}\n",
        "saple_id\n",
        "\n",
        "my_list_2 = [1, 2, 3, 4, 3, 2, 1]\n",
        "\n",
        "my_list_1 = ['A', 'B', 'C']\n",
        "\n",
        "unique(my_list_2)"
      ],
      "metadata": {
        "id": "HYFzOP7Md5Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "a = np.array(['a', 'b', 'c', 'a'])\n",
        "\n",
        "\n",
        "print(a == 'a')\n",
        "\n",
        "a[a == 'a'] = 'AAA'\n",
        "\n",
        "a\n",
        "\n"
      ],
      "metadata": {
        "id": "gdXLKuz4jvY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}