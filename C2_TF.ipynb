{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mq53ARLRHHfj",
        "Eg9FXGzsBqLh",
        "pupmi3GAHWJ1",
        "UZaUyq91Kn4u",
        "iIyAtOK90QDj",
        "KVIsV85L0wUj",
        "NLWZPcD-16rl",
        "Oz_KHhZ-DetU",
        "uFbIY43n4HUD",
        "fNHHRj4GjzZT",
        "iJ1KOHMd-gDO",
        "TPrFTRGcCXk3",
        "STSmhw4qkyD3",
        "bG-7WxiMl8hI",
        "UeyOn8wCtxGS",
        "h-0aj0uuk7TV"
      ],
      "authorship_tag": "ABX9TyMHNdqUBXC3+ZytBPZzHtYv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HomayounfarM/machine-learning-andrew-ng/blob/main/C2_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W1_Lab01_TF"
      ],
      "metadata": {
        "id": "mq53ARLRHHfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Tensorflow and Keras\n",
        "Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by FranÃ§ois Chollet that creates a simple, layer-centric interface to Tensorflow.\n",
        "The following example shows to use tersorflow and Keras to create a layer. The first layer includes three cells (units). The second layer infludes one cell.\n",
        "\n",
        "In the following example, we have two layers: the first layer includes three unit (cell) and the second layer includes one unit (cell).\n",
        "Our x can be an array with any length. then, w will be an array with the same length and the inner production of these two array plus a 'b' wil be assigned to the first cell of that layer."
      ],
      "metadata": {
        "id": "Eg9FXGzsBqLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7eVKTN4__8Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "#from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
        "#from tensorflow.keras.activations import sigmoid\n",
        "#import matplotlib.colors as colors\n",
        "\n",
        "x = np.array([[200, 1, 4, 6, 7, 8]], dtype = float)\n",
        "\n",
        "print(type(x))\n",
        "print(x.shape)\n",
        "print(x.dtype)\n",
        "\n",
        "layer_1 = tf.keras.layers.Dense(units = 3, activation = 'sigmoid')\n",
        "a1 = layer_1(x)\n",
        "print(a1.numpy())\n",
        "print(a1)\n",
        "\n",
        "layer_2 = Dense(units = 1, activation = 'sigmoid')\n",
        "a2 = layer_2(a1)\n",
        "\n",
        "print(a2.numpy())    # use .numpy() to convert the tensorflow to an np.array\n",
        "\n",
        "if a2 > 0.5:\n",
        "    yhat = 1\n",
        "else:\n",
        "    yhat = 0\n",
        "\n",
        "print(yhat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "X_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\n",
        "Y_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\n",
        "\n",
        "pos = Y_train == 1\n",
        "neg = Y_train == 0\n",
        "X_train[pos]\n",
        "\n",
        "pos = Y_train == 1\n",
        "neg = Y_train == 0\n",
        "\n",
        "\n",
        "dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC')\n",
        "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
        "ax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "ax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',\n",
        "              edgecolors=dlc[\"dlmagenta\"],lw=3)\n",
        "\n",
        "ax.set_ylim(-0.08,1.1)\n",
        "ax.set_ylabel('y', fontsize=12)\n",
        "ax.set_xlabel('x', fontsize=12)\n",
        "ax.set_title('one variable plot')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hDy7DrsbCIUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Neuron\n",
        "\n",
        "We can implement a 'logistic neuron' by adding a sigmoid activation. The function of the neuron is then described by (1).  \n",
        "\n",
        "$$ f_{\\mathbf{w},b}(x^{(i)}) = g(\\mathbf{w}x^{(i)} + b) \\tag{1}$$\n",
        "where $$g(x) = sigmoid(x)$$\n",
        "\n",
        "Let's set $w$ and $b$ to some known values and check the model.\n",
        "\n",
        "This section will create a Tensorflow Model that contains our logistic layer to demonstrate an alternate method of creating models. Tensorflow is most often used to create multi-layer models. The [Sequential](https://keras.io/guides/sequential_model/) model is a convenient means of constructing these models.\n",
        "We can implement a 'logistic'\n",
        "\n"
      ],
      "metadata": {
        "id": "8eajVEDoDH6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Rvy3-4v_Dhdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model.summary()` shows the layers and number of parameters in the model. There is only one layer in this model and that layer has only one unit. The unit has two parameters, $w$ and $b$.\n",
        "The first term in the 'Sequential function' determine the number of cells (units) in a layer. The second term, 'input_dim', is to determine the dimention of input to this layer.  "
      ],
      "metadata": {
        "id": "FS_AQ-2yD863"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "KvHWvdOsECw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W and b are randomly assigned. We need to train the model to find the best values for  and b"
      ],
      "metadata": {
        "id": "Yboq3i8WwBmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_layer = model.get_layer('L1')\n",
        "w,b = logistic_layer.get_weights()\n",
        "print(w,b)\n",
        "print(w.shape,b.shape)"
      ],
      "metadata": {
        "id": "EnXcZg4wED5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set the weight and bias to some known values."
      ],
      "metadata": {
        "id": "-7LVRJoREM2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I1uR3XLTEM1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_w = np.array([[2]])\n",
        "set_b = np.array([-4.5])\n",
        "# set_weights takes a list of numpy arrays\n",
        "logistic_layer.set_weights([set_w, set_b])\n",
        "print(logistic_layer.get_weights())"
      ],
      "metadata": {
        "id": "wME2GEXFEL92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare equation (2) to the layer output."
      ],
      "metadata": {
        "id": "GBh9fB-tEVYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = model.predict(X_train[0].reshape(1,1))\n",
        "print(a1)\n"
      ],
      "metadata": {
        "id": "oWU6UugGEWHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The followings are the functions used above"
      ],
      "metadata": {
        "id": "raaKGXU8FPOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoidnp(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    z : array_like\n",
        "        A scalar or numpy array of any size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "     g : array_like\n",
        "         sigmoid(z)\n",
        "    \"\"\"\n",
        "    z = np.clip( z, -500, 500 )           # protect against overflow\n",
        "    g = 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "VEqu_euWEsM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_logistic(X_train, Y_train, model, set_w, set_b, pos, neg):\n",
        "    fig,ax = plt.subplots(1,2,figsize=(16,4))\n",
        "\n",
        "    layerf= lambda x : model.predict(x)\n",
        "    plt_prob_1d(ax[0], layerf)\n",
        "\n",
        "    ax[0].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "    ax[0].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',\n",
        "                  edgecolors=dlc[\"dlblue\"],lw=3)\n",
        "\n",
        "    ax[0].set_ylim(-0.08,1.1)\n",
        "    ax[0].set_xlim(-0.5,5.5)\n",
        "    ax[0].set_ylabel('y', fontsize=16)\n",
        "    ax[0].set_xlabel('x', fontsize=16)\n",
        "    ax[0].set_title('Tensorflow Model', fontsize=20)\n",
        "    ax[0].legend(fontsize=16)\n",
        "\n",
        "    layerf= lambda x : sigmoidnp(np.dot(set_w,x.reshape(1,1)) + set_b)\n",
        "    plt_prob_1d(ax[1], layerf)\n",
        "\n",
        "    ax[1].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "    ax[1].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',\n",
        "                  edgecolors=dlc[\"dlblue\"],lw=3)\n",
        "\n",
        "    ax[1].set_ylim(-0.08,1.1)\n",
        "    ax[1].set_xlim(-0.5,5.5)\n",
        "    ax[1].set_ylabel('y', fontsize=16)\n",
        "    ax[1].set_xlabel('x', fontsize=16)\n",
        "    ax[1].set_title('Numpy Model', fontsize=20)\n",
        "    ax[1].legend(fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BuSuJLwYEyoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_prob_1d(ax,fwb):\n",
        "    \"\"\" plots a decision boundary but include shading to indicate the probability \"\"\"\n",
        "    #setup useful ranges and common linspaces\n",
        "    x_space  = np.linspace(0, 5 , 50)\n",
        "    y_space  = np.linspace(0, 1 , 50)\n",
        "\n",
        "    # get probability for x range, extend to y\n",
        "    z = np.zeros((len(x_space),len(y_space)))\n",
        "    for i in range(len(x_space)):\n",
        "        x = np.array([[x_space[i]]])\n",
        "        z[:,i] = fwb(x)\n",
        "\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "    new_cmap = truncate_colormap(cmap, 0.0, 0.5)\n",
        "    pcm = ax.pcolormesh(x_space, y_space, z,\n",
        "                   norm=cm.colors.Normalize(vmin=0, vmax=1),\n",
        "                   cmap=new_cmap, shading='nearest', alpha = 0.9)\n",
        "    ax.figure.colorbar(pcm, ax=ax)\n",
        "\n",
        "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
        "    \"\"\" truncates color map \"\"\"\n",
        "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
        "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
        "        cmap(np.linspace(minval, maxval, n)))\n",
        "    return new_cmap"
      ],
      "metadata": {
        "id": "e4Cxz4odE1fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W1_Lab02_TF"
      ],
      "metadata": {
        "id": "pupmi3GAHWJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataSet"
      ],
      "metadata": {
        "id": "UZaUyq91Kn4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = load_coffee_data();\n",
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "id": "1UDAe7B8HkrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize Data\n",
        "Fitting the weights to the data (back-propagation, covered in next week's lectures) will proceed more quickly if the data is normalized. This is the same procedure you used in Course 1 where features in the data are each normalized to have a similar range.\n",
        "The procedure below uses a Keras [normalization layer](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/). It has the following steps:\n",
        "- create a \"Normalization Layer\". Note, as applied here, this is not a layer in your model.\n",
        "- 'adapt' the data. This learns the mean and variance of the data set and saves the values internally.\n",
        "- normalize the data.  \n",
        "It is important to apply normalization to any future data that utilizes the learned model."
      ],
      "metadata": {
        "id": "FF3ACvgLMpX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\")\n",
        "print(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\")\n",
        "norm_l = tf.keras.layers.Normalization(axis=-1)    # create a \"Normalization Layer\"\n",
        "norm_l.adapt(X)  # learns mean, variance\n",
        "Xn = norm_l(X)    # normalize the data.\n",
        "print(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\")\n",
        "print(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\")"
      ],
      "metadata": {
        "id": "Vrn_POzZL61V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tile/copy our data to increase the training set size and reduce the number of training epochs."
      ],
      "metadata": {
        "id": "sDvnh2OsM93l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xt = np.tile(Xn,(1000,1))\n",
        "Yt= np.tile(Y,(1000,1))\n",
        "print(Xt.shape, Yt.shape)"
      ],
      "metadata": {
        "id": "oOEQ1ke7NCg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Model"
      ],
      "metadata": {
        "id": "hqUMM7S-PpBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build the \"Coffee Roasting Network\". There are two layers with sigmoid activations as shown below:"
      ],
      "metadata": {
        "id": "l7QYMVyKPvjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(2,)),\n",
        "        Dense(3, activation='sigmoid', name = 'layer1'),\n",
        "        Dense(1, activation='sigmoid', name = 'layer2')\n",
        "     ]\n",
        ")"
      ],
      "metadata": {
        "id": "fvo2onrTP0sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer has six w and three b. The second layer has three w and one b.\n"
      ],
      "metadata": {
        "id": "74x2AdOCQcR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Note 1:** The `tf.keras.Input(shape=(2,)),` specifies the expected shape of the input. This allows Tensorflow to size the weights and bias parameters at this point.  This is useful when exploring Tensorflow models. This statement can be omitted in practice and Tensorflow will size the network parameters when the input data is specified in the `model.fit` statement.  \n",
        ">**Note 2:** Including the sigmoid activation in the final layer is not considered best practice. It would instead be accounted for in the loss which improves numerical stability. This will be described in more detail in a later lab.\n",
        "\n",
        "The `model.summary()` provides a description of the network:"
      ],
      "metadata": {
        "id": "Sc_hdtFKRAtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Cjs9GECeQDVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine the weights and biases Tensorflow has instantiated.  The weights $W$ should be of size (number of features in input, number of units in the layer) while the bias $b$ size should match the number of units in the layer:\n",
        "- In the first layer with 3 units, we expect W to have a size of (2,3) and $b$ should have 3 elements.\n",
        "- In the second layer with 1 unit, we expect W to have a size of (3,1) and $b$ should have 1 element."
      ],
      "metadata": {
        "id": "kFQr__rfS1ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
        "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)"
      ],
      "metadata": {
        "id": "p8OftCbsS8oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following statements will be described in detail later. For now:\n",
        "- The `model.compile` statement defines a loss function and specifies a compile optimization.\n",
        "- The `model.fit` statement runs gradient descent and fits the weights to the data."
      ],
      "metadata": {
        "id": "0z8-t2emTDui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    Xt,Yt,\n",
        "    epochs=10,\n",
        ")"
      ],
      "metadata": {
        "id": "gNa8v4EdS8Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Updated Weights\n",
        "After fitting, the weights have been updated:"
      ],
      "metadata": {
        "id": "zBsAmqw2VMdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
        "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
      ],
      "metadata": {
        "id": "0jFNEyHYVPFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by creating input data. The model is expecting one or more examples where examples are in the rows of matrix. In this case, we have two features so the matrix will be (m,2) where m is the number of examples.\n",
        "Recall, we have normalized the input features so we must normalize our test data as well.   \n",
        "To make a prediction, you apply the `predict` method."
      ],
      "metadata": {
        "id": "pdICQ-bZXhmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array([\n",
        "    [200,13.9],  # postive example\n",
        "    [200,17]])   # negative example\n",
        "X_testn = norm_l(X_test)\n",
        "predictions = model.predict(X_testn)\n",
        "print(\"predictions = \\n\", predictions)"
      ],
      "metadata": {
        "id": "k1ao7C3CXknh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Epochs and batches\n",
        "In the `compile` statement above, the number of `epochs` was set to 10. This specifies that the entire data set should be applied during training 10 times.  During training, you see output describing the progress of training that looks like this:\n",
        "```\n",
        "Epoch 1/10\n",
        "6250/6250 [==============================] - 6s 910us/step - loss: 0.1782\n",
        "```\n",
        "The first line, `Epoch 1/10`, describes which epoch the model is currently running. For efficiency, the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line `6250/6250 [====` is describing which batch has been executed."
      ],
      "metadata": {
        "id": "OtR3H_CEX61v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert the probabilities to a decision, we apply a threshold:"
      ],
      "metadata": {
        "id": "atV6VDDWYAgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = np.zeros_like(predictions)\n",
        "for i in range(len(predictions)):\n",
        "    if predictions[i] >= 0.5:\n",
        "        yhat[i] = 1\n",
        "    else:\n",
        "        yhat[i] = 0\n",
        "print(f\"decisions = \\n{yhat}\")"
      ],
      "metadata": {
        "id": "fTtz2kPRYHin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be accomplished more succinctly:"
      ],
      "metadata": {
        "id": "IfavVteuYK9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = (predictions >= 0.5).astype(int)\n",
        "print(f\"decisions = \\n{yhat}\")"
      ],
      "metadata": {
        "id": "z2kbGpBkYQ1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###These are the functions we use above:"
      ],
      "metadata": {
        "id": "RbqgWk6mI76Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_coffee_data():\n",
        "    \"\"\" Creates a coffee roasting data set.\n",
        "        roasting duration: 12-15 minutes is best\n",
        "        temperature range: 175-260C is best\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(2)\n",
        "    X = rng.random(400).reshape(-1,2)\n",
        "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
        "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
        "    Y = np.zeros(len(X))\n",
        "\n",
        "    i=0\n",
        "    for t,d in X:\n",
        "        y = -3/(260-175)*t + 21\n",
        "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
        "            Y[i] = 1\n",
        "        else:\n",
        "            Y[i] = 0\n",
        "        i += 1\n",
        "\n",
        "    return (X, Y.reshape(-1,1))"
      ],
      "metadata": {
        "id": "Vi_j45jrI7iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Y0hpWLwI6iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W1_Lab03_TF"
      ],
      "metadata": {
        "id": "iIyAtOK90QDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lab: Neural Networks for Handwritten Digit Recognition, BinaryÂ¶\n",
        "In this exercise, you will use a neural network to recognize the hand-written digits zero and one."
      ],
      "metadata": {
        "id": "KVIsV85L0wUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from autils import *\n",
        "#%matplotlib inline\n",
        "\n",
        "#import logging\n",
        "#logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "#tf.autograph.set_verbosity(0)"
      ],
      "metadata": {
        "id": "CDd2Pblm0iEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "UD7IDFqZ_4Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem Statement\n",
        "In this exercise, you will use a neural network to recognize two handwritten digits, zero and one. This is a binary classification task. Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks. You will extend this network to recognize all 10 digits (0-9) in a future assignment.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "You will start by loading the dataset for this task.\n",
        "- The `load_data()` function shown below loads the data into variables `X` and `y`\n",
        "\n",
        "\n",
        "- The data set contains 1000 training examples of handwritten digits $^1$, here limited to zero and one.  \n",
        "\n",
        "    - Each training example is a 20-pixel x 20-pixel grayscale image of the digit.\n",
        "        - Each pixel is represented by a floating-point number indicating the grayscale intensity at that location.\n",
        "        - The 20 by 20 grid of pixels is âunrolledâ into a 400-dimensional vector.\n",
        "        - Each training example becomes a single row in our data matrix `X`.\n",
        "        - This gives us a 1000 x 400 matrix `X` where every row is a training example of a handwritten digit image.\n",
        "\n",
        "$$X =\n",
        "\\left(\\begin{array}{cc}\n",
        "--- (x^{(1)}) --- \\\\\n",
        "--- (x^{(2)}) --- \\\\\n",
        "\\vdots \\\\\n",
        "--- (x^{(m)}) ---\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "- The second part of the training set is a 1000 x 1 dimensional vector `y` that contains labels for the training set\n",
        "    - `y = 0` if the image is of the digit `0`, `y = 1` if the image is of the digit `1`.\n",
        "\n",
        "$^1$<sub> This is a subset of the MNIST handwritten digit dataset (http://yann.lecun.com/exdb/mnist/)</sub>"
      ],
      "metadata": {
        "id": "NLWZPcD-16rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "link = 'https://drive.google.com/file/d/1iR3DzNzotfp6y4MWjr2MJJWIvaE527xT/view?usp=share_link'\n",
        "link1 = 'https://drive.google.com/file/d/1PTtDlUkJ3TatNpaSiIb2t98uIs3B4_Da/view?usp=share_link'\n",
        "\n",
        "# to get the id part of the file\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('data.csv')\n",
        "df = pd.read_csv('data.csv', index_col=None)\n",
        "df = df.drop('Unnamed: 0', axis=1)\n",
        "X = df.to_numpy()\n",
        "\n",
        "id1 = link1.split(\"/\")[-2]\n",
        "downloaded1 = drive.CreateFile({'id':id1})\n",
        "downloaded1.GetContentFile('data1.csv')\n",
        "df1 = pd.read_csv('data1.csv', index_col=None)\n",
        "df1 = df1.drop('Unnamed: 0', axis=1)\n",
        "y = df1.to_numpy()"
      ],
      "metadata": {
        "id": "Rg-Du-Ro0iBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing the Data\n",
        "\n",
        "You will begin by visualizing a subset of the training set.\n",
        "- In the cell below, the code randomly selects 64 rows from `X`, maps each row back to a 20 pixel by 20 pixel grayscale image and displays the images together.\n",
        "- The label for each image is displayed above the image"
      ],
      "metadata": {
        "id": "HwyQLc50_tS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "# You do not need to modify anything in this cell\n",
        "\n",
        "m, n = X.shape\n",
        "\n",
        "fig, axes = plt.subplots(8,8, figsize=(8,8))\n",
        "fig.tight_layout(pad=0.1)\n",
        "\n",
        "for i,ax in enumerate(axes.flat):\n",
        "    # Select random indices\n",
        "    random_index = np.random.randint(m)\n",
        "\n",
        "    # Select rows corresponding to the random indices and\n",
        "    # reshape the image\n",
        "    X_random_reshaped = X[random_index].reshape((20,20)).T\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(X_random_reshaped, cmap='Greys')\n",
        "\n",
        "    # Display the label above the image\n",
        "    ax.set_title(y[random_index,0])\n",
        "    ax.set_axis_off()"
      ],
      "metadata": {
        "id": "8g3mFpfy_C4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model representation\n",
        "\n",
        "The neural network you will use in this assignment is shown in the figure below.\n",
        "- This has three dense layers with sigmoid activations.\n",
        "    - Recall that our inputs are pixel values of digit images.\n",
        "    - Since the images are of size 20 Ã 20, this gives us 400 inputs  \n",
        "    \n",
        "![picture](https://drive.google.com/uc?export=view&id=1UfjXiLEWsYCl8fvizkflk3SXxey2y7Le)\n",
        "\n"
      ],
      "metadata": {
        "id": "Oz_KHhZ-DetU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The parameters have dimensions that are sized for a neural network with $25$ units in layer 1, $15$ units in layer 2 and $1$ output unit in layer 3.\n",
        "\n",
        "    - Recall that the dimensions of these parameters are determined as follows:\n",
        "        - If network has $s_{in}$ units in a layer and $s_{out}$ units in the next layer, then\n",
        "            - $W$ will be of dimension $s_{in} \\times s_{out}$.\n",
        "            - $b$ will a vector with $s_{out}$ elements\n",
        "  \n",
        "    - Therefore, the shapes of `W`, and `b`,  are\n",
        "        - layer1: The shape of `W1` is (400, 25) and the shape of `b1` is (25,)\n",
        "        - layer2: The shape of `W2` is (25, 15) and the shape of `b2` is: (15,)\n",
        "        - layer3: The shape of `W3` is (15, 1) and the shape of `b3` is: (1,)\n",
        ">**Note:** The bias vector `b` could be represented as a 1-D (n,) or 2-D (1,n) array. Tensorflow utilizes a 1-D representation and this lab will maintain that convention."
      ],
      "metadata": {
        "id": "UZ_GmVxuIQi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Model Implementation"
      ],
      "metadata": {
        "id": "CsFI4KmIKUXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow models are built layer by layer. A layer's input dimensions ($s_{in}$ above) are calculated for you. You specify a layer's *output dimensions* and this determines the next layer's input dimension. The input dimension of the first layer is derived from the size of the input data specified in the `model.fit` statement below.\n",
        ">**Note:** It is also possible to add an input layer that specifies the input dimension of the first layer. For example:  \n",
        "`tf.keras.Input(shape=(400,)),    #specify input shape`  \n",
        "We will include that here to illuminate some model sizing."
      ],
      "metadata": {
        "id": "0ax8vqcLKY92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(400,)),    #specify input size\n",
        "        ### START CODE HERE ###\n",
        "        tf.keras.layers.Dense(25, input_dim=400,  activation = 'sigmoid', name='L1'),\n",
        "        tf.keras.layers.Dense(15, input_dim=25,  activation = 'sigmoid', name='L2'),\n",
        "        tf.keras.layers.Dense(1, input_dim=15,  activation = 'sigmoid', name='L3')\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "    ], name = \"my_model\"\n",
        ")"
      ],
      "metadata": {
        "id": "qYzzYbkL4JhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "YIReF-Ol4JeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can examine details of the model by first extracting the layers with model.layers and then extracting the weights with layerx.get_weights() as shown below."
      ],
      "metadata": {
        "id": "O-3ikM3W_CWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[layer1, layer2, layer3] = model.layers\n",
        "#### Examine Weights shapes\n",
        "W1,b1 = layer1.get_weights()\n",
        "W2,b2 = layer2.get_weights()\n",
        "W3,b3 = layer3.get_weights()\n",
        "print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
        "print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\n",
        "print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")"
      ],
      "metadata": {
        "id": "DlxLGAcA4JbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "xx.get_weights returns a NumPy array. One can also access the weights directly in their tensor form. Note the shape of the tensors in the final layer."
      ],
      "metadata": {
        "id": "vitR-MlKArJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layers[2].weights)"
      ],
      "metadata": {
        "id": "0PMOzPkE4JYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will define a loss function and run gradient descent to fit the weights of the model to the training data. This will be explained in more detail later."
      ],
      "metadata": {
        "id": "8kkilKO5A-ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X,y,\n",
        "    epochs=20\n",
        ")"
      ],
      "metadata": {
        "id": "mk4AmlQX4JV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the coefficients after trainig thwe model\n",
        "print(model.layers[2].weights)"
      ],
      "metadata": {
        "id": "1FnhlFVvBrim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the model on an example to make a prediction, use Keras predict. The input to predict is an array so the single example is reshaped to be two dimensional."
      ],
      "metadata": {
        "id": "eDOUAIatCQ8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(X[0].reshape(1,400))  # a zero\n",
        "print(f\" predicting a zero: {prediction}\")\n",
        "prediction = model.predict(X[500].reshape(1,400))  # a one\n",
        "print(f\" predicting a one:  {prediction}\")"
      ],
      "metadata": {
        "id": "WsR0us93Brfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the model is interpreted as a probability. In the first example above, the input is a zero. The model predicts the probability that the input is a one is nearly zero. In the second example, the input is a one. The model predicts the probability that the input is a one is nearly one. As in the case of logistic regression, the probability is compared to a threshold to make a final prediction."
      ],
      "metadata": {
        "id": "967HllniCarv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if prediction >= 0.5:\n",
        "    yhat = 1\n",
        "else:\n",
        "    yhat = 0\n",
        "print(f\"prediction after threshold: {yhat}\")"
      ],
      "metadata": {
        "id": "dpWQlRB4Brcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the predictions vs the labels for a random sample of 64 digits. This takes a moment to run."
      ],
      "metadata": {
        "id": "Fyt8q6ynC5ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "# You do not need to modify anything in this cell\n",
        "\n",
        "m, n = X.shape\n",
        "\n",
        "fig, axes = plt.subplots(8,8, figsize=(8,8))\n",
        "fig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]\n",
        "\n",
        "for i,ax in enumerate(axes.flat):\n",
        "    # Select random indices\n",
        "    random_index = np.random.randint(m)\n",
        "\n",
        "    # Select rows corresponding to the random indices and\n",
        "    # reshape the image\n",
        "    X_random_reshaped = X[random_index].reshape((20,20)).T\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(X_random_reshaped, cmap='gray')\n",
        "\n",
        "    # Predict using the Neural Network\n",
        "    prediction = model.predict(X[random_index].reshape(1,400))\n",
        "    if prediction >= 0.5:\n",
        "        yhat = 1\n",
        "    else:\n",
        "        yhat = 0\n",
        "\n",
        "    # Display the label above the image\n",
        "    ax.set_title(f\"{y[random_index,0]},{yhat}\")\n",
        "    ax.set_axis_off()\n",
        "fig.suptitle(\"Label, yhat\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hWb2iF4NBrZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLaEyzJXBrWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kg97oXlpBrT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###These are the functions we use above:"
      ],
      "metadata": {
        "id": "uFbIY43n4HUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "    X = np.load(\"data/X.npy\")\n",
        "    y = np.load(\"data/y.npy\")\n",
        "    X = X[0:1000]\n",
        "    y = y[0:1000]\n",
        "    return X, y\n",
        "\n",
        "def load_weights():\n",
        "    w1 = np.load(\"data/w1.npy\")\n",
        "    b1 = np.load(\"data/b1.npy\")\n",
        "    w2 = np.load(\"data/w2.npy\")\n",
        "    b2 = np.load(\"data/b2.npy\")\n",
        "    return w1, b1, w2, b2\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1. / (1. + np.exp(-x))"
      ],
      "metadata": {
        "id": "q9JOmyu70hwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W2_Lab04_TF (Multiclass)"
      ],
      "metadata": {
        "id": "fNHHRj4GjzZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Lab - Multi-class Classification"
      ],
      "metadata": {
        "id": "uSWbifRxkzuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib widget\n",
        "from sklearn.datasets import make_blobs\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n"
      ],
      "metadata": {
        "id": "CC-DPvaukq8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare and visualize our data\n",
        "We will use Scikit-Learn make_blobs function to make a training data set with 4 categories as shown in the plot below."
      ],
      "metadata": {
        "id": "lLiEQyZIlKSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make 4-class dataset for classification\n",
        "classes = 4\n",
        "m = 100\n",
        "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
        "std = 1.0\n",
        "X_train, y_train = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=30)"
      ],
      "metadata": {
        "id": "DjTM9IyilJxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_mc(X_train,y_train,classes, centers, std=std)"
      ],
      "metadata": {
        "id": "fnrk3vs8lY8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each dot represents a training example. The axis (x0,x1) are the inputs and the color represents the class the example is associated with. Once trained, the model will be presented with a new example, (x0,x1), and will predict the class.\n",
        "\n",
        "While generated, this data set is representative of many real-world classification problems. There are several input features (x0,...,xn) and several output categories. The model is trained to use the input features to predict the correct output category."
      ],
      "metadata": {
        "id": "aVpaYnmynw9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "This lab will use a 2-layer network as shown. Unlike the binary classification networks, this network has four outputs, one for each class. Given an input example, the output with the highest value is the predicted class of the input.\n",
        "Below is an example of how to construct this network in Tensorflow. Notice the output layer uses a linear rather than a softmax activation. While it is possible to include the softmax in the output layer, it is more numerically stable if linear outputs are passed to the loss function during training. If the model is used to predict probabilities, the softmax can be applied at that point.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=12PG4uzZM4dr40M7WQ2jt1gehJhjzAccz)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F1S2mEuqTteM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(2, activation = 'relu',   name = \"L1\"),\n",
        "        Dense(4, activation = 'linear', name = \"L2\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "6qYPtSkjl131"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statements below compile and train the network. Setting from_logits=True as an argument to the loss function specifies that the output activation was linear rather than a softmax."
      ],
      "metadata": {
        "id": "MnsCBHNFVWuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(0.01),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=200\n",
        ")"
      ],
      "metadata": {
        "id": "LZCms9Xkl109"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can see that we have two X's, so, in the first layer, we have two w's and one b for each unit. We have two unit in the first layer. So, we have 6 parameters there. In the secon layer, we have 4 units, therefor, we have 12 parameters."
      ],
      "metadata": {
        "id": "sOZM6ZDhT_wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "aQD3UN-cT-nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the model trained, we can see how the model has classified the training data."
      ],
      "metadata": {
        "id": "oYq3A1tN5SSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt_cat_mc(X_train, y_train, model, classes)"
      ],
      "metadata": {
        "id": "cTYUoQdyl1yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, the decision boundaries show how the model has partitioned the input space. This very simple model has had no trouble classifying the training data. How did it accomplish this? Let's look at the network in more detail.\n",
        "\n",
        "Below, we will pull the trained weights from the model and use that to plot the function of each of the network units. Further down, there is a more detailed explanation of the results. You don't need to know these details to successfully use neural networks, but it may be helpful to gain more intuition about how the layers combine to solve a classification problem."
      ],
      "metadata": {
        "id": "qaEiS5mSVg6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gather the trained parameters from the first layer\n",
        "l1 = model.get_layer(\"L1\")\n",
        "W1,b1 = l1.get_weights()"
      ],
      "metadata": {
        "id": "HfupL5FNl1vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the function of the first layer\n",
        "plt_layer_relu(X_train, y_train.reshape(-1,), W1, b1, classes)"
      ],
      "metadata": {
        "id": "w9OWbbBEVqVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "Layer 1\n",
        "These plots show the function of Units 0 and 1 in the first layer of the network. The inputs are ( ð¥0,ð¥1\n",
        " ) on the axis. The output of the unit is represented by the color of the background. This is indicated by the color bar on the right of each graph. Notice that since these units are using a ReLu, the outputs do not necessarily fall between 0 and 1 and in this case are greater than 20 at their peaks. The contour lines in this graph show the transition point between the output,  ð[1]ð\n",
        "  being zero and non-zero. Recall the graph for a ReLu : The contour line in the graph is the inflection point in the ReLu.\n",
        "\n",
        "Unit 0 has separated classes 0 and 1 from classes 2 and 3. Points to the left of the line (classes 0 and 1) will output zero, while points to the right will output a value greater than zero.\n",
        "Unit 1 has separated classes 0 and 2 from classes 1 and 3. Points above the line (classes 0 and 2 ) will output a zero, while points below will output a value greater than zero. Let's see how this works out in the next layer!"
      ],
      "metadata": {
        "id": "6mkghqoAPLEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gather the trained parameters from the output layer\n",
        "l2 = model.get_layer(\"L2\")\n",
        "W2, b2 = l2.get_weights()\n",
        "# create the 'new features', the training examples after L1 transformation\n",
        "Xl2 = np.maximum(0, np.dot(X_train,W1) + b1)\n",
        "\n",
        "plt_output_layer_linear(Xl2, y_train.reshape(-1,), W2, b2, classes,\n",
        "                        x0_rng = (-0.25,np.amax(Xl2[:,0])), x1_rng = (-0.25,np.amax(Xl2[:,1])))"
      ],
      "metadata": {
        "id": "MoWyDpHUXyYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer 2, the output layer\n",
        "The dots in these graphs are the training examples translated by the first layer. One way to think of this is the first layer has created a new set of features for evaluation by the 2nd layer. The axes in these plots are the outputs of the previous layer  ð[1]0\n",
        "  and  ð[1]1\n",
        " . As predicted above, classes 0 and 1 (blue and green) have  ð[1]0=0\n",
        "  while classes 0 and 2 (blue and orange) have  ð[1]1=0\n",
        " .\n",
        "Once again, the intensity of the background color indicates the highest values.\n",
        "Unit 0 will produce its maximum value for values near (0,0), where class 0 (blue) has been mapped.\n",
        "Unit 1 produces its highest values in the upper left corner selecting class 1 (green).\n",
        "Unit 2 targets the lower right corner where class 2 (orange) resides.\n",
        "Unit 3 produces its highest values in the upper right selecting our final class (purple).\n",
        "\n",
        "One other aspect that is not obvious from the graphs is that the values have been coordinated between the units. It is not sufficient for a unit to produce a maximum value for the class it is selecting for, it must also be the highest value of all the units for points in that class. This is done by the implied softmax function that is part of the loss function (SparseCategoricalCrossEntropy). Unlike other activation functions, the softmax works across all the outputs."
      ],
      "metadata": {
        "id": "p55UHkkEPRWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the functions we use above:"
      ],
      "metadata": {
        "id": "VsY0G-9vl0tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import warnings\n",
        "from matplotlib import cm\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
        "import matplotlib.colors as colors\n",
        "from matplotlib import cm\n",
        "\n",
        "dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC')\n",
        "dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC'\n",
        "dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\n",
        "\n",
        "dkcolors = plt.cm.Paired((1,3,7,9,5,11))\n",
        "ltcolors = plt.cm.Paired((0,2,6,8,4,10))\n",
        "dkcolors_map = mpl.colors.ListedColormap(dkcolors)\n",
        "ltcolors_map = mpl.colors.ListedColormap(ltcolors)\n",
        "\n",
        "def plt_mc(X_train,y_train,classes, centers, std):\n",
        "    css = np.unique(y_train)\n",
        "    fig,ax = plt.subplots(1,1,figsize=(3,3))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "    plt_mc_data(ax, X_train,y_train,classes, map=dkcolors_map, legend=True, size=50, equal_xy = False)\n",
        "    ax.set_title(\"Multiclass Data\")\n",
        "    ax.set_xlabel(\"x0\")\n",
        "    ax.set_ylabel(\"x1\")\n",
        "    #for c in css:\n",
        "    #    circ = plt.Circle(centers[c], 2*std, color=dkcolors_map(c), clip_on=False, fill=False, lw=0.5)\n",
        "    #    ax.add_patch(circ)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plt_mc_data(ax, X, y, classes,  class_labels=None, map=plt.cm.Paired,\n",
        "                legend=False, size=50, m='o', equal_xy = False):\n",
        "    \"\"\" Plot multiclass data. Note, if equal_xy is True, setting ylim on the plot may not work \"\"\"\n",
        "    for i in range(classes):\n",
        "        idx = np.where(y == i)\n",
        "        col = len(idx[0])*[i]\n",
        "        label = class_labels[i] if class_labels else \"c{}\".format(i)\n",
        "        # this didn't work on coursera but did in local version\n",
        "        #ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n",
        "        #            c=col, vmin=0, vmax=map.N, cmap=map,\n",
        "        #            s=size, label=label)\n",
        "        ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n",
        "                    color=map(col), vmin=0, vmax=map.N,\n",
        "                    s=size, label=label)\n",
        "    if legend: ax.legend()\n",
        "    if equal_xy: ax.axis(\"equal\")\n",
        "\n",
        "def plt_cat_mc(X_train, y_train, model, classes):\n",
        "    #make a model for plotting routines to call\n",
        "    model_predict = lambda Xl: np.argmax(model.predict(Xl),axis=1)\n",
        "\n",
        "    fig,ax = plt.subplots(1,1, figsize=(3,3))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "\n",
        "    #add the original data to the decison boundary\n",
        "    plt_mc_data(ax, X_train,y_train, classes, map=dkcolors_map, legend=True)\n",
        "    #plot the decison boundary.\n",
        "    plot_cat_decision_boundary_mc(ax, X_train, model_predict, vector=True)\n",
        "    ax.set_title(\"model decision boundary\")\n",
        "\n",
        "    plt.xlabel(r'$x_0$');\n",
        "    plt.ylabel(r\"$x_1$\");\n",
        "    plt.show()\n",
        "\n",
        "def plot_cat_decision_boundary_mc(ax, X, predict , class_labels=None, legend=False, vector=True):\n",
        "\n",
        "    # create a mesh to points to plot\n",
        "    x_min, x_max = X[:, 0].min()- 0.5, X[:, 0].max()+0.5\n",
        "    y_min, y_max = X[:, 1].min()- 0.5, X[:, 1].max()+0.5\n",
        "    h = max(x_max-x_min, y_max-y_min)/100\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    #print(\"points\", points.shape)\n",
        "    #print(\"xx.shape\", xx.shape)\n",
        "\n",
        "    #make predictions for each point in mesh\n",
        "    if vector:\n",
        "        Z = predict(points)\n",
        "    else:\n",
        "        Z = np.zeros((len(points),))\n",
        "        for i in range(len(points)):\n",
        "            Z[i] = predict(points[i].reshape(1,2))\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    #contour plot highlights boundaries between values - classes in this case\n",
        "    ax.contour(xx, yy, Z, linewidths=1)\n",
        "    #ax.axis('tight')\n",
        "\n",
        "def plt_layer_relu(X, Y, W1, b1, classes):\n",
        "    nunits = (W1.shape[1])\n",
        "    Y = Y.reshape(-1,)\n",
        "    fig,ax = plt.subplots(1,W1.shape[1], figsize=(7,2.5))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "\n",
        "    for i in range(nunits):\n",
        "        layerf= lambda x : np.maximum(0,(np.dot(x,W1[:,i]) + b1[i]))\n",
        "        plt_prob_z(ax[i], layerf)\n",
        "        plt_mc_data(ax[i], X, Y, classes, map=dkcolors_map,legend=True, size=50, m='o')\n",
        "        ax[i].set_title(f\"Layer 1 Unit {i}\")\n",
        "        ax[i].set_ylabel(r\"$x_1$\",size=10)\n",
        "        ax[i].set_xlabel(r\"$x_0$\",size=10)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plt_prob_z(ax,fwb, x0_rng=(-8,8), x1_rng=(-5,4)):\n",
        "    \"\"\" plots a decision boundary but include shading to indicate the probability\n",
        "        and adds a conouter to show where z=0\n",
        "    \"\"\"\n",
        "    #setup useful ranges and common linspaces\n",
        "    x0_space  = np.linspace(x0_rng[0], x0_rng[1], 40)\n",
        "    x1_space  = np.linspace(x1_rng[0], x1_rng[1], 40)\n",
        "\n",
        "    # get probability for x0,x1 ranges\n",
        "    tmp_x0,tmp_x1 = np.meshgrid(x0_space,x1_space)\n",
        "    z = np.zeros_like(tmp_x0)\n",
        "    c = np.zeros_like(tmp_x0)\n",
        "    for i in range(tmp_x0.shape[0]):\n",
        "        for j in range(tmp_x1.shape[1]):\n",
        "            x = np.array([[tmp_x0[i,j],tmp_x1[i,j]]])\n",
        "            z[i,j] = fwb(x)\n",
        "            c[i,j] = 0. if z[i,j] == 0 else 1.\n",
        "    with warnings.catch_warnings():  # suppress no contour warning\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        #ax.contour(tmp_x0, tmp_x1, c, colors='b', linewidths=1)\n",
        "        ax.contour(tmp_x0, tmp_x1, c, linewidths=1)\n",
        "\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "    new_cmap = truncate_colormap(cmap, 0.0, 0.7)\n",
        "\n",
        "    pcm = ax.pcolormesh(tmp_x0, tmp_x1, z,\n",
        "                   norm=cm.colors.Normalize(vmin=np.amin(z), vmax=np.amax(z)),\n",
        "                   cmap=new_cmap, shading='nearest', alpha = 0.9)\n",
        "    ax.figure.colorbar(pcm, ax=ax)\n",
        "\n",
        "def plt_output_layer_linear(X, Y, W, b, classes, x0_rng=None, x1_rng=None):\n",
        "    nunits = (W.shape[1])\n",
        "    Y = Y.reshape(-1,)\n",
        "    fig,ax = plt.subplots(2,int(nunits/2), figsize=(7,5))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "    for i,axi in enumerate(ax.flat):\n",
        "        layerf = lambda x : np.dot(x,W[:,i]) + b[i]\n",
        "        plt_prob_z(axi, layerf, x0_rng=x0_rng, x1_rng=x1_rng)\n",
        "        plt_mc_data(axi, X, Y, classes, map=dkcolors_map,legend=True, size=50, m='o')\n",
        "        axi.set_ylabel(r\"$a^{[1]}_1$\",size=9)\n",
        "        axi.set_xlabel(r\"$a^{[1]}_0$\",size=9)\n",
        "        axi.set_xlim(x0_rng)\n",
        "        axi.set_ylim(x1_rng)\n",
        "        axi.set_title(f\"Linear Output Unit {i}\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
        "    \"\"\" truncates color map \"\"\"\n",
        "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
        "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
        "        cmap(np.linspace(minval, maxval, n)))\n",
        "    return new_cmap"
      ],
      "metadata": {
        "id": "LXiSvLiqmNEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W2_Lab05_TF (Derivatives)\n"
      ],
      "metadata": {
        "id": "iJ1KOHMd-gDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Lab - Derivatives\n",
        "This lab will give you a more intuitive understanding of derivatives. It will show you a simple way of calculating derivatives arithmetically. It will also introduce you to a handy Python library that allows you to calculate derivatives symbolically."
      ],
      "metadata": {
        "id": "oVTl_zMO_CDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import symbols, diff"
      ],
      "metadata": {
        "id": "chPV75y1_FGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Informal definition of derivatives\n",
        "The formal definition of derivatives can be a bit daunting with limits and values 'going to zero'. The idea is really much simpler.\n",
        "\n",
        "The derivative of a function describes how the output of a function changes when there is a small change in an input variable.\n",
        "\n",
        "Let's use the cost function $J(w)$ as an example. The cost $J$ is the output and $w$ is the input variable.  \n",
        "Let's give a 'small change' a name *epsilon* or $\\epsilon$. We use these Greek letters because it is traditional in mathematics to use *epsilon*($\\epsilon$) or *delta* ($\\Delta$) to represent a small value. You can think of it as representing 0.001 or some other small value.  \n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\text{if } w \\uparrow \\epsilon \\text{ causes }J(w) \\uparrow \\text{by }k \\times \\epsilon \\text{ then}  \\\\\n",
        "\\frac{\\partial J(w)}{\\partial w} = k \\tag{1}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "This just says if you change the input to the function $J(w)$ by a little bit and the output changes by $k$ times that little bit, then the derivative of $J(w)$ is equal to $k$.\n",
        "\n",
        "Let's try this out.  Let's look at the derivative of the function $J(w) = w^2$ at the point $w=3$ and $\\epsilon = 0.001$"
      ],
      "metadata": {
        "id": "VdviazUA_OEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J = (3)**2\n",
        "J_epsilon = (3 + 0.001)**2\n",
        "k = (J_epsilon - J)/0.001    # difference divided by epsilon\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k:0.6f} \")"
      ],
      "metadata": {
        "id": "6spHLMt7_uAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have increased the input value a little bit (0.001), causing the output to change from 9 to 9.006001, an increase of 6 times the input increase. Referencing (1) above, this says that $k=6$, so $\\frac{\\partial J(w)}{\\partial w} \\approx 6$. If you are familiar with calculus, you know, written symbolically,  $\\frac{\\partial J(w)}{\\partial w} = 2 w$. With $w=3$ this is 6. Our calculation above is not exactly 6 because to be exactly correct $\\epsilon$ would need to be [infinitesimally small](https://www.dictionary.com/browse/infinitesimally) or really, really small. That is why we use the symbols $\\approx$ or ~= rather than =. Let's see what happens if we make $\\epsilon$ smaller."
      ],
      "metadata": {
        "id": "uQM0ZbTz_5Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J = (3)**2\n",
        "J_epsilon = (3 + 0.000000001)**2\n",
        "k = (J_epsilon - J)/0.000000001\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")"
      ],
      "metadata": {
        "id": "8saN6_5U_8BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value gets close to exactly 6 as we reduce the size of $\\epsilon$. Feel free to try reducing the value further."
      ],
      "metadata": {
        "id": "-Wn4wGslAD-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding symbolic derivatives\n",
        "In backprop it is useful to know the derivative of simple functions at any input value. Put another way, we would like to know the 'symbolic' derivative rather than the 'arithmetic' derivative. An example of a symbolic derivative is,  $\\frac{\\partial J(w)}{\\partial w} = 2 w$, the derivative of $J(w) = w^2$ above.  With the symbolic derivative you can find the value of the derivative at any input value $w$.  \n",
        "\n",
        "If you have taken a calculus course, you are familiar with the many [differentiation rules](https://en.wikipedia.org/wiki/Differentiation_rules#Power_laws,_polynomials,_quotients,_and_reciprocals) that mathematicians have developed to solve for a derivative given an expression. Well, it turns out this process has been automated with symbolic differentiation programs. An example of this in python is the [SymPy](https://www.sympy.org/en/index.html) library. Let's take a look at how to use this."
      ],
      "metadata": {
        "id": "IxMSSvViAV28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $J = w^2$\n",
        "Define the python variables and their symbolic names."
      ],
      "metadata": {
        "id": "sNPbbMDgAY5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J, w = symbols('J, w')"
      ],
      "metadata": {
        "id": "GNFuwH25AgAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define and print the expression. Note SymPy produces a latex string which generates a nicely readable equation."
      ],
      "metadata": {
        "id": "kE1YgFDdAjOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use SymPy's `diff` to differentiate the expression for $J$ with respect to $w$. Note the result matches our earlier example."
      ],
      "metadata": {
        "id": "SzoAWaOqAq4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J=w**2\n",
        "J"
      ],
      "metadata": {
        "id": "aGNqNuoBAl_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw = diff(J,w)\n",
        "dJ_dw"
      ],
      "metadata": {
        "id": "zHXUjgr3AyaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the derivative at a few points by 'substituting' numeric values for the symbolic values. In the first example, $w$ is replaced by $2$."
      ],
      "metadata": {
        "id": "-Q-rJXmaAwpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw.subs([(w,2)])    # derivative at the point w = 2"
      ],
      "metadata": {
        "id": "DGg36QmFBAO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw.subs([(w,3)])    # derivative at the point w = 3"
      ],
      "metadata": {
        "id": "HCRwo5ClBB2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw.subs([(w,-3)])    # derivative at the point w = -3"
      ],
      "metadata": {
        "id": "sF-RAckWBIDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
        "    \n",
        "```python\n",
        "J= 1/w**2\n",
        "dJ_dw = diff(J,w)\n",
        "dJ_dw.subs([(w,4)])\n",
        "```\n",
        "  \n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "IZy8tOxEBu5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W2_Lab06_TF (Backprop)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TPrFTRGcCXk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   List item\n",
        "*   List item"
      ],
      "metadata": {
        "id": "6kP1T785riwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Lab: Back propagation using a computation graph\n",
        "Working through this lab will give you insight into a key algorithm used by most machine learning frameworks. Gradient descent requires the derivative of the cost with respect to each parameter in the network.  Neural networks can have millions or even billions of parameters. The *back propagation* algorithm is used to compute those derivatives. *Computation graphs* are used to simplify the operation. Let's dig into this below."
      ],
      "metadata": {
        "id": "dikdGfBGCYNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import *\n",
        "import numpy as np\n",
        "import re\n",
        "#%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.widgets import TextBox\n",
        "from matplotlib.widgets import Button\n",
        "import ipywidgets as widgets\n",
        "#from lab_utils_backprop import *"
      ],
      "metadata": {
        "id": "S2lhkBcQCpaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computation Graph\n",
        "A computation graph simplifies the computation of complex derivatives by breaking them into smaller steps. Let's see how this works.\n",
        "\n",
        "Let's calculate the derivative of this slightly complex expression, $J = (2+3w)^2$. We would like to find the derivative of $J$ with respect to $w$ or $\\frac{\\partial J}{\\partial w}$."
      ],
      "metadata": {
        "id": "32AecCcEDBBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Propagation   \n",
        "Let's calculate the values in the forward direction.\n",
        "\n",
        ">Just a note about this section. It uses global variables and reuses them as the calculation progresses. If you run cells out of order, you may get funny results. If you do, go back to this point and run them in order."
      ],
      "metadata": {
        "id": "rG7AGPZUINUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = 3\n",
        "a = 2+3*w\n",
        "J = a**2\n",
        "print(f\"a = {a}, J = {J}\")"
      ],
      "metadata": {
        "id": "2BIDdOV3IP-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backprop\n",
        "<img align=\"left\" src=\"https://github.com/HomayounfarM/machine-learning-andrew-ng/blob/main/images/C2_W2_BP_network0_j.PNG?raw=1\"     style=\" width:100px; padding: 10px 20px; \" > Backprop is the algorithm we use to calculate derivatives. As described in the lectures, backprop starts at the right and moves to the left. The first node to consider is $J = a^2 $ and the first step is to find $\\frac{\\partial J}{\\partial a}$\n"
      ],
      "metadata": {
        "id": "QOesZqwgIVcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\frac{\\partial J}{\\partial a}$\n",
        "#### Arithmetically\n",
        "Find $\\frac{\\partial J}{\\partial a}$ by finding how $J$ changes as a result of a little change in $a$. This is described in detail in the derivatives optional lab."
      ],
      "metadata": {
        "id": "txOVSwUnIbqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_epsilon = a + 0.001       # a epsilon\n",
        "J_epsilon = a_epsilon**2    # J_epsilon\n",
        "k = (J_epsilon - J)/0.001   # difference divided by epsilon\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_da ~= k = {k} \")"
      ],
      "metadata": {
        "id": "OOphDG0pIhNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\frac{\\partial J}{\\partial a}$ is 22 which is $2\\times a$. Our result is not exactly $2 \\times a$ because our epsilon value is not infinitesimally small.\n",
        "#### Symbolically\n",
        "Now, let's use SymPy to calculate derivatives symbolically as we did in the derivatives optional lab. We will prefix the name of the variable with an 's' to indicate this is a *symbolic* variable."
      ],
      "metadata": {
        "id": "me3h0qhCIxjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sw,sJ,sa = symbols('w,J,a')\n",
        "sJ = sa**2\n",
        "sJ"
      ],
      "metadata": {
        "id": "cH0lLJj2Iwxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sJ.subs([(sa,a)])"
      ],
      "metadata": {
        "id": "EKCuxg_KI3Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_da = diff(sJ, sa)\n",
        "dJ_da"
      ],
      "metadata": {
        "id": "jI4ZULWZI5Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, $\\frac{\\partial J}{\\partial a} = 2a$. When $a=11$, $\\frac{\\partial J}{\\partial a} = 22$. This matches our arithmetic calculation above.\n",
        "If you have not already done so, you can go back to the diagram above and fill in the value for $\\frac{\\partial J}{\\partial a}$."
      ],
      "metadata": {
        "id": "pvqd3_vNJAOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\frac{\\partial J}{\\partial w}$\n",
        "<img align=\"left\" src=\"https://github.com/HomayounfarM/machine-learning-andrew-ng/blob/main/images/C2_W2_BP_network0_a.PNG?raw=1\"     style=\" width:100px; padding: 10px 20px; \" >  Moving from right to left, the next value we would like to compute is $\\frac{\\partial J}{\\partial w}$. To do this, we first need to calculate $\\frac{\\partial a}{\\partial w}$ which describes how the output of this node, $a=2+3w$, changes when the input $w$ changes a little bit."
      ],
      "metadata": {
        "id": "flGxQBulJExW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Arithmetically\n",
        "Find $\\frac{\\partial a}{\\partial w}$ by finding how $a$ changes as a result of a little change in $w$."
      ],
      "metadata": {
        "id": "06E5SRxkJ4bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_epsilon = w + 0.001       # a  plus a small value, epsilon\n",
        "a_epsilon = 2 + 3*w_epsilon\n",
        "k = (a_epsilon - a)/0.001   # difference divided by epsilon\n",
        "print(f\"a = {a}, a_epsilon = {a_epsilon}, da_dw ~= k = {k} \")"
      ],
      "metadata": {
        "id": "gSNoQXohJ6q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculated arithmetically,  $\\frac{\\partial a}{\\partial w} \\approx 3$. Let's try it with SymPy."
      ],
      "metadata": {
        "id": "XUEBp1cKJ-Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sa = 2 + 3*sw\n",
        "sa"
      ],
      "metadata": {
        "id": "JWbpZkbpKBMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "da_dw = diff(sa,sw)\n",
        "da_dw"
      ],
      "metadata": {
        "id": "TZ2jbxtVKDHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">The next step is the interesting part:\n",
        "> - We know that a small change in $w$ will cause $a$ to change by 3 times that amount.\n",
        "> - We know that a small change in $a$ will cause $J$ to change by $2\\times a$ times that amount. (a=11 in this example)    \n",
        " so, putting these together,\n",
        "> - We  know that a small change in $w$ will cause $J$ to change by $3 \\times 2\\times a$ times that amount.\n",
        ">\n",
        "> These cascading changes go by the name of *the chain rule*.  It can be written like this:\n",
        " $$\\frac{\\partial J}{\\partial w} = \\frac{\\partial a}{\\partial w} \\frac{\\partial J}{\\partial a} $$\n",
        "\n",
        "It's worth spending some time thinking this through if it is not clear. This is a key take-away.\n",
        "\n",
        " Let's try calculating it:"
      ],
      "metadata": {
        "id": "CjUGgM1BJwyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw = da_dw * dJ_da\n",
        "dJ_dw"
      ],
      "metadata": {
        "id": "j5HCh5vbKQ5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And  a  is 11 in this example so  âJâw=66 . We can check this arithmetically:"
      ],
      "metadata": {
        "id": "nTeFFa3EKbab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_epsilon = w + 0.001\n",
        "a_epsilon = 2 + 3*w_epsilon\n",
        "J_epsilon = a_epsilon**2\n",
        "k = (J_epsilon - J)/0.001   # difference divided by epsilon\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")"
      ],
      "metadata": {
        "id": "j5mNGPczKXZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK! You can now fill the values for  $\\frac{\\partial a}{\\partial w}$ and $\\frac{\\partial J}{\\partial w}$ in  the diagram if you have not already done so."
      ],
      "metadata": {
        "id": "clRDtK6ZKfpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R2lQ0J7ukivP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pEGJu8TargoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W3_Lab07 (Model_Evaluation_and_Selection)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "STSmhw4qkyD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantifying a learning algorithm's performance and comparing different models are some of the common tasks when applying machine learning to real world applications. In this lab, you will practice doing these using the tips shared in class. Specifically, you will:\n",
        "\n",
        "* split datasets into training, cross validation, and test sets\n",
        "* evaluate regression and classification models\n",
        "* add polynomial features to improve the performance of a linear regression model\n",
        "* compare several neural network architectures"
      ],
      "metadata": {
        "id": "fnD_jUt3rabz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Lab Setup\n",
        "\n",
        "First, you will import the packages needed for the tasks in this lab. We also included some commands to make the outputs later more readable by reducing verbosity and suppressing non-critical warnings."
      ],
      "metadata": {
        "id": "rvH07KwNeQ7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Lab Setup"
      ],
      "metadata": {
        "id": "gcKqtN38lc9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for array computations and loading data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# for building linear regression models and preparing data\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# for building and training neural networks\n",
        "import tensorflow as tf\n",
        "\n",
        "# custom functions\n",
        "#import utils\n",
        "\n",
        "# reduce display precision on numpy arrays\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# suppress warnings\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.autograph.set_verbosity(0)"
      ],
      "metadata": {
        "id": "9gfIxFCHkxNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression\n",
        "\n",
        "First, you will be tasked to develop a model for a regression problem. You are given the dataset below consisting of 50 examples of an input feature `x` and its corresponding target `y`."
      ],
      "metadata": {
        "id": "OpY81Qozmhf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "url = '/content/drive/My Drive/Colab Notebooks/Data/c2_w3/data_w3_ex1.csv'\n",
        "df = pd.read_csv(url, header=None)\n",
        "df.columns = ['X', 'y']\n",
        "df.head(5)\n",
        "df = df.to_numpy()"
      ],
      "metadata": {
        "id": "52Rhe4u6juZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the inputs and outputs into separate arrays\n",
        "\n",
        "x = df[:,0]\n",
        "y = df[:,1]\n",
        "\n",
        "# Convert 1-D arrays into 2-D because the commands later will require it\n",
        "x = np.expand_dims(x, axis=1)\n",
        "y = np.expand_dims(y, axis=1)\n",
        "\n",
        "print(f\"the shape of the inputs x is: {x.shape}\")\n",
        "print(f\"the shape of the targets y is: {y.shape}\")"
      ],
      "metadata": {
        "id": "jfl0vDbvmyA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can plot the dataset to get an idea of how the target behaves with respect to the input."
      ],
      "metadata": {
        "id": "P_duWd9voT58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the entire dataset\n",
        "plot_dataset(x=x, y=y, title=\"input vs. target\")"
      ],
      "metadata": {
        "id": "pqH2v6OdoadS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the dataset into training, cross validation, and test sets\n",
        "\n",
        "In previous labs, you might have used the entire dataset to train your models. In practice however, it is best to hold out a portion of your data to measure how well your model generalizes to new examples. This will let you know if the model has overfit to your training set.\n",
        "\n",
        "As mentioned in the lecture, it is common to split your data into three parts:\n",
        "\n",
        "* ***training set*** - used to train the model\n",
        "* ***cross validation set (also called validation, development, or dev set)*** - used to evaluate the different model configurations you are choosing from. For example, you can use this to make a decision on what polynomial features to add to your dataset.\n",
        "* ***test set*** - used to give a fair estimate of your chosen model's performance against new examples. This should not be used to make decisions while you are still developing the models.\n",
        "\n",
        "Scikit-learn provides a [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split your data into the parts mentioned above. In the code cell below, you will split the entire dataset into 60% training, 20% cross validation, and 20% test."
      ],
      "metadata": {
        "id": "MqOi1HTVpQ_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
        "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
        "\n",
        "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
        "\n",
        "# Delete temporary variables\n",
        "del x_, y_\n",
        "\n",
        "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
        "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
        "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
        "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
        "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
        "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "2dfXeP1epU1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can plot the dataset again below to see which points were used as training, cross validation, or test data."
      ],
      "metadata": {
        "id": "zS8QXAmGp2RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title=\"input vs. target\")"
      ],
      "metadata": {
        "id": "kfGrVxYap5Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit a linear model\n",
        "\n",
        "Now that you have split the data, one of the first things you can try is to fit a linear model. You will do that in the next sections below."
      ],
      "metadata": {
        "id": "iPG8O4HDqDj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous course of this specialization, you saw that it is usually a good idea to perform feature scaling to help your model converge faster. This is especially true if your input features have widely different ranges of values. Later in this lab, you will be adding polynomial terms so your input features will indeed have different ranges. For example, $x$ runs from around 1600 to 3600, while $x^2$ will run from 2.56 million to 12.96 million.\n",
        "\n",
        "You will only use $x$ for this first model but it's good to practice feature scaling now so you can apply it later. For that, you will use the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) class from scikit-learn. This computes the z-score of your inputs. As a refresher, the z-score is given by the equation:\n",
        "\n",
        "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
        "\n",
        "where $\\mu$ is the mean of the feature values and $\\sigma$ is the standard deviation. The code below shows how to prepare the training set using the said class. You can plot the results again to inspect if it still follows the same pattern as before. The new graph should have a reduced range of values for `x`."
      ],
      "metadata": {
        "id": "QBVOXp1gqXsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the class\n",
        "scaler_linear = StandardScaler()\n",
        "\n",
        "# Compute the mean and standard deviation of the training set then transform it\n",
        "X_train_scaled = scaler_linear.fit_transform(x_train)\n",
        "\n",
        "print(f\"Computed mean of the training set: {scaler_linear.mean_.squeeze():.2f}\")\n",
        "print(f\"Computed standard deviation of the training set: {scaler_linear.scale_.squeeze():.2f}\")\n",
        "\n",
        "# Plot the results\n",
        "plot_dataset(x=X_train_scaled, y=y_train, title=\"scaled input vs. target\")"
      ],
      "metadata": {
        "id": "_cGlSs9LqCzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model\n",
        "\n",
        "Next, you will create and train a regression model. For this lab, you will use the [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class but take note that there are other [linear regressors](https://scikit-learn.org/stable/modules/classes.html#classical-linear-regressors) which you can also use."
      ],
      "metadata": {
        "id": "8N1KVMLisf-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the class\n",
        "linear_model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "linear_model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "SmenicKRspUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the Model\n",
        "\n",
        "To evaluate the performance of your model, you will measure the error for the training and cross validation sets. For the training error, recall the equation for calculating the mean squared error (MSE):\n",
        "\n",
        "$$J_{train}(\\vec{w}, b) = \\frac{1}{2m_{train}}\\left[\\sum_{i=1}^{m_{train}}(f_{\\vec{w},b}(\\vec{x}_{train}^{(i)}) - y_{train}^{(i)})^2\\right]$$\n",
        "\n",
        "Scikit-learn also has a built-in [`mean_squared_error()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function that you can use. Take note though that [as per the documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error), scikit-learn's implementation only divides by `m` and not `2*m`, where `m` is the number of examples. As mentioned in Course 1 of this Specialization (cost function lectures), dividing by `2m` is a convention we will follow but the calculations should still work whether or not you include it. Thus, to match the equation above, you can use the scikit-learn function then divide by 2 as shown below. We also included a for-loop implementation so you can check that it's equal.\n",
        "\n",
        "Another thing to take note: Since you trained the model on scaled values (i.e. using the z-score), you should also feed in the scaled training set instead of its raw values."
      ],
      "metadata": {
        "id": "Y0iB6H9ftRoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed the scaled training set and get the predictions\n",
        "yhat = linear_model.predict(X_train_scaled)\n",
        "\n",
        "# Use scikit-learn's utility function and divide by 2\n",
        "print(f\"training MSE (using sklearn function): {mean_squared_error(y_train, yhat) / 2}\")\n",
        "\n",
        "# for-loop implementation\n",
        "total_squared_error = 0\n",
        "\n",
        "for i in range(len(yhat)):\n",
        "  squared_error_i  = (yhat[i] - y_train[i])**2\n",
        "  total_squared_error += squared_error_i\n",
        "\n",
        "mse = total_squared_error / (2*len(yhat))\n",
        "\n",
        "print(f\"training MSE (for-loop implementation): {mse.squeeze()}\")"
      ],
      "metadata": {
        "id": "WfVyxZZ7tWe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then compute the MSE for the cross validation set with basically the same equation:\n",
        "\n",
        "$$J_{cv}(\\vec{w}, b) = \\frac{1}{2m_{cv}}\\left[\\sum_{i=1}^{m_{cv}}(f_{\\vec{w},b}(\\vec{x}_{cv}^{(i)}) - y_{cv}^{(i)})^2\\right]$$\n",
        "\n",
        "As with the training set, you will also want to scale the cross validation set. An *important* thing to note when using the z-score is you have to use the mean and standard deviation of the **training set** when scaling the cross validation set. This is to ensure that your input features are transformed as expected by the model. One way to gain intuition is with this scenario:\n",
        "\n",
        "* Say that your training set has an input feature equal to `500` which is scaled down to `0.5` using the z-score.\n",
        "* After training, your model is able to accurately map this scaled input `x=0.5` to the target output `y=300`.\n",
        "* Now let's say that you deployed this model and one of your users fed it a sample equal to `500`.\n",
        "* If you get this input sample's z-score using any other values of the mean and standard deviation, then it might not be scaled to `0.5` and your model will most likely make a wrong prediction (i.e. not equal to `y=300`).\n",
        "\n",
        "You will scale the cross validation set below by using the same `StandardScaler` you used earlier but only calling its [`transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.transform) method instead of [`fit_transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform)."
      ],
      "metadata": {
        "id": "c0QKPZ6PyUYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the cross validation set using the mean and standard deviation of the training set\n",
        "X_cv_scaled = scaler_linear.transform(x_cv)\n",
        "\n",
        "print(f\"Mean used to scale the CV set: {scaler_linear.mean_.squeeze():.2f}\")\n",
        "print(f\"Standard deviation used to scale the CV set: {scaler_linear.scale_.squeeze():.2f}\")\n",
        "\n",
        "# Feed the scaled cross validation set\n",
        "yhat = linear_model.predict(X_cv_scaled)\n",
        "\n",
        "# Use scikit-learn's utility function and divide by 2\n",
        "print(f\"Cross validation MSE: {mean_squared_error(y_cv, yhat) / 2}\")"
      ],
      "metadata": {
        "id": "Kd03hxuHyXnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Polynomial Features\n",
        "\n",
        "From the graphs earlier, you may have noticed that the target `y` rises more sharply at smaller values of `x` compared to higher ones. A straight line might not be the best choice because the target `y` seems to flatten out as `x` increases. Now that you have these values of the training and cross validation MSE from the linear model, you can try adding polynomial features to see if you can get a better performance. The code will mostly be the same but with a few extra preprocessing steps. Let's see that below."
      ],
      "metadata": {
        "id": "JoWSpE7_yhYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the additional features\n",
        "\n",
        "First, you will generate the polynomial features from your training set. The code below demonstrates how to do this using the [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class. It will create a new input feature which has the squared values of the input `x` (i.e. degree=2)."
      ],
      "metadata": {
        "id": "qa1jFfEcypv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the class to make polynomial features\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "# Compute the number of features and transform the training set\n",
        "X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "# Preview the first 5 elements of the new training set. Left column is `x` and right column is `x^2`\n",
        "# Note: The `e+<number>` in the output denotes how many places the decimal point should\n",
        "# be moved. For example, `3.24e+03` is equal to `3240`\n",
        "print(X_train_mapped[:5])"
      ],
      "metadata": {
        "id": "Eus1utvZywtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will then scale the inputs as before to narrow down the range of values."
      ],
      "metadata": {
        "id": "3Jw4diC3y_Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the class\n",
        "scaler_poly = StandardScaler()\n",
        "\n",
        "# Compute the mean and standard deviation of the training set then transform it\n",
        "X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "\n",
        "# Preview the first 5 elements of the scaled training set.\n",
        "print(X_train_mapped_scaled[:5])"
      ],
      "metadata": {
        "id": "Kcg0MckNzHcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then proceed to train the model. After that, you will measure the model's performance against the cross validation set. Like before, you should make sure to perform the same transformations as you did in the training set. You will add the same number of polynomial features then scale the range of values."
      ],
      "metadata": {
        "id": "jSz0fBdEzPa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the class\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_mapped_scaled, y_train )\n",
        "\n",
        "# Compute the training MSE\n",
        "yhat = model.predict(X_train_mapped_scaled)\n",
        "print(f\"Training MSE: {mean_squared_error(y_train, yhat) / 2}\")\n",
        "\n",
        "# Add the polynomial features to the cross validation set\n",
        "X_cv_mapped = poly.transform(x_cv)\n",
        "\n",
        "# Scale the cross validation set using the mean and standard deviation of the training set\n",
        "X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "# Compute the cross validation MSE\n",
        "yhat = model.predict(X_cv_mapped_scaled)\n",
        "print(f\"Cross validation MSE: {mean_squared_error(y_cv, yhat) / 2}\")"
      ],
      "metadata": {
        "id": "gfel3KqyzR3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll notice that the MSEs are significantly better for both the training and cross validation set when you added the 2nd order polynomial. You may want to introduce more polynomial terms and see which one gives the best performance. As shown in class, you can have 10 different models like this:\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1hwZ5FoBJO-FFf3DmWBXI4SUBdqkp1o-o)\n",
        "\n",
        "You can create a loop that contains all the steps in the previous code cells. Here is one implementation that adds polynomial features up to degree=10. We'll plot it at the end to make it easier to compare the results for each model."
      ],
      "metadata": {
        "id": "AQxF3AmrzepP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists containing the lists, models, and scalers\n",
        "train_mses = []\n",
        "cv_mses = []\n",
        "models = []\n",
        "scalers = []\n",
        "\n",
        "# Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "for degree in range(1,11):\n",
        "\n",
        "    # Add polynomial features to the training set\n",
        "    poly = PolynomialFeatures(degree, include_bias=False)\n",
        "    X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "    # Scale the training set\n",
        "    scaler_poly = StandardScaler()\n",
        "    X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "    scalers.append(scaler_poly)\n",
        "\n",
        "    # Create and train the model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_mapped_scaled, y_train )\n",
        "    models.append(model)\n",
        "\n",
        "    # Compute the training MSE\n",
        "    yhat = model.predict(X_train_mapped_scaled)\n",
        "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "    train_mses.append(train_mse)\n",
        "\n",
        "    # Add polynomial features and scale the cross validation set\n",
        "    poly = PolynomialFeatures(degree, include_bias=False)\n",
        "    X_cv_mapped = poly.fit_transform(x_cv)\n",
        "    X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "    # Compute the cross validation MSE\n",
        "    yhat = model.predict(X_cv_mapped_scaled)\n",
        "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "    cv_mses.append(cv_mse)\n",
        "\n",
        "# Plot the results\n",
        "degrees=range(1,11)\n",
        "plot_train_cv_mses(degrees, train_mses, cv_mses, title=\"degree of polynomial vs. train and CV MSEs\")"
      ],
      "metadata": {
        "id": "zikfw33800Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing the best model\n",
        "\n",
        "When selecting a model, you want to choose one that performs well both on the training and cross validation set. It implies that it is able to learn the patterns from your training set without overfitting. If you used the defaults in this lab, you will notice a sharp drop in cross validation error from the models with degree=1 to degree=2. This is followed by a relatively flat line up to degree=5. After that, however, the cross validation error is generally getting worse as you add more polynomial features. Given these, you can decide to use the model with the lowest `cv_mse` as the one best suited for your application."
      ],
      "metadata": {
        "id": "pWXH40af1Fki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model with the lowest CV MSE (add 1 because list indices start at 0)\n",
        "# This also corresponds to the degree of the polynomial added\n",
        "degree = np.argmin(cv_mses) + 1\n",
        "print(f\"Lowest CV MSE is found in the model with degree={degree}\")"
      ],
      "metadata": {
        "id": "sXenKQF81SnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then publish the generalization error by computing the test set's MSE. As usual, you should transform this data the same way you did with the training and cross validation sets."
      ],
      "metadata": {
        "id": "pQQ6D6zL1kDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add polynomial features to the test set\n",
        "poly = PolynomialFeatures(degree, include_bias=False)\n",
        "X_test_mapped = poly.fit_transform(x_test)\n",
        "\n",
        "# Scale the test set\n",
        "X_test_mapped_scaled = scalers[degree-1].transform(X_test_mapped)\n",
        "\n",
        "# Compute the test MSE\n",
        "yhat = models[degree-1].predict(X_test_mapped_scaled)\n",
        "test_mse = mean_squared_error(y_test, yhat) / 2\n",
        "\n",
        "print(f\"Training MSE: {train_mses[degree-1]:.2f}\")\n",
        "print(f\"Cross Validation MSE: {cv_mses[degree-1]:.2f}\")\n",
        "print(f\"Test MSE: {test_mse:.2f}\")"
      ],
      "metadata": {
        "id": "vkuvaomj1nAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks\n",
        "\n",
        "The same model selection process can also be used when choosing between different neural network architectures. In this section, you will create the models shown below and apply it to the same regression task above.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1my8evNUeqV-rC16apoFwnkMuSDSgSmKy)\n"
      ],
      "metadata": {
        "id": "EQRlYl8j2BZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the Data\n",
        "\n",
        "You will use the same training, cross validation, and test sets you generated in the previous section. From earlier lectures in this course, you may have known that neural networks can learn non-linear relationships so you can opt to skip adding polynomial features. The code is still included below in case you want to try later and see what effect it will have on your results. The default `degree` is set to `1` to indicate that it will just use `x_train`, `x_cv`, and `x_test` as is (i.e. without any additional polynomial features)."
      ],
      "metadata": {
        "id": "inLQUy1o2lZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add polynomial features\n",
        "degree = 4\n",
        "poly = PolynomialFeatures(degree, include_bias=False)\n",
        "X_train_mapped = poly.fit_transform(x_train)\n",
        "X_cv_mapped = poly.transform(x_cv)\n",
        "X_test_mapped = poly.transform(x_test)"
      ],
      "metadata": {
        "id": "0Wehefk12AtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you will scale the input features to help gradient descent converge faster. Again, notice that you are using the mean and standard deviation computed from the training set by just using `transform()` in the cross validation and test sets instead of `fit_transform()`."
      ],
      "metadata": {
        "id": "e_u2rmp4427-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features using the z-score\n",
        "scaler = StandardScaler()\n",
        "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
        "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
        "X_test_mapped_scaled = scaler.transform(X_test_mapped)"
      ],
      "metadata": {
        "id": "GfeARtbx48i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_mapped_scaled.shape"
      ],
      "metadata": {
        "id": "7bVYYcCI-GMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and train the models\n",
        "\n",
        "You will then create the neural network architectures shown earlier. The code is provided in the `build_models()` function in case you want to inspect or modify it. You will use that in the loop below then proceed to train the models. For each model, you will also record the training and cross validation errors."
      ],
      "metadata": {
        "id": "hJs4YHFz5BD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists that will contain the errors for each model\n",
        "nn_train_mses = []\n",
        "nn_cv_mses = []\n",
        "\n",
        "# Build the models\n",
        "nn_models = build_models()\n",
        "\n",
        "# Loop over the the models\n",
        "for model in nn_models:\n",
        "\n",
        "    # Setup the loss and optimizer\n",
        "    model.compile(\n",
        "    loss='mse',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
        "    )\n",
        "\n",
        "    print(f\"Training {model.name}...\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        X_train_mapped_scaled, y_train,\n",
        "        epochs=300,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    print(\"Done!\\n\")\n",
        "\n",
        "\n",
        "    # Record the training MSEs\n",
        "    yhat = model.predict(X_train_mapped_scaled)\n",
        "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "    nn_train_mses.append(train_mse)\n",
        "\n",
        "    # Record the cross validation MSEs\n",
        "    yhat = model.predict(X_cv_mapped_scaled)\n",
        "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "    nn_cv_mses.append(cv_mse)\n",
        "\n",
        "# print models summary\n",
        "print(\"Summary of the models:\")\n",
        "for model in nn_models:\n",
        "  print(model.summary())\n",
        "\n",
        "# print results\n",
        "print(\"RESULTS:\")\n",
        "for model_num in range(len(nn_train_mses)):\n",
        "    print(\n",
        "        f\"Model {model_num+1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" +\n",
        "        f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "TGOCn3la557d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the recorded errors, you can decide which is the best model for your application. Look at the results above and see if you agree with the selected `model_num` below. Finally, you will compute the test error to estimate how well it generalizes to new examples."
      ],
      "metadata": {
        "id": "HnLmMLwzBR2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the model with the lowest CV MSE\n",
        "model_num = 3\n",
        "\n",
        "# Compute the test MSE\n",
        "yhat = nn_models[model_num-1].predict(X_test_mapped_scaled)\n",
        "test_mse = mean_squared_error(y_test, yhat) / 2\n",
        "\n",
        "print(f\"Selected Model: {model_num}\")\n",
        "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
        "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
        "print(f\"Test MSE: {test_mse:.2f}\")"
      ],
      "metadata": {
        "id": "yu7bcuP6BVBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification\n",
        "\n",
        "In this last part of the lab, you will practice model evaluation and selection on a classification task. The process will be similar, with the main difference being the computation of the errors. You will see that in the following sections."
      ],
      "metadata": {
        "id": "ABsa986QHBNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Dataset\n",
        "\n",
        "First, you will load a dataset for a binary classification task. It has 200 examples of two input features (`x1` and `x2`), and a target `y` of either `0` or `1`."
      ],
      "metadata": {
        "id": "fNr1aCc9HFzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from a text file\n",
        "url = '/content/drive/My Drive/Colab Notebooks/Data/c2_w3/data_w3_ex2.csv'\n",
        "data = np.loadtxt(url, delimiter=',')\n",
        "print(type(data))\n",
        "# Split the inputs and outputs into separate arrays\n",
        "x_bc = data[:,:-1]\n",
        "y_bc = data[:,-1]\n",
        "\n",
        "# Convert y into 2-D because the commands later will require it (x is already 2-D)\n",
        "y_bc = np.expand_dims(y_bc, axis=1)\n",
        "\n",
        "print(f\"the shape of the inputs x is: {x_bc.shape}\")\n",
        "print(f\"the shape of the targets y is: {y_bc.shape}\")"
      ],
      "metadata": {
        "id": "6Y8YNhRWHLL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can plot the dataset to examine how the examples are separated."
      ],
      "metadata": {
        "id": "Puel1Q2UIVrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_bc_dataset(x=x_bc, y=y_bc, title=\"x1 vs. x2\")"
      ],
      "metadata": {
        "id": "zEK3_OfKIYTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split and prepare the dataset\n",
        "\n",
        "Next, you will generate the training, cross validation, and test sets. You will use the same 60/20/20 proportions as before. You will also scale the features as you did in the previous section."
      ],
      "metadata": {
        "id": "m4_y3Pb8Iikh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
        "x_bc_train, x_, y_bc_train, y_ = train_test_split(x_bc, y_bc, test_size=0.40, random_state=1)\n",
        "\n",
        "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "x_bc_cv, x_bc_test, y_bc_cv, y_bc_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
        "\n",
        "# Delete temporary variables\n",
        "del x_, y_\n",
        "\n",
        "print(f\"the shape of the training set (input) is: {x_bc_train.shape}\")\n",
        "print(f\"the shape of the training set (target) is: {y_bc_train.shape}\\n\")\n",
        "print(f\"the shape of the cross validation set (input) is: {x_bc_cv.shape}\")\n",
        "print(f\"the shape of the cross validation set (target) is: {y_bc_cv.shape}\\n\")\n",
        "print(f\"the shape of the test set (input) is: {x_bc_test.shape}\")\n",
        "print(f\"the shape of the test set (target) is: {y_bc_test.shape}\")"
      ],
      "metadata": {
        "id": "_h5tJti6IlHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features\n",
        "\n",
        "# Initialize the class\n",
        "scaler_linear = StandardScaler()\n",
        "\n",
        "# Compute the mean and standard deviation of the training set then transform it\n",
        "x_bc_train_scaled = scaler_linear.fit_transform(x_bc_train)\n",
        "x_bc_cv_scaled = scaler_linear.transform(x_bc_cv)\n",
        "x_bc_test_scaled = scaler_linear.transform(x_bc_test)"
      ],
      "metadata": {
        "id": "wJcBLXeyIpAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the error for classification models\n",
        "\n",
        "In the previous sections on regression models, you used the mean squared error to measure how well your model is doing. For classification, you can get a similar metric by getting the fraction of the data that the model has misclassified. For example, if your model made wrong predictions for 2 samples out of 5, then you will report an error of `40%` or `0.4`. The code below demonstrates this using a for-loop and also with Numpy's [`mean()`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) function."
      ],
      "metadata": {
        "id": "2a4VxxTaIsX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample model output\n",
        "probabilities = np.array([0.2, 0.6, 0.7, 0.3, 0.8])\n",
        "\n",
        "# Apply a threshold to the model output. If greater than 0.5, set to 1. Else 0.\n",
        "predictions = np.where(probabilities >= 0.5, 1, 0)\n",
        "\n",
        "# Ground truth labels\n",
        "ground_truth = np.array([1, 1, 1, 1, 1])\n",
        "\n",
        "# Initialize counter for misclassified data\n",
        "misclassified = 0\n",
        "\n",
        "# Get number of predictions\n",
        "num_predictions = len(predictions)\n",
        "\n",
        "# Loop over each prediction\n",
        "for i in range(num_predictions):\n",
        "\n",
        "    # Check if it matches the ground truth\n",
        "    if predictions[i] != ground_truth[i]:\n",
        "\n",
        "        # Add one to the counter if the prediction is wrong\n",
        "        misclassified += 1\n",
        "\n",
        "# Compute the fraction of the data that the model misclassified\n",
        "fraction_error = misclassified/num_predictions\n",
        "\n",
        "print(f\"probabilities: {probabilities}\")\n",
        "print(f\"predictions with threshold=0.5: {predictions}\")\n",
        "print(f\"targets: {ground_truth}\")\n",
        "print(f\"fraction of misclassified data (for-loop): {fraction_error}\")\n",
        "print(f\"fraction of misclassified data (with np.mean()): {np.mean(predictions != ground_truth)}\")"
      ],
      "metadata": {
        "id": "HgPCGnCzJKRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and train the model\n",
        "\n",
        "You will use the same neural network architectures in the previous section so you can call the `build_models()` function again to create new instances of these models.\n",
        "\n",
        "You will follow the recommended approach mentioned last week where you use a `linear` activation for the output layer (instead of `sigmoid`) then set `from_logits=True` when declaring the loss function of the model. You will use the [binary crossentropy loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) because this is a binary classification problem.\n",
        "\n",
        "After training, you will use a [sigmoid function](https://www.tensorflow.org/api_docs/python/tf/math/sigmoid) to convert the model outputs into probabilities. From there, you can set a threshold and get the fraction of misclassified examples from the training and cross validation sets.\n",
        "\n",
        "You can see all these in the code cell below."
      ],
      "metadata": {
        "id": "5Ci1Lh7ZJRO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists that will contain the errors for each model\n",
        "nn_train_error = []\n",
        "nn_cv_error = []\n",
        "\n",
        "# Build the models\n",
        "models_bc = build_models()\n",
        "\n",
        "# Loop over each model\n",
        "for model in models_bc:\n",
        "\n",
        "    # Setup the loss and optimizer\n",
        "    model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    )\n",
        "\n",
        "    print(f\"Training {model.name}...\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        x_bc_train_scaled, y_bc_train,\n",
        "        epochs=200,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    print(\"Done!\\n\")\n",
        "\n",
        "    # Set the threshold for classification\n",
        "    threshold = 0.5\n",
        "\n",
        "    # Record the fraction of misclassified examples for the training set\n",
        "    yhat = model.predict(x_bc_train_scaled)\n",
        "    yhat = tf.math.sigmoid(yhat)\n",
        "    yhat = np.where(yhat >= threshold, 1, 0)\n",
        "    train_error = np.mean(yhat != y_bc_train)\n",
        "    nn_train_error.append(train_error)\n",
        "\n",
        "    # Record the fraction of misclassified examples for the cross validation set\n",
        "    yhat = model.predict(x_bc_cv_scaled)\n",
        "    yhat = tf.math.sigmoid(yhat)\n",
        "    yhat = np.where(yhat >= threshold, 1, 0)\n",
        "    cv_error = np.mean(yhat != y_bc_cv)\n",
        "    nn_cv_error.append(cv_error)\n",
        "\n",
        "# Print the result\n",
        "for model_num in range(len(nn_train_error)):\n",
        "    print(\n",
        "        f\"Model {model_num+1}: Training Set Classification Error: {nn_train_error[model_num]:.5f}, \" +\n",
        "        f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "M7V7RyxkJrs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output above, you can choose which one performed best. If there is a tie on the cross validation set error, then you can pick the one with the lower training set error. Finally, you can compute the test error to report the model's generalization error."
      ],
      "metadata": {
        "id": "kQ7rPKbRJ6RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the model with the lowest error\n",
        "model_num = 3\n",
        "\n",
        "# Compute the test error\n",
        "yhat = models_bc[model_num-1].predict(x_bc_test_scaled)\n",
        "yhat = tf.math.sigmoid(yhat)\n",
        "yhat = np.where(yhat >= threshold, 1, 0)\n",
        "nn_test_error = np.mean(yhat != y_bc_test)\n",
        "\n",
        "print(f\"Selected Model: {model_num}\")\n",
        "print(f\"Training Set Classification Error: {nn_train_error[model_num-1]:.4f}\")\n",
        "print(f\"CV Set Classification Error: {nn_cv_error[model_num-1]:.4f}\")\n",
        "print(f\"Test Set Classification Error: {nn_test_error:.4f}\")"
      ],
      "metadata": {
        "id": "VsMlV9tTKA-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the functions we use above:"
      ],
      "metadata": {
        "id": "bG-7WxiMl8hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "#plt.style.use('./deeplearning.mplstyle')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def plot_dataset(x, y, title):\n",
        "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
        "    plt.rcParams[\"lines.markersize\"] = 12\n",
        "    plt.scatter(x, y, marker='x', c='r');\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "    plt.show()\n",
        "\n",
        "def plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title):\n",
        "    plt.scatter(x_train, y_train, marker='x', c='r', label='training');\n",
        "    plt.scatter(x_cv, y_cv, marker='o', c='b', label='cross validation');\n",
        "    plt.scatter(x_test, y_test, marker='^', c='g', label='test');\n",
        "    plt.title(\"input vs. target\")\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_train_cv_mses(degrees, train_mses, cv_mses, title):\n",
        "    degrees = range(1,11)\n",
        "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_bc_dataset(x, y, title):\n",
        "    for i in range(len(y)):\n",
        "        marker = 'x' if y[i] == 1 else 'o'\n",
        "        c = 'r' if y[i] == 1 else 'b'\n",
        "        plt.scatter(x[i,0], x[i,1], marker=marker, c=c);\n",
        "    plt.title(\"x1 vs x2\")\n",
        "    plt.xlabel(\"x1\");\n",
        "    plt.ylabel(\"x2\");\n",
        "    y_0 = mlines.Line2D([], [], color='r', marker='x', markersize=12, linestyle='None', label='y=1')\n",
        "    y_1 = mlines.Line2D([], [], color='b', marker='o', markersize=12, linestyle='None', label='y=0')\n",
        "    plt.title(title)\n",
        "    plt.legend(handles=[y_0, y_1])\n",
        "    plt.show()\n",
        "\n",
        "def build_models():\n",
        "\n",
        "    tf.random.set_seed(20)\n",
        "\n",
        "    model_1 = Sequential(\n",
        "        [\n",
        "            Dense(25, activation = 'relu'),\n",
        "            Dense(15, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_1'\n",
        "    )\n",
        "\n",
        "    model_2 = Sequential(\n",
        "        [\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_2'\n",
        "    )\n",
        "\n",
        "    model_3 = Sequential(\n",
        "        [\n",
        "            Dense(32, activation = 'relu'),\n",
        "            Dense(16, activation = 'relu'),\n",
        "            Dense(8, activation = 'relu'),\n",
        "            Dense(4, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_3'\n",
        "    )\n",
        "\n",
        "    model_list = [model_1, model_2, model_3]\n",
        "\n",
        "    return model_list\n",
        "\n",
        "\n",
        "# Not used in the lab. You can call this for the binary\n",
        "# classification problem if you set `from_logits=False`\n",
        "# when declaring the loss. With this, you will also not need\n",
        "# to call the `tf.math.sigmoid()` function in the loop\n",
        "# because the model output is already a probability\n",
        "def build_bc_models():\n",
        "\n",
        "    tf.random.set_seed(20)\n",
        "\n",
        "    model_1_bc = Sequential(\n",
        "        [\n",
        "            Dense(25, activation = 'relu'),\n",
        "            Dense(15, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_1_bc'\n",
        "    )\n",
        "\n",
        "    model_2_bc = Sequential(\n",
        "        [\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_2_bc'\n",
        "    )\n",
        "\n",
        "    model_3_bc = Sequential(\n",
        "        [\n",
        "            Dense(32, activation = 'relu'),\n",
        "            Dense(16, activation = 'relu'),\n",
        "            Dense(8, activation = 'relu'),\n",
        "            Dense(4, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_3_bc'\n",
        "    )\n",
        "\n",
        "    models_bc = [model_1_bc, model_2_bc, model_3_bc]\n",
        "\n",
        "    return models_bc\n",
        "\n",
        "\n",
        "def prepare_dataset(filename):\n",
        "\n",
        "    data = np.loadtxt(filename, delimiter=\",\")\n",
        "\n",
        "    x = data[:,:-1]\n",
        "    y = data[:,-1]\n",
        "\n",
        "    # Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
        "    x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=80)\n",
        "\n",
        "    # Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "    x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=80)\n",
        "\n",
        "    return x_train, y_train, x_cv, y_cv, x_test, y_test\n",
        "\n",
        "def train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "    degrees = range(1,max_degree+1)\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for degree in degrees:\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model.fit(X_train_mapped_scaled, y_train )\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv)\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
        "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
        "    plt.xticks(degrees)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for reg_param in reg_params:\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model = Ridge(alpha=reg_param)\n",
        "        model.fit(X_train_mapped_scaled, y_train)\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv)\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    reg_params = [str(x) for x in reg_params]\n",
        "    plt.plot(reg_params, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(reg_params, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(reg_params, np.repeat(baseline, len(reg_params)), linestyle='--', label='baseline')\n",
        "    plt.title(\"lambda vs. train and CV MSEs\")\n",
        "    plt.xlabel(\"lambda\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_diff_datasets(model, files, max_degree=10, baseline=None):\n",
        "\n",
        "    for file in files:\n",
        "\n",
        "        x_train, y_train, x_cv, y_cv, x_test, y_test = prepare_dataset(file['filename'])\n",
        "\n",
        "        train_mses = []\n",
        "        cv_mses = []\n",
        "        models = []\n",
        "        scalers = []\n",
        "        degrees = range(1,max_degree+1)\n",
        "\n",
        "        # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "        for degree in degrees:\n",
        "\n",
        "            # Add polynomial features to the training set\n",
        "            poly = PolynomialFeatures(degree, include_bias=False)\n",
        "            X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "            # Scale the training set\n",
        "            scaler_poly = StandardScaler()\n",
        "            X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "            scalers.append(scaler_poly)\n",
        "\n",
        "            # Create and train the model\n",
        "            model.fit(X_train_mapped_scaled, y_train )\n",
        "            models.append(model)\n",
        "\n",
        "            # Compute the training MSE\n",
        "            yhat = model.predict(X_train_mapped_scaled)\n",
        "            train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "            train_mses.append(train_mse)\n",
        "\n",
        "            # Add polynomial features and scale the cross-validation set\n",
        "            poly = PolynomialFeatures(degree, include_bias=False)\n",
        "            X_cv_mapped = poly.fit_transform(x_cv)\n",
        "            X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "            # Compute the cross-validation MSE\n",
        "            yhat = model.predict(X_cv_mapped_scaled)\n",
        "            cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "            cv_mses.append(cv_mse)\n",
        "\n",
        "        # Plot the results\n",
        "        plt.plot(degrees, train_mses, marker='o', c='r', linestyle=file['linestyle'], label=f\"{file['label']} training MSEs\");\n",
        "        plt.plot(degrees, cv_mses, marker='o', c='b', linestyle=file['linestyle'], label=f\"{file['label']} CV MSEs\");\n",
        "\n",
        "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
        "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
        "    plt.xticks(degrees)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_learning_curve(model, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "    num_samples_train_and_cv = []\n",
        "    percents = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for percent in percents:\n",
        "\n",
        "        num_samples_train = round(len(x_train) * (percent/100.0))\n",
        "        num_samples_cv = round(len(x_cv) * (percent/100.0))\n",
        "        num_samples_train_and_cv.append(num_samples_train + num_samples_cv)\n",
        "\n",
        "        x_train_sub = x_train[:num_samples_train]\n",
        "        y_train_sub = y_train[:num_samples_train]\n",
        "        x_cv_sub = x_cv[:num_samples_cv]\n",
        "        y_cv_sub = y_cv[:num_samples_cv]\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train_sub)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model.fit(X_train_mapped_scaled, y_train_sub)\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train_sub, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv_sub)\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv_sub, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    plt.plot(num_samples_train_and_cv, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(num_samples_train_and_cv, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(num_samples_train_and_cv, np.repeat(baseline, len(percents)), linestyle='--', label='baseline')\n",
        "    plt.title(\"number of examples vs. train and CV MSEs\")\n",
        "    plt.xlabel(\"total number of training and cv examples\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6nFAV60Nl9Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W3_Lab08 (Optional Lab: Diagnosing Bias and Variance)\n"
      ],
      "metadata": {
        "id": "UeyOn8wCtxGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous optional lab, you saw how to evaluate a learning algorithm's performance by measuring its training and cross validation error. Given these values, you are able to quantify how well a model is doing and this helps you make a decision on which one to use for a given application. In this lab, you will build upon this process and explore some tips to improve the performance of your models. As it turns out, the training and cross validation errors can tell you what to try next to improve your models. Specifically, it will show if you have a high bias (underfitting) or high variance (overfitting) problem. This lecture slide shown below gives an example:\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1XVShoJUmg0VPOwjY12BnxmbokMUwtA4w)\n",
        "\n",
        "The leftmost figure shows a high bias problem where the model is not capturing the patterns in the training data. As a result, you will have a high training and cross validation error. The rightmost figure, on the other hand, shows a high variance problem where the model has overfit the training set. Thus, even though it has a low training error, it will perform poorly on new examples. That is indicated by a high cross validation error. The ideal model would be the figure in the middle, where it successfully learns from the training set and also generalizes well to unseen data. The lectures gave some tips on what to do next to achieve this \"just right\" model.\n",
        "\n",
        "To fix a high bias problem, you can:\n",
        "* try adding polynomial features\n",
        "* try getting additional features\n",
        "* try decreasing the regularization parameter\n",
        "\n",
        "To fix a high variance problem, you can:\n",
        "* try increasing the regularization parameter\n",
        "* try smaller sets of features\n",
        "* get more training examples\n",
        "\n",
        "You will try all these tips in this lab. Let's begin!"
      ],
      "metadata": {
        "id": "LqGQ8s1IuCmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Establishing Baseline Level of Performance\n",
        "\n",
        "Before you can diagnose a model for high bias or high variance, it is usually helpful to first have an idea of what level of error you can reasonably get to. As mentioned in class, you can use any of the following to set a baseline level of performance.\n",
        "\n",
        "* human level performance\n",
        "* competing algorithm's performance\n",
        "* guess based on experience\n",
        "\n",
        "Real-world data can be very noisy and it's often infeasible to get to 0% error. For example, you might think that you have a high bias problem because you're getting 10% training and 15% cross validation error on a computer vision application. However, you later found out that even humans can't perform better than 10% error. If you consider this the baseline level, then you now instead have a high variance problem because you've prioritized minimizing the gap between cross validation and training error.\n",
        "\n",
        "With this in mind, let's begin exploring the techniques to address these issues."
      ],
      "metadata": {
        "id": "L69BrAscwjlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for array computations and loading data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# for building linear regression models and preparing data\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# for building and training neural networks\n",
        "import tensorflow as tf\n",
        "\n",
        "# custom functions\n",
        "#import utils\n",
        "\n",
        "# reduce display precision on numpy arrays\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# suppress warnings\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.autograph.set_verbosity(0)"
      ],
      "metadata": {
        "id": "N8ybJVdExHRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixing High Bias\n",
        "\n",
        "You will first look at things to try when your model is underfitting. In other words, when the training error is far worse than the baseline level of performance."
      ],
      "metadata": {
        "id": "7xeqFjBdxk9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try adding polynomial features\n",
        "\n",
        "You've already seen this in the previous lab. Adding polynomial features can help your model learn more complex patterns in your data. Here again is an example of a plot showing how the training and cross validation errors change as you add more polynomial features. You will be using a synthetic dataset for a regression problem with one feature and one target. In addition, you will also define an arbitrary baseline performance and include it in the plot."
      ],
      "metadata": {
        "id": "E4g0JTlQx1sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "H5aawYom0n-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the data file\n",
        "url = '/content/drive/My Drive/Colab Notebooks/Data/c2_w3/c2w3_lab2_data1.csv'\n",
        "\n",
        "data = np.loadtxt(url, delimiter=',')\n",
        "\n",
        "x = data[:,0].reshape(-1,1)\n",
        "y = data[:,1]\n",
        "\n"
      ],
      "metadata": {
        "id": "EbgAxNN9y7V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
        "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=80)\n",
        "\n",
        "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=80)\n",
        "\n",
        "# Split the dataset into train, cv, and test\n",
        "#_train, y_train, x_cv, y_cv, x_test, y_test = prepare_dataset('data/c2w3_lab2_data1.csv')\n",
        "\n",
        "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
        "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
        "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
        "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
        "\n",
        "# Preview the first 5 rows\n",
        "print(f\"first 5 rows of the training inputs (1 feature):\\n {x_train[:5]}\\n\")\n",
        "\n",
        "# Instantiate the regression model class\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train and plot polynomial regression models\n",
        "train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=400)"
      ],
      "metadata": {
        "id": "8OswQvkpx6py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the more polynomial features you add, the better the model fits to the training data. In this example, it even performed better than the baseline. At this point, you can say that the models with degree greater than 4 are low-bias because they perform close to or better than the baseline.\n",
        "\n",
        "However, if the baseline is defined lower (e.g. you consulted an expert regarding the acceptable error), then the models are still considered high bias. You can then try other methods to improve this."
      ],
      "metadata": {
        "id": "u2K_auPGo2gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "\n",
        "# Instantiate the model class\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train and plot polynomial regression models. Dataset used has two features.\n",
        "train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=250)\n"
      ],
      "metadata": {
        "id": "SJ0X0F4y3Ns1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try getting additional features\n",
        "\n",
        "Another thing you can try is to acquire other features. Let's say that after you got the results above, you decided to launch another data collection campaign that captures another feature. Your dataset will now have 2 columns for the input features as shown below."
      ],
      "metadata": {
        "id": "bYJ1r9cVmunn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the data file\n",
        "url = '/content/drive/My Drive/Colab Notebooks/Data/c2_w3/c2w3_lab2_data2.csv'\n",
        "\n",
        "data = np.loadtxt(url, delimiter=',')\n",
        "\n",
        "x = data[:,0:2]\n",
        "y = data[:,2]\n"
      ],
      "metadata": {
        "id": "ORxPcyFOrJD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
        "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=80)\n",
        "\n",
        "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=80)\n",
        "\n",
        "\n",
        "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
        "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
        "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
        "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
        "\n",
        "# Preview the first 5 rows\n",
        "print(f\"first 5 rows of the training inputs (1 feature):\\n {x_train[:5]}\\n\")\n"
      ],
      "metadata": {
        "id": "jdg65dLMm-64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model class\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train and plot polynomial regression models. Dataset used has two features.\n",
        "train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=6, baseline=250)"
      ],
      "metadata": {
        "id": "tm7hQ85uoFp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try decreasing the regularization parameter\n",
        "\n",
        "At this point, you might want to introduce regularization to avoid overfitting. One thing to watch out for is you might make your models underfit if you set the regularization parameter too high. The cell below trains a 4th degree polynomial model using the [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) class which allows you to set a regularization parameter (i.e. lambda or $\\lambda$). You will try several values and compare the results."
      ],
      "metadata": {
        "id": "6rITwK0qsZmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define lambdas to plot\n",
        "reg_params = [10, 5, 2, 1, 0.5, 0.2, 0.1]\n",
        "\n",
        "# Define degree of polynomial and train for each value of lambda\n",
        "train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
      ],
      "metadata": {
        "id": "VgzrTA3nsyIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting plot shows an initial $\\lambda$ of `10` and as you can see, the training error is worse than the baseline at that point. This implies that it is placing a huge penalty on the `w` parameters and this prevents the model from learning more complex patterns in your data. As you decrease $\\lambda$, the model loosens this restriction and the training error is able to approach the baseline performance."
      ],
      "metadata": {
        "id": "JcExbLSXuJSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixing High Variance\n",
        "\n",
        "You will now look at some things to try when your model has overfit the training set. The main objective is to have a model that generalizes well to new examples so you want to minimize the cross validation error."
      ],
      "metadata": {
        "id": "DpzN6KwZvaQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try increasing the regularization parameter\n",
        "In contrast to the last exercise above, setting a very small value of the regularization parameter will keep the model low bias but might not do much to improve the variance. As shown below, you can improve your cross validation error by increasing the value of  ð\n",
        " ."
      ],
      "metadata": {
        "id": "pLEi3pIRvkH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define lambdas to plot\n",
        "reg_params = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\n",
        "\n",
        "# Define degree of polynomial and train for each value of lambda\n",
        "train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
      ],
      "metadata": {
        "id": "JGI0TEWQvne4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try smaller sets of features\n",
        "\n",
        "You've already seen in the last lab that having too many polynomial terms can result in overfitting. You can reduce the number of such terms and see where you get the best balance of training and cross validation error. Another scenario where reducing the number of features would be helpful is when you have irrelevant features in your data. For example, patient IDs that hospitals give will not help in diagnosing a tumor so you should make sure to remove it from your training data.\n",
        "\n",
        "To illustrate how removing features can improve performance, you will do polynomial regression for 2 datasets: the same data you used above (2 features) and another with a random ID column (3 features). You can preview these using the cell below. Notice that 2 columns are identical and a 3rd one is added to include random numbers."
      ],
      "metadata": {
        "id": "T7OTe1D4wA8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the data file\n",
        "url_2 = '/content/drive/My Drive/Colab Notebooks/Data/c2_w3/c2w3_lab2_data2.csv'\n",
        "url_3 = '/content/drive/My Drive/Colab Notebooks/Data/c2_w3/c2w3_lab2_data3.csv'\n",
        "\n",
        "# Prepare dataset with randomID feature\n",
        "data_2 = np.loadtxt(url_2, delimiter = ',')\n",
        "data_3 = np.loadtxt(url_3, delimiter = ',')\n",
        "\n",
        "x_2 = data_2[:,0:2]\n",
        "y_2 = data_2[:,2]\n",
        "\n",
        "x_3 = data_3[:,0:3]\n",
        "y_3 = data_3[:,3]\n",
        "\n",
        "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
        "x_train, x_, y_train, y_ = train_test_split(x_2, y_2, test_size=0.40, random_state=80)\n",
        "\n",
        "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=80)\n",
        "\n",
        "# Preview the first 5 rows\n",
        "print(f\"first 5 rows of the training set with 2 features:\\n {x_train[:5]}\\n\")\n",
        "\n",
        "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
        "x_train, x_, y_train, y_ = train_test_split(x_3, y_3, test_size=0.40, random_state=80)\n",
        "\n",
        "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=80)\n",
        "\n",
        "# Preview the first 5 rows\n",
        "print(f\"first 5 rows of the training set with 3 features (1st column is a random ID):\\n {x_train[:5]}\\n\")"
      ],
      "metadata": {
        "id": "81RkYLuH0wj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you will train the models and plot the results. The solid lines in the plot show the errors for the data with 2 features while the dotted lines show the errors for the dataset with 3 features. As you can see, the one with 3 features has higher cross validation error especially as you introduce more polynomial terms. This is because the model is also trying to learn from the random IDs even though it has nothing to do with the target.\n",
        "\n",
        "Another way to look at it is to observe the points at degree=4. You'll notice that even though the *training error* is lower with 3 features, the *gap between the training error and cross validation error* is a lot wider than when you only use 2 features. This should also warn you that the model is overfitting."
      ],
      "metadata": {
        "id": "ZeP4cEzA22Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define properties of the 2 datasets\n",
        "file1 = {'filename':'/content/drive/My Drive/Colab Notebooks/Data/c2_w3/c2w3_lab2_data3.csv', 'label': '3 features', 'linestyle': 'dotted'}\n",
        "file2 = {'filename':'/content/drive/My Drive/Colab Notebooks/Data/c2_w3/c2w3_lab2_data2.csv', 'label': '2 features', 'linestyle': 'solid'}\n",
        "files = [file1, file2]\n",
        "\n",
        "# Train and plot for each dataset\n",
        "train_plot_diff_datasets(model, files, max_degree=4, baseline=250)"
      ],
      "metadata": {
        "id": "VCUhAoxv24LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get more training examples\n",
        "\n",
        "Lastly, you can try to minimize the cross validation error by getting more examples. In the cell below, you will train a 4th degree polynomial model then plot the *learning curve* of your model to see how the errors behave when you get more examples."
      ],
      "metadata": {
        "id": "MLe3_03q4VLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset\n",
        "x_train, y_train, x_cv, y_cv, x_test, y_test = prepare_dataset('/content/drive/My Drive/Colab Notebooks/Data/c2_w3/c2w3_lab2_data4.csv')\n",
        "print(f\"the shape of the entire training set (input) is: {x_train.shape}\")\n",
        "print(f\"the shape of the entire training set (target) is: {y_train.shape}\\n\")\n",
        "print(f\"the shape of the entire cross validation set (input) is: {x_cv.shape}\")\n",
        "print(f\"the shape of the entire cross validation set (target) is: {y_cv.shape}\\n\")\n",
        "\n",
        "# Instantiate the model class\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define the degree of polynomial and train the model using subsets of the dataset.\n",
        "train_plot_learning_curve(model, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
      ],
      "metadata": {
        "id": "M0bd-Wqu4oRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the results, it shows that the cross validation error starts to approach the training error as you increase the dataset size. Another insight you can get from this is that adding more examples will not likely solve a high bias problem. That's because the training error remains relatively flat even as the dataset increases."
      ],
      "metadata": {
        "id": "4N8HtQQU42sS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the functions we use above:"
      ],
      "metadata": {
        "id": "NzQ9VwazwEzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "#plt.style.use('./deeplearning.mplstyle')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def plot_dataset(x, y, title):\n",
        "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
        "    plt.rcParams[\"lines.markersize\"] = 12\n",
        "    plt.scatter(x, y, marker='x', c='r');\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "    plt.show()\n",
        "\n",
        "def plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title):\n",
        "    plt.scatter(x_train, y_train, marker='x', c='r', label='training');\n",
        "    plt.scatter(x_cv, y_cv, marker='o', c='b', label='cross validation');\n",
        "    plt.scatter(x_test, y_test, marker='^', c='g', label='test');\n",
        "    plt.title(\"input vs. target\")\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_train_cv_mses(degrees, train_mses, cv_mses, title):\n",
        "    degrees = range(1,11)\n",
        "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_bc_dataset(x, y, title):\n",
        "    for i in range(len(y)):\n",
        "        marker = 'x' if y[i] == 1 else 'o'\n",
        "        c = 'r' if y[i] == 1 else 'b'\n",
        "        plt.scatter(x[i,0], x[i,1], marker=marker, c=c);\n",
        "    plt.title(\"x1 vs x2\")\n",
        "    plt.xlabel(\"x1\");\n",
        "    plt.ylabel(\"x2\");\n",
        "    y_0 = mlines.Line2D([], [], color='r', marker='x', markersize=12, linestyle='None', label='y=1')\n",
        "    y_1 = mlines.Line2D([], [], color='b', marker='o', markersize=12, linestyle='None', label='y=0')\n",
        "    plt.title(title)\n",
        "    plt.legend(handles=[y_0, y_1])\n",
        "    plt.show()\n",
        "\n",
        "def build_models():\n",
        "\n",
        "    tf.random.set_seed(20)\n",
        "\n",
        "    model_1 = Sequential(\n",
        "        [\n",
        "            Dense(25, activation = 'relu'),\n",
        "            Dense(15, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_1'\n",
        "    )\n",
        "\n",
        "    model_2 = Sequential(\n",
        "        [\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_2'\n",
        "    )\n",
        "\n",
        "    model_3 = Sequential(\n",
        "        [\n",
        "            Dense(32, activation = 'relu'),\n",
        "            Dense(16, activation = 'relu'),\n",
        "            Dense(8, activation = 'relu'),\n",
        "            Dense(4, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_3'\n",
        "    )\n",
        "\n",
        "    model_list = [model_1, model_2, model_3]\n",
        "\n",
        "    return model_list\n",
        "\n",
        "\n",
        "# Not used in the lab. You can call this for the binary\n",
        "# classification problem if you set `from_logits=False`\n",
        "# when declaring the loss. With this, you will also not need\n",
        "# to call the `tf.math.sigmoid()` function in the loop\n",
        "# because the model output is already a probability\n",
        "def build_bc_models():\n",
        "\n",
        "    tf.random.set_seed(20)\n",
        "\n",
        "    model_1_bc = Sequential(\n",
        "        [\n",
        "            Dense(25, activation = 'relu'),\n",
        "            Dense(15, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_1_bc'\n",
        "    )\n",
        "\n",
        "    model_2_bc = Sequential(\n",
        "        [\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_2_bc'\n",
        "    )\n",
        "\n",
        "    model_3_bc = Sequential(\n",
        "        [\n",
        "            Dense(32, activation = 'relu'),\n",
        "            Dense(16, activation = 'relu'),\n",
        "            Dense(8, activation = 'relu'),\n",
        "            Dense(4, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_3_bc'\n",
        "    )\n",
        "\n",
        "    models_bc = [model_1_bc, model_2_bc, model_3_bc]\n",
        "\n",
        "    return models_bc\n",
        "\n",
        "\n",
        "def prepare_dataset(filename):\n",
        "\n",
        "    data = np.loadtxt(filename, delimiter=\",\")\n",
        "\n",
        "    x = data[:,:-1]\n",
        "    y = data[:,-1]\n",
        "\n",
        "    # Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
        "    x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=80)\n",
        "\n",
        "    # Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "    x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=80)\n",
        "\n",
        "    return x_train, y_train, x_cv, y_cv, x_test, y_test\n",
        "\n",
        "def train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "    degrees = range(1,max_degree+1)\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for degree in degrees:\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model.fit(X_train_mapped_scaled, y_train )\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv)\n",
        "        # Mehran: we use \"transform\" instead of \"fit_transform\" to use the mean\n",
        "        # and standard deviation of training set.\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
        "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
        "    plt.xticks(degrees)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for reg_param in reg_params:\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model = Ridge(alpha=reg_param)\n",
        "        model.fit(X_train_mapped_scaled, y_train)\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv)\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    reg_params = [str(x) for x in reg_params]\n",
        "    plt.plot(reg_params, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(reg_params, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(reg_params, np.repeat(baseline, len(reg_params)), linestyle='--', label='baseline')\n",
        "    plt.title(\"lambda vs. train and CV MSEs\")\n",
        "    plt.xlabel(\"lambda\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_diff_datasets(model, files, max_degree=10, baseline=None):\n",
        "\n",
        "    for file in files:\n",
        "\n",
        "        x_train, y_train, x_cv, y_cv, x_test, y_test = prepare_dataset(file['filename'])\n",
        "\n",
        "        train_mses = []\n",
        "        cv_mses = []\n",
        "        models = []\n",
        "        scalers = []\n",
        "        degrees = range(1,max_degree+1)\n",
        "\n",
        "        # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "        for degree in degrees:\n",
        "\n",
        "            # Add polynomial features to the training set\n",
        "            poly = PolynomialFeatures(degree, include_bias=False)\n",
        "            X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "            # Scale the training set\n",
        "            scaler_poly = StandardScaler()\n",
        "            X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "            scalers.append(scaler_poly)\n",
        "\n",
        "            # Create and train the model\n",
        "            model.fit(X_train_mapped_scaled, y_train )\n",
        "            models.append(model)\n",
        "\n",
        "            # Compute the training MSE\n",
        "            yhat = model.predict(X_train_mapped_scaled)\n",
        "            train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "            train_mses.append(train_mse)\n",
        "\n",
        "            # Add polynomial features and scale the cross-validation set\n",
        "            poly = PolynomialFeatures(degree, include_bias=False)\n",
        "            X_cv_mapped = poly.fit_transform(x_cv)\n",
        "            X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "            # Compute the cross-validation MSE\n",
        "            yhat = model.predict(X_cv_mapped_scaled)\n",
        "            cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "            cv_mses.append(cv_mse)\n",
        "\n",
        "        # Plot the results\n",
        "        plt.plot(degrees, train_mses, marker='o', c='r', linestyle=file['linestyle'], label=f\"{file['label']} training MSEs\");\n",
        "        plt.plot(degrees, cv_mses, marker='o', c='b', linestyle=file['linestyle'], label=f\"{file['label']} CV MSEs\");\n",
        "\n",
        "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
        "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
        "    plt.xticks(degrees)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_learning_curve(model, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "    num_samples_train_and_cv = []\n",
        "    percents = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for percent in percents:\n",
        "\n",
        "        num_samples_train = round(len(x_train) * (percent/100.0))\n",
        "        num_samples_cv = round(len(x_cv) * (percent/100.0))\n",
        "        num_samples_train_and_cv.append(num_samples_train + num_samples_cv)\n",
        "\n",
        "        x_train_sub = x_train[:num_samples_train]\n",
        "        y_train_sub = y_train[:num_samples_train]\n",
        "        x_cv_sub = x_cv[:num_samples_cv]\n",
        "        y_cv_sub = y_cv[:num_samples_cv]\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train_sub)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model.fit(X_train_mapped_scaled, y_train_sub)\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train_sub, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv_sub)\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv_sub, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    plt.plot(num_samples_train_and_cv, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(num_samples_train_and_cv, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(num_samples_train_and_cv, np.repeat(baseline, len(percents)), linestyle='--', label='baseline')\n",
        "    plt.title(\"number of examples vs. train and CV MSEs\")\n",
        "    plt.xlabel(\"total number of training and cv examples\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0S7ozf2txFNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W3_Lab09 (C2_W3_Assignment)\n"
      ],
      "metadata": {
        "id": "jMzOVtuzptju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Packages\n",
        "\n",
        "First, let's run the cell below to import all the packages that you will need during this assignment.\n",
        "- [numpy](https://numpy.org/) is the fundamental package for scientific computing Python.\n",
        "- [matplotlib](http://matplotlib.org) is a popular library to plot graphs in Python.\n",
        "- [scikitlearn](https://scikit-learn.org/stable/) is a basic library for data mining\n",
        "- [tensorflow](https://www.tensorflow.org/) a popular platform for machine learning."
      ],
      "metadata": {
        "id": "1-S3BVitqEtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import relu,linear\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "\n",
        "#from public_tests_a1 import *\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "#from assigment_utils import *\n",
        "\n",
        "tf.autograph.set_verbosity(0)"
      ],
      "metadata": {
        "id": "sZi7zeyQqHg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Evaluating a Learning Algorithm (Polynomial Regression)\n",
        "\n",
        "Let's say you have created a machine learning model and you find it *fits* your training data very well. You're done? Not quite. The goal of creating the model was to be able to predict values for <span style=\"color:blue\">*new* </span> examples.\n",
        "\n",
        "How can you test your model's performance on new data before deploying it?   \n",
        "The answer has two parts:\n",
        "* Split your original data set into \"Training\" and \"Test\" sets.\n",
        "    * Use the training data to fit the parameters of the model\n",
        "    * Use the test data to evaluate the model on *new* data\n",
        "* Develop an error function to evaluate your model."
      ],
      "metadata": {
        "id": "Tl4F2J8fr5k8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Splitting your data set\n",
        "Lectures advised reserving 20-40% of your data set for testing. Let's use an `sklearn` function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to perform the split. Double-check the shapes after running the following cell."
      ],
      "metadata": {
        "id": "eR6tJ2qlsIv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some data\n",
        "X,y,x_ideal,y_ideal = gen_data(18, 2, 0.7)\n",
        "print(\"X.shape\", X.shape, \"y.shape\", y.shape)\n",
        "\n",
        "#split the data using sklearn routine\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=1)\n",
        "print(\"X_train.shape\", X_train.shape, \"y_train.shape\", y_train.shape)\n",
        "print(\"X_test.shape\", X_test.shape, \"y_test.shape\", y_test.shape)"
      ],
      "metadata": {
        "id": "GYxZ34gnsO3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.1 Plot Train, Test sets\n",
        "You can see below the data points that will be part of training (in red) are intermixed with those that the model is not trained on (test). This particular data set is a quadratic function with noise added. The \"ideal\" curve is shown for reference."
      ],
      "metadata": {
        "id": "hwOckeuEszAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
        "ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
        "ax.set_title(\"Training, Test\",fontsize = 14)\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"y\")\n",
        "\n",
        "ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
        "ax.scatter(X_test, y_test,   color = dlc[\"dlblue\"],   label=\"test\")\n",
        "ax.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O0IJbUOxs5If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Error calculation for model evaluation, linear regression\n",
        "When *evaluating* a linear regression model, you average the squared error difference of the predicted values and the target values.\n",
        "\n",
        "$$ J_\\text{test}(\\mathbf{w},b) =\n",
        "            \\frac{1}{2m_\\text{test}}\\sum_{i=0}^{m_\\text{test}-1} ( f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}_\\text{test}) - y^{(i)}_\\text{test} )^2\n",
        "            \\tag{1}\n",
        "$$"
      ],
      "metadata": {
        "id": "S3uLaa_ztEEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1\n",
        "\n",
        "Below, create a function to evaluate the error on a data set for a linear regression model."
      ],
      "metadata": {
        "id": "q8jqxY7C_x3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C1\n",
        "# GRADED CELL: eval_mse\n",
        "def eval_mse(y, yhat):\n",
        "    \"\"\"\n",
        "    Calculate the mean squared error on a data set.\n",
        "    Args:\n",
        "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
        "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
        "    Returns:\n",
        "      err: (scalar)\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    err = 0.0\n",
        "    #for i in range(m):\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    err = (1/(2*m))*sum((y-yhat)**2)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return(err)"
      ],
      "metadata": {
        "id": "p8CKi7PJtLxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = np.array([2.4, 4.2])\n",
        "y_tmp = np.array([2.3, 4.1])\n",
        "eval_mse(y_hat, y_tmp)\n",
        "\n",
        "# BEGIN UNIT TEST\n",
        "test_eval_mse(eval_mse)\n",
        "# END UNIT TEST"
      ],
      "metadata": {
        "id": "CkEG6cUy_tJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Compare performance on training and test data\n",
        "Let's build a high degree polynomial model to minimize training error. This will use the linear_regression functions from `sklearn`. The code is in the imported utility file if you would like to see the details. The steps below are:\n",
        "* create and fit the model. ('fit' is another name for training or running gradient descent).\n",
        "* compute the error on the training data.\n",
        "* compute the error on the test data."
      ],
      "metadata": {
        "id": "6FkDZTKbDcf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model in sklearn, train on training data\n",
        "degree = 10\n",
        "lmodel = lin_model(degree)\n",
        "lmodel.fit(X_train, y_train)\n",
        "\n",
        "# predict on training data, find training error\n",
        "yhat = lmodel.predict(X_train)\n",
        "err_train = lmodel.mse(y_train, yhat)\n",
        "\n",
        "# predict on test data, find error\n",
        "yhat = lmodel.predict(X_test)\n",
        "err_test = lmodel.mse(y_test, yhat)"
      ],
      "metadata": {
        "id": "4lFciOK0Dfdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The computed error on the training set is substantially less than that of the test set."
      ],
      "metadata": {
        "id": "ledFnhQStIUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"training err {err_train:0.2f}, test err {err_test:0.2f}\")"
      ],
      "metadata": {
        "id": "498i2SNhDpP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following plot shows why this is. The model fits the training data very well. To do so, it has created a complex function. The test data was not part of the training and the model does a poor job of predicting on this data.  \n",
        "This model would be described as 1) is overfitting, 2) has high variance 3) 'generalizes' poorly."
      ],
      "metadata": {
        "id": "20WS3WozDypX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot predictions over data range\n",
        "x = np.linspace(0,int(X.max()),100)  # predict values for plot\n",
        "y_pred = lmodel.predict(x).reshape(-1,1)\n",
        "\n",
        "plt_train_test(X_train, y_train, X_test, y_test, x, y_pred, x_ideal, y_ideal, degree)"
      ],
      "metadata": {
        "id": "GqbNawJxD12W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test set error shows this model will not work well on new data. If you use the test error to guide improvements in the model, then the model will perform well on the test data... but the test data was meant to represent *new* data.\n",
        "You need yet another set of data to test new data performance.\n",
        "\n",
        "The proposal made during lecture is to separate data into three groups. The distribution of training, cross-validation and test sets shown in the below table is a typical distribution, but can be varied depending on the amount of data available.\n",
        "\n",
        "| data             | % of total | Description |\n",
        "|------------------|:----------:|:---------|\n",
        "| training         | 60         | Data used to tune model parameters $w$ and $b$ in training or fitting |\n",
        "| cross-validation | 20         | Data used to tune other model parameters like degree of polynomial, regularization or the architecture of a neural network.|\n",
        "| test             | 20         | Data used to test the model after tuning to gauge performance on new data |\n",
        "\n",
        "\n",
        "Let's generate three data sets below. We'll once again use `train_test_split` from `sklearn` but will call it twice to get three splits:"
      ],
      "metadata": {
        "id": "7fMqvtLLEKr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate  data\n",
        "X,y, x_ideal,y_ideal = gen_data(40, 5, 0.7)\n",
        "print(\"X.shape\", X.shape, \"y.shape\", y.shape)\n",
        "\n",
        "#split the data using sklearn routine\n",
        "X_train, X_, y_train, y_ = train_test_split(X,y,test_size=0.40, random_state=1)\n",
        "X_cv, X_test, y_cv, y_test = train_test_split(X_,y_,test_size=0.50, random_state=1)\n",
        "print(\"X_train.shape\", X_train.shape, \"y_train.shape\", y_train.shape)\n",
        "print(\"X_cv.shape\", X_cv.shape, \"y_cv.shape\", y_cv.shape)\n",
        "print(\"X_test.shape\", X_test.shape, \"y_test.shape\", y_test.shape)"
      ],
      "metadata": {
        "id": "GuBUN0DjEP4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Bias and Variance<img align=\"Right\" src=\"https://github.com/HomayounfarM/machine-learning-andrew-ng/blob/main/images/C2_W3_BiasVarianceDegree.png?raw=1\"  style=\" width:500px; padding: 10px 20px ; \">\n",
        " Above, it was clear the degree of the polynomial model was too high. How can you choose a good value? It turns out, as shown in the diagram, the training and cross-validation performance can provide guidance. By trying a range of degree values, the training and cross-validation performance can be evaluated. As the degree becomes too large, the cross-validation performance will start to degrade relative to the training performance. Let's try this on our example."
      ],
      "metadata": {
        "id": "DyM_22g2EdjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Plot Train, Cross-Validation, Test\n",
        "You can see below the datapoints that will be part of training (in red) are intermixed with those that the model is not trained on (test and cv)."
      ],
      "metadata": {
        "id": "hKN9utIPElQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
        "ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
        "ax.set_title(\"Training, CV, Test\",fontsize = 14)\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"y\")\n",
        "\n",
        "ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
        "ax.scatter(X_cv, y_cv,       color = dlc[\"dlorange\"], label=\"cv\")\n",
        "ax.scatter(X_test, y_test,   color = dlc[\"dlblue\"],   label=\"test\")\n",
        "ax.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KVHduwd4EoGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Finding the optimal degree\n",
        "In previous labs, you found that you could create a model capable of fitting complex curves by utilizing a polynomial (See Course1, Week2 Feature Engineering and Polynomial Regression Lab).  Further, you demonstrated that by increasing the *degree* of the polynomial, you could *create* overfitting. (See Course 1, Week3, Over-Fitting Lab). Let's use that knowledge here to test our ability to tell the difference between over-fitting and under-fitting.\n",
        "\n",
        "Let's train the model repeatedly, increasing the degree of the polynomial each iteration. Here, we're going to use the [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) linear regression model for speed and simplicity."
      ],
      "metadata": {
        "id": "2I1Hq05iE5yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_degree = 9\n",
        "err_train = np.zeros(max_degree)\n",
        "err_cv = np.zeros(max_degree)\n",
        "x = np.linspace(0,int(X.max()),100)\n",
        "y_pred = np.zeros((100,max_degree))  #columns are lines to plot\n",
        "\n",
        "for degree in range(max_degree):\n",
        "    lmodel = lin_model(degree+1)\n",
        "    lmodel.fit(X_train, y_train)\n",
        "    yhat = lmodel.predict(X_train)\n",
        "    err_train[degree] = lmodel.mse(y_train, yhat)\n",
        "    yhat = lmodel.predict(X_cv)\n",
        "    err_cv[degree] = lmodel.mse(y_cv, yhat)\n",
        "    y_pred[:,degree] = lmodel.predict(x)\n",
        "\n",
        "optimal_degree = np.argmin(err_cv)+1"
      ],
      "metadata": {
        "id": "KFEjQCbhE5TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close(\"all\")\n",
        "plt_optimal_degree(X_train, y_train, X_cv, y_cv, x, y_pred, x_ideal, y_ideal,\n",
        "                   err_train, err_cv, optimal_degree, max_degree)"
      ],
      "metadata": {
        "id": "RelxymHDFCE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot above demonstrates that separating data into two groups, data the model is trained on and data the model has not been trained on, can be used to determine if the model is underfitting or overfitting. In our example, we created a variety of models varying from underfitting to overfitting by increasing the degree of the polynomial used.\n",
        "- On the left plot, the solid lines represent the predictions from these models. A polynomial model with degree 1 produces a straight line that intersects very few data points, while the maximum degree hews very closely to every data point.\n",
        "- on the right:\n",
        "    - the error on the trained data (blue) decreases as the model complexity increases as expected\n",
        "    - the error of the cross-validation data decreases initially as the model starts to conform to the data, but then increases as the model starts to over-fit on the training data (fails to *generalize*).     \n",
        "    \n",
        "It's worth noting that the curves in these examples as not as smooth as one might draw for a lecture. It's clear the specific data points assigned to each group can change your results significantly. The general trend is what is important."
      ],
      "metadata": {
        "id": "2UQx4bkPFt57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Tuning Regularization.\n",
        "In previous labs, you have utilized *regularization* to reduce overfitting. Similar to degree, one can use the same methodology to tune the regularization parameter lambda ($\\lambda$).\n",
        "\n",
        "Let's demonstrate this by starting with a high degree polynomial and varying the regularization parameter."
      ],
      "metadata": {
        "id": "z-k0NcJZFy9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_range = np.array([0.0, 1e-6, 1e-5, 1e-4,1e-3,1e-2, 1e-1,1,10,100])\n",
        "num_steps = len(lambda_range)\n",
        "degree = 10\n",
        "err_train = np.zeros(num_steps)\n",
        "err_cv = np.zeros(num_steps)\n",
        "x = np.linspace(0,int(X.max()),100)\n",
        "y_pred = np.zeros((100,num_steps))  #columns are lines to plot\n",
        "\n",
        "for i in range(num_steps):\n",
        "    lambda_= lambda_range[i]\n",
        "    lmodel = lin_model(degree, regularization=True, lambda_=lambda_)\n",
        "    lmodel.fit(X_train, y_train)\n",
        "    yhat = lmodel.predict(X_train)\n",
        "    err_train[i] = lmodel.mse(y_train, yhat)\n",
        "    yhat = lmodel.predict(X_cv)\n",
        "    err_cv[i] = lmodel.mse(y_cv, yhat)\n",
        "    y_pred[:,i] = lmodel.predict(x)\n",
        "\n",
        "optimal_reg_idx = np.argmin(err_cv)"
      ],
      "metadata": {
        "id": "DZYxB_CUGB5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close(\"all\")\n",
        "plt_tune_regularization(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, optimal_reg_idx, lambda_range)"
      ],
      "metadata": {
        "id": "noshOcoiGF-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, the plots show that as regularization increases, the model moves from a high variance (overfitting) model to a high bias (underfitting) model. The vertical line in the right plot shows the optimal value of lambda. In this example, the polynomial degree was set to 10."
      ],
      "metadata": {
        "id": "g-fKpA_vGTvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Getting more data: Increasing Training Set Size (m)\n",
        "When a model is overfitting (high variance), collecting additional data can improve performance. Let's try that here."
      ],
      "metadata": {
        "id": "XxmYrKG4GZVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range,degree = tune_m()\n",
        "plt_tune_m(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range, degree)"
      ],
      "metadata": {
        "id": "l8cK2LE7Gd4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above plots show that when a model has high variance and is overfitting, adding more examples improves performance. Note the curves on the left plot. The final curve with the highest value of $m$ is a smooth curve that is in the center of the data. On the right, as the number of examples increases, the performance of the training set and cross-validation set converge to similar values. Note that the curves are not as smooth as one might see in a lecture. That is to be expected. The trend remains clear: more data improves generalization.\n",
        "\n",
        "> Note that adding more examples when the model has high bias (underfitting) does not improve performance."
      ],
      "metadata": {
        "id": "7N9xIx7FG3BP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Evaluating a Learning Algorithm (Neural Network)\n",
        "Above, you tuned aspects of a polynomial regression model. Here, you will work with a neural network model. Let's start by creating a classification data set."
      ],
      "metadata": {
        "id": "BzTDRoYBG9Be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Data Set\n",
        "Run the cell below to generate a data set and split it into training, cross-validation (CV) and test sets. In this example, we're increasing the percentage of cross-validation data points for emphasis.  "
      ],
      "metadata": {
        "id": "mmvMyKGdHBCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and split data set\n",
        "X, y, centers, classes, std = gen_blobs()\n",
        "\n",
        "# split the data. Large CV population for demonstration\n",
        "X_train, X_, y_train, y_ = train_test_split(X,y,test_size=0.50, random_state=1)\n",
        "X_cv, X_test, y_cv, y_test = train_test_split(X_,y_,test_size=0.20, random_state=1)\n",
        "print(\"X_train.shape:\", X_train.shape, \"X_cv.shape:\", X_cv.shape, \"X_test.shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "Mi5vniQpHG7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_train_eq_dist(X_train, y_train,classes, X_cv, y_cv, centers, std)"
      ],
      "metadata": {
        "id": "VR7-U3utHJLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, you can see the data on the left. There are six clusters identified by color. Both training points (dots) and cross-validataion points (triangles) are shown. The interesting points are those that fall in ambiguous locations where either cluster might consider them members. What would you expect a neural network model to do? What would be an example of overfitting? underfitting?  \n",
        "On the right is an example of an 'ideal' model, or a model one might create knowing the source of the data. The lines represent 'equal distance' boundaries where the distance between center points is equal. It's worth noting that this model would \"misclassify\" roughly 8% of the total data set."
      ],
      "metadata": {
        "id": "miThk4GMHqGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Evaluating categorical model by calculating classification error\n",
        "The evaluation function for categorical models used here is simply the fraction of incorrect predictions:  \n",
        "$$ J_{cv} =\\frac{1}{m}\\sum_{i=0}^{m-1}\n",
        "\\begin{cases}\n",
        "    1, & \\text{if $\\hat{y}^{(i)} \\neq y^{(i)}$}\\\\\n",
        "    0, & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "<a name=\"ex02\"></a>\n",
        "### Exercise 2\n",
        "\n",
        "Below, complete the routine to calculate classification error. Note, in this lab, target values are the index of the category and are not [one-hot encoded](https://en.wikipedia.org/wiki/One-hot)."
      ],
      "metadata": {
        "id": "9UHkO_3sH8zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C2\n",
        "# GRADED CELL: eval_cat_err\n",
        "def eval_cat_err(y, yhat):\n",
        "    \"\"\"\n",
        "    Calculate the categorization error\n",
        "    Args:\n",
        "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
        "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
        "    Returns:|\n",
        "      cerr: (scalar)\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    incorrect = 0\n",
        "    for i in range(m):\n",
        "    ### START CODE HERE ###\n",
        "      if(y[i] != yhat[i]):\n",
        "        incorrect+=1\n",
        "    cerr = (1/m)*incorrect\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return(cerr)"
      ],
      "metadata": {
        "id": "4O5T9cDMH_yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = np.array([1, 2, 0])\n",
        "y_tmp = np.array([1, 2, 3])\n",
        "print(f\"categorization error {np.squeeze(eval_cat_err(y_hat, y_tmp)):0.3f}, expected:0.333\" )\n",
        "y_hat = np.array([[1], [2], [0], [3]])\n",
        "y_tmp = np.array([[1], [2], [1], [3]])\n",
        "print(f\"categorization error {np.squeeze(eval_cat_err(y_hat, y_tmp)):0.3f}, expected:0.250\" )\n",
        "\n",
        "# BEGIN UNIT TEST\n",
        "test_eval_cat_err(eval_cat_err)\n",
        "# END UNIT TEST"
      ],
      "metadata": {
        "id": "9YKcI7X4ICh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 - Model Complexity\n",
        "Below, you will build two models. A complex model and a simple model. You will evaluate the models to determine if they are likely to overfit or underfit.\n",
        "\n",
        "###  5.1 Complex model\n",
        "\n",
        "<a name=\"ex03\"></a>\n",
        "### Exercise 3\n",
        "Below, compose a three-layer model:\n",
        "* Dense layer with 120 units, relu activation\n",
        "* Dense layer with 40 units, relu activation\n",
        "* Dense layer with 6 units and a linear activation (not softmax)  \n",
        "Compile using\n",
        "* loss with `SparseCategoricalCrossentropy`, remember to use  `from_logits=True`\n",
        "* Adam optimizer with learning rate of 0.01."
      ],
      "metadata": {
        "id": "Je8G1MU4QIFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C3\n",
        "# GRADED CELL: model\n",
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "model = Sequential(\n",
        "    [\n",
        "        ### START CODE HERE ###\n",
        "    Dense(units=120, activation='relu'),\n",
        "    Dense(units=40, activation='relu'),\n",
        "    Dense(units=6, activation='linear')\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    ], name=\"Complex\"\n",
        ")\n",
        "model.compile(\n",
        "    ### START CODE HERE ###\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=Adam(learning_rate=1e-2)\n",
        "    ### END CODE HERE ###\n",
        ")"
      ],
      "metadata": {
        "id": "9lVQc_XVQME9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN UNIT TEST\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=1000\n",
        ")\n",
        "# END UNIT TEST"
      ],
      "metadata": {
        "id": "81DUX8hlS8a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN UNIT TEST\n",
        "model.summary()\n",
        "\n",
        "model_test(model, classes, X_train.shape[1])\n",
        "# END UNIT TEST"
      ],
      "metadata": {
        "id": "Ax03Cw_1TGAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make a model for plotting routines to call\n",
        "model_predict = lambda Xl: np.argmax(tf.nn.softmax(model.predict(Xl)).numpy(),axis=1)\n",
        "plt_nn(model_predict,X_train,y_train, classes, X_cv, y_cv, suptitle=\"Complex Model\")"
      ],
      "metadata": {
        "id": "q8Ehgl8RTViG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model has worked very hard to capture outliers of each category. As a result, it has miscategorized some of the cross-validation data. Let's calculate the classification error."
      ],
      "metadata": {
        "id": "0qFmh8TMTd4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_cerr_complex = eval_cat_err(y_train, model_predict(X_train))\n",
        "cv_cerr_complex = eval_cat_err(y_cv, model_predict(X_cv))\n",
        "print(f\"categorization error, training, complex model: {training_cerr_complex:0.3f}\")\n",
        "print(f\"categorization error, cv,       complex model: {cv_cerr_complex:0.3f}\")"
      ],
      "metadata": {
        "id": "3tHs3IE-TmmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5.1 Simple model\n",
        "Now, let's try a simple model\n",
        "\n",
        "\n",
        "### Exercise 4\n",
        "\n",
        "Below, compose a two-layer model:\n",
        "* Dense layer with 6 units, relu activation\n",
        "* Dense layer with 6 units and a linear activation.\n",
        "Compile using\n",
        "* loss with `SparseCategoricalCrossentropy`, remember to use  `from_logits=True`\n",
        "* Adam optimizer with learning rate of 0.01."
      ],
      "metadata": {
        "id": "Squu9P5cT1z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C4\n",
        "# GRADED CELL: model_s\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "model_s = Sequential(\n",
        "    [\n",
        "        ### START CODE HERE ###\n",
        "    Dense(units=6, activation='relu'),\n",
        "    Dense(units=6, activation='linear'),\n",
        "        ### END CODE HERE ###\n",
        "    ], name = \"Simple\"\n",
        ")\n",
        "model_s.compile(\n",
        "    ### START CODE HERE ###\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=Adam(learning_rate=1e-2)\n",
        "    ### START CODE HERE ###\n",
        ")"
      ],
      "metadata": {
        "id": "yZ0dAZRfUaJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "\n",
        "# BEGIN UNIT TEST\n",
        "model_s.fit(\n",
        "    X_train,y_train,\n",
        "    epochs=1000\n",
        ")\n",
        "# END UNIT TEST"
      ],
      "metadata": {
        "id": "RWthlTEnT1N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN UNIT TEST\n",
        "model_s.summary()\n",
        "\n",
        "model_s_test(model_s, classes, X_train.shape[1])\n",
        "# END UNIT TEST"
      ],
      "metadata": {
        "id": "WDZ9JDErUhcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make a model for plotting routines to call\n",
        "model_predict_s = lambda Xl: np.argmax(tf.nn.softmax(model_s.predict(Xl)).numpy(),axis=1)\n",
        "plt_nn(model_predict_s,X_train,y_train, classes, X_cv, y_cv, suptitle=\"Simple Model\")"
      ],
      "metadata": {
        "id": "DYDEcgLBUowK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This simple models does pretty well. Let's calculate the classification error."
      ],
      "metadata": {
        "id": "MnLwOGvzUtNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_cerr_simple = eval_cat_err(y_train, model_predict_s(X_train))\n",
        "cv_cerr_simple = eval_cat_err(y_cv, model_predict_s(X_cv))\n",
        "print(f\"categorization error, training, simple model, {training_cerr_simple:0.3f}, complex model: {training_cerr_complex:0.3f}\" )\n",
        "print(f\"categorization error, cv,       simple model, {cv_cerr_simple:0.3f}, complex model: {cv_cerr_complex:0.3f}\" )"
      ],
      "metadata": {
        "id": "eezwJbZQU5Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our simple model has a little higher classification error on training data but does better on cross-validation data than the more complex model."
      ],
      "metadata": {
        "id": "D4AYAO0tU9Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 - Regularization\n",
        "As in the case of polynomial regression, one can apply regularization to moderate the impact of a more complex model. Let's try this below.\n",
        "\n",
        "### Exercise 5\n",
        "\n",
        "Reconstruct your complex model, but this time include regularization.\n",
        "Below, compose a three-layer model:\n",
        "* Dense layer with 120 units, relu activation, `kernel_regularizer=tf.keras.regularizers.l2(0.1)`\n",
        "* Dense layer with 40 units, relu activation, `kernel_regularizer=tf.keras.regularizers.l2(0.1)`\n",
        "* Dense layer with 6 units and a linear activation.\n",
        "Compile using\n",
        "* loss with `SparseCategoricalCrossentropy`, remember to use  `from_logits=True`\n",
        "* Adam optimizer with learning rate of 0.01."
      ],
      "metadata": {
        "id": "sBM_vdDHVM8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C5\n",
        "# GRADED CELL: model_r\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "model_r = Sequential(\n",
        "    [\n",
        "        ### START CODE HERE ###\n",
        "    Dense(units=120, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
        "    Dense(units=40, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
        "    Dense(units=6, activation='linear')\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "    ], name= None\n",
        ")\n",
        "model_r.compile(\n",
        "    ### START CODE HERE ###\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=Adam(learning_rate=1e-2)\n",
        "    ### START CODE HERE ###\n",
        ")"
      ],
      "metadata": {
        "id": "jTaEEPdMVyML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN UNIT TEST\n",
        "model_r.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=1000\n",
        ")\n",
        "# END UNIT TEST"
      ],
      "metadata": {
        "id": "zh3MIS5uV4eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BEGIN UNIT TEST\n",
        "model_r.summary()\n",
        "\n",
        "model_r_test(model_r, classes, X_train.shape[1])\n",
        "# END UNIT TEST"
      ],
      "metadata": {
        "id": "-SL4QyepV8mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make a model for plotting routines to call\n",
        "model_predict_r = lambda Xl: np.argmax(tf.nn.softmax(model_r.predict(Xl)).numpy(),axis=1)\n",
        "\n",
        "plt_nn(model_predict_r, X_train,y_train, classes, X_cv, y_cv, suptitle=\"Regularized\")"
      ],
      "metadata": {
        "id": "1-MqFxkyZoj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results look very similar to the 'ideal' model. Let's check classification error."
      ],
      "metadata": {
        "id": "_vi7483GZty3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_cerr_reg = eval_cat_err(y_train, model_predict_r(X_train))\n",
        "cv_cerr_reg = eval_cat_err(y_cv, model_predict_r(X_cv))\n",
        "test_cerr_reg = eval_cat_err(y_test, model_predict_r(X_test))\n",
        "print(f\"categorization error, training, regularized: {training_cerr_reg:0.3f}, simple model, {training_cerr_simple:0.3f}, complex model: {training_cerr_complex:0.3f}\" )\n",
        "print(f\"categorization error, cv,       regularized: {cv_cerr_reg:0.3f}, simple model, {cv_cerr_simple:0.3f}, complex model: {cv_cerr_complex:0.3f}\" )"
      ],
      "metadata": {
        "id": "J3iBliY8Zwb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simple model is a bit better in the training set than the regularized model but worse in the cross validation set."
      ],
      "metadata": {
        "id": "nbXxl6XrZzl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 - Iterate to find optimal regularization value\n",
        "As you did in linear regression, you can try many regularization values. This code takes several minutes to run. If you have time, you can run it and check the results. If not, you have completed the graded parts of the assignment!"
      ],
      "metadata": {
        "id": "ioNK1xJ7Z3MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)\n",
        "lambdas = [0.0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "models=[None] * len(lambdas)\n",
        "\n",
        "for i in range(len(lambdas)):\n",
        "    lambda_ = lambdas[i]\n",
        "    models[i] =  Sequential(\n",
        "        [\n",
        "            Dense(120, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
        "            Dense(40, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
        "            Dense(classes, activation = 'linear')\n",
        "        ]\n",
        "    )\n",
        "    models[i].compile(\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "    )\n",
        "\n",
        "    models[i].fit(\n",
        "        X_train,y_train,\n",
        "        epochs=1000\n",
        "    )\n",
        "    print(f\"Finished lambda = {lambda_}\")"
      ],
      "metadata": {
        "id": "FGLj20-CZ5oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_iterate(lambdas, models, X_train, y_train, X_cv, y_cv)"
      ],
      "metadata": {
        "id": "Fxpg06atZ85O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As regularization is increased, the performance of the model on the training and cross-validation data sets converge. For this data set and model, lambda > 0.01 seems to be a reasonable choice."
      ],
      "metadata": {
        "id": "mHx7EpE0Z_Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Test\n",
        "Let's try our optimized models on the test set and compare them to 'ideal' performance."
      ],
      "metadata": {
        "id": "jItxrZF-aCqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt_compare(X_test,y_test, classes, model_predict_s, model_predict_r, centers)"
      ],
      "metadata": {
        "id": "iCi0tbReaFPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our test set is small and seems to have a number of outliers so classification error is high. However, the performance of our optimized models is comparable to ideal performance."
      ],
      "metadata": {
        "id": "L8OVaMuLaH8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the functions we use above:"
      ],
      "metadata": {
        "id": "5PMUo_rpq79a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "assignment_utils.py\n",
        "contains routines used by C2_W3 Assignments\n",
        "\"\"\"\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
        "from matplotlib.widgets import Button, CheckButtons\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "from ipywidgets import Output\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC')\n",
        "dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC'\n",
        "dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\n",
        "#plt.style.use('./deeplearning.mplstyle')\n",
        "\n",
        "# --- Assignment ----------------------------------------\n",
        "def gen_data(m, seed=1, scale=0.7):\n",
        "    \"\"\" generate a data set based on a x^2 with added noise \"\"\"\n",
        "    c = 0\n",
        "    x_train = np.linspace(0,49,m)\n",
        "    np.random.seed(seed)\n",
        "    y_ideal = x_train**2 + c\n",
        "    y_train = y_ideal + scale * y_ideal*(np.random.sample((m,))-0.5)\n",
        "    x_ideal = x_train #for redraw when new data included in X\n",
        "    return x_train, y_train, x_ideal, y_ideal\n",
        "\n",
        "def gen_blobs():\n",
        "    classes = 6\n",
        "    m = 800\n",
        "    std = 0.4\n",
        "    centers = np.array([[-1, 0], [1, 0], [0, 1], [0, -1],  [-2,1],[-2,-1]])\n",
        "    X, y = make_blobs(n_samples=m, centers=centers, cluster_std=std, random_state=2, n_features=2)\n",
        "    return (X, y, centers, classes, std)\n",
        "\n",
        "class lin_model:\n",
        "    def __init__(self, degree, regularization = False, lambda_=0):\n",
        "        if regularization:\n",
        "            self.linear_model = Ridge(alpha=lambda_)\n",
        "        else:\n",
        "            self.linear_model = LinearRegression()\n",
        "        self.poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def fit(self, X_train,y_train):\n",
        "        ''' just fits the data. mapping and scaling are not repeated '''\n",
        "        X_train_mapped = self.poly.fit_transform(X_train.reshape(-1,1))\n",
        "        X_train_mapped_scaled = self.scaler.fit_transform(X_train_mapped)\n",
        "        self.linear_model.fit(X_train_mapped_scaled, y_train )\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_mapped = self.poly.transform(X.reshape(-1,1))\n",
        "        X_mapped_scaled = self.scaler.transform(X_mapped)\n",
        "        yhat = self.linear_model.predict(X_mapped_scaled)\n",
        "        return(yhat)\n",
        "\n",
        "    def mse(self, y, yhat):\n",
        "        err = mean_squared_error(y,yhat)/2   #sklean doesn't have div by 2\n",
        "        return (err)\n",
        "\n",
        "def plt_train_test(X_train, y_train, X_test, y_test, x, y_pred, x_ideal, y_ideal, degree):\n",
        "    fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "\n",
        "    ax.set_title(\"Poor Performance on Test Data\",fontsize = 12)\n",
        "    ax.set_xlabel(\"x\")\n",
        "    ax.set_ylabel(\"y\")\n",
        "\n",
        "    ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
        "    ax.scatter(X_test, y_test,       color = dlc[\"dlblue\"], label=\"test\")\n",
        "    ax.set_xlim(ax.get_xlim())\n",
        "    ax.set_ylim(ax.get_ylim())\n",
        "    ax.plot(x, y_pred,  lw=0.5, label=f\"predicted, degree={degree}\")\n",
        "    ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
        "    ax.legend(loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plt_optimal_degree(X_train, y_train, X_cv, y_cv, x, y_pred, x_ideal, y_ideal, err_train, err_cv, optimal_degree, max_degree):\n",
        "    fig, ax = plt.subplots(1,2,figsize=(8,4))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "\n",
        "    ax[0].set_title(\"predictions vs data\",fontsize = 12)\n",
        "    ax[0].set_xlabel(\"x\")\n",
        "    ax[0].set_ylabel(\"y\")\n",
        "\n",
        "    ax[0].plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
        "    ax[0].scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
        "    ax[0].scatter(X_cv, y_cv,       color = dlc[\"dlorange\"], label=\"cv\")\n",
        "    ax[0].set_xlim(ax[0].get_xlim())\n",
        "    ax[0].set_ylim(ax[0].get_ylim())\n",
        "    for i in range(0,max_degree):\n",
        "        ax[0].plot(x, y_pred[:,i],  lw=0.5, label=f\"{i+1}\")\n",
        "    ax[0].legend(loc='upper left')\n",
        "\n",
        "    ax[1].set_title(\"error vs degree\",fontsize = 12)\n",
        "    cpts = list(range(1, max_degree+1))\n",
        "    ax[1].plot(cpts, err_train[0:], marker='o',label=\"train error\", lw=2,  color = dlc[\"dlblue\"])\n",
        "    ax[1].plot(cpts, err_cv[0:],    marker='o',label=\"cv error\",  lw=2, color = dlc[\"dlorange\"])\n",
        "    ax[1].set_ylim(*ax[1].get_ylim())\n",
        "    ax[1].axvline(optimal_degree, lw=1, color = dlc[\"dlmagenta\"])\n",
        "    ax[1].annotate(\"optimal degree\", xy=(optimal_degree,80000),xycoords='data',\n",
        "                xytext=(0.3, 0.8), textcoords='axes fraction', fontsize=10,\n",
        "                   arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\",\n",
        "                                   color=dlc['dldarkred'], lw=1))\n",
        "    ax[1].set_xlabel(\"degree\")\n",
        "    ax[1].set_ylabel(\"error\")\n",
        "    ax[1].legend()\n",
        "    fig.suptitle(\"Find Optimal Degree\",fontsize = 12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plt_tune_regularization(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, optimal_reg_idx, lambda_range):\n",
        "    fig, ax = plt.subplots(1,2,figsize=(8,4))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "\n",
        "    ax[0].set_title(\"predictions vs data\",fontsize = 12)\n",
        "    ax[0].set_xlabel(\"x\")\n",
        "    ax[0].set_ylabel(\"y\")\n",
        "\n",
        "    ax[0].scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
        "    ax[0].scatter(X_cv, y_cv,       color = dlc[\"dlorange\"], label=\"cv\")\n",
        "    ax[0].set_xlim(ax[0].get_xlim())\n",
        "    ax[0].set_ylim(ax[0].get_ylim())\n",
        "#   ax[0].plot(x, y_pred[:,:],  lw=0.5, label=[f\"$\\lambda =${i}\" for i in lambda_range])\n",
        "    for i in (0,3,7,9):\n",
        "        ax[0].plot(x, y_pred[:,i],  lw=0.5, label=f\"$\\lambda =${lambda_range[i]}\")\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].set_title(\"error vs regularization\",fontsize = 12)\n",
        "    ax[1].plot(lambda_range, err_train[:], label=\"train error\", color = dlc[\"dlblue\"])\n",
        "    ax[1].plot(lambda_range, err_cv[:],    label=\"cv error\",    color = dlc[\"dlorange\"])\n",
        "    ax[1].set_xscale('log')\n",
        "    ax[1].set_ylim(*ax[1].get_ylim())\n",
        "    opt_x = lambda_range[optimal_reg_idx]\n",
        "    ax[1].vlines(opt_x, *ax[1].get_ylim(), color = \"black\", lw=1)\n",
        "    ax[1].annotate(\"optimal lambda\", (opt_x,150000), xytext=(-80,10), textcoords=\"offset points\",\n",
        "                  arrowprops={'arrowstyle':'simple'})\n",
        "    ax[1].set_xlabel(\"regularization (lambda)\")\n",
        "    ax[1].set_ylabel(\"error\")\n",
        "    fig.suptitle(\"Tuning Regularization\",fontsize = 12)\n",
        "    ax[1].text(0.05,0.44,\"High\\nVariance\",fontsize=12, ha='left',transform=ax[1].transAxes,color = dlc[\"dlblue\"])\n",
        "    ax[1].text(0.95,0.44,\"High\\nBias\",    fontsize=12, ha='right',transform=ax[1].transAxes,color = dlc[\"dlblue\"])\n",
        "    ax[1].legend(loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def tune_m():\n",
        "    \"\"\" tune the number of examples to reduce overfitting \"\"\"\n",
        "    m = 50\n",
        "    m_range = np.array(m*np.arange(1,16))\n",
        "    num_steps = m_range.shape[0]\n",
        "    degree = 16\n",
        "    err_train = np.zeros(num_steps)\n",
        "    err_cv = np.zeros(num_steps)\n",
        "    y_pred = np.zeros((100,num_steps))\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        X, y, y_ideal, x_ideal = gen_data(m_range[i],5,0.7)\n",
        "        x = np.linspace(0,int(X.max()),100)\n",
        "        X_train, X_, y_train, y_ = train_test_split(X,y,test_size=0.40, random_state=1)\n",
        "        X_cv, X_test, y_cv, y_test = train_test_split(X_,y_,test_size=0.50, random_state=1)\n",
        "\n",
        "        lmodel = lin_model(degree)  # no regularization\n",
        "        lmodel.fit(X_train, y_train)\n",
        "        yhat = lmodel.predict(X_train)\n",
        "        err_train[i] = lmodel.mse(y_train, yhat)\n",
        "        yhat = lmodel.predict(X_cv)\n",
        "        err_cv[i] = lmodel.mse(y_cv, yhat)\n",
        "        y_pred[:,i] = lmodel.predict(x)\n",
        "    return(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range,degree)\n",
        "\n",
        "def plt_tune_m(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range, degree):\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize=(8,4))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "\n",
        "    ax[0].set_title(\"predictions vs data\",fontsize = 12)\n",
        "    ax[0].set_xlabel(\"x\")\n",
        "    ax[0].set_ylabel(\"y\")\n",
        "\n",
        "    ax[0].scatter(X_train, y_train, color = \"red\",           s=3, label=\"train\", alpha=0.4)\n",
        "    ax[0].scatter(X_cv, y_cv,       color = dlc[\"dlorange\"], s=3, label=\"cv\",    alpha=0.4)\n",
        "    ax[0].set_xlim(ax[0].get_xlim())\n",
        "    ax[0].set_ylim(ax[0].get_ylim())\n",
        "    for i in range(0,len(m_range),3):\n",
        "        ax[0].plot(x, y_pred[:,i],  lw=1, label=f\"$m =${m_range[i]}\")\n",
        "    ax[0].legend(loc='upper left')\n",
        "    ax[0].text(0.05,0.5,f\"degree = {degree}\", fontsize=10, ha='left',transform=ax[0].transAxes,color = dlc[\"dlblue\"])\n",
        "\n",
        "    ax[1].set_title(\"error vs number of examples\",fontsize = 12)\n",
        "    ax[1].plot(m_range, err_train[:], label=\"train error\", color = dlc[\"dlblue\"])\n",
        "    ax[1].plot(m_range, err_cv[:],    label=\"cv error\",    color = dlc[\"dlorange\"])\n",
        "    ax[1].set_xlabel(\"Number of Examples (m)\")\n",
        "    ax[1].set_ylabel(\"error\")\n",
        "    fig.suptitle(\"Tuning number of examples\",fontsize = 12)\n",
        "    ax[1].text(0.05,0.5,\"High\\nVariance\",        fontsize=12, ha='left',transform=ax[1].transAxes,color = dlc[\"dlblue\"])\n",
        "    ax[1].text(0.95,0.5,\"Good \\nGeneralization\", fontsize=12, ha='right',transform=ax[1].transAxes,color = dlc[\"dlblue\"])\n",
        "    ax[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# cm.Paired has 12 colors, alternating light and dark.\n",
        "#https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
        "dkcolors = plt.cm.Paired((1,3,7,9,5,11))  # pick paired colors\n",
        "ltcolors = plt.cm.Paired((0,2,6,8,4,10))\n",
        "dkcolors_map = mpl.colors.ListedColormap(dkcolors) # turn colors into a color map\n",
        "ltcolors_map = mpl.colors.ListedColormap(ltcolors) # These have 6 entries - see lt_colors_map.colors\n",
        "\n",
        "def plt_mc_data(ax, X, y, classes,  class_labels=None, map=plt.cm.Paired, legend=False, size=50, m='o'):\n",
        "    normy = mpl.colors.Normalize(vmin=0, vmax=classes)\n",
        "    for i in range(classes):\n",
        "        idx = np.where(y == i)\n",
        "        label = class_labels[i] if class_labels else \"c{}\".format(i)\n",
        "        ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n",
        "                   color=map(normy(i)),\n",
        "                   s=size, label=label)\n",
        "    if legend: ax.legend(loc='lower right')\n",
        "    ax.axis('equal')\n",
        "\n",
        "\n",
        "#Plot a multi-class categorical decision boundary\n",
        "# This version handles a non-vector prediction (adds a for-loop over points)\n",
        "def plot_cat_decision_boundary(ax, X,predict , class_labels=None, legend=False, vector=True, color='g', lw = 1):\n",
        "\n",
        "    # create a mesh to points to plot\n",
        "    pad = 0.5\n",
        "    x_min, x_max = X[:, 0].min() - pad, X[:, 0].max() + pad\n",
        "    y_min, y_max = X[:, 1].min() - pad, X[:, 1].max() + pad\n",
        "    h = max(x_max-x_min, y_max-y_min)/200\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    #print(\"points\", points.shape)\n",
        "    #make predictions for each point in mesh\n",
        "    if vector:\n",
        "        Z = predict(points)\n",
        "    else:\n",
        "        Z = np.zeros((len(points),))\n",
        "        for i in range(len(points)):\n",
        "            Z[i] = predict(points[i].reshape(1,2))\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    #contour plot highlights boundaries between values - classes in this case\n",
        "    ax.contour(xx, yy, Z, colors=color, linewidths=lw)\n",
        "    ax.axis('tight')\n",
        "\n",
        "def recat(pt, origins):\n",
        "    \"\"\" categorize a point based on distance from origin of clusters \"\"\"\n",
        "    nclusters = len(origins)\n",
        "    min_dist = 10000\n",
        "    y_new = None\n",
        "    for j in range(nclusters):\n",
        "        temp = origins[j] - pt.reshape(2,)\n",
        "        #print(temp.shape,origins[j].shape)\n",
        "        dist = np.sqrt(np.dot(temp.T, temp))\n",
        "        if dist < min_dist:\n",
        "            y_new = j\n",
        "            min_dist = dist\n",
        "    return(y_new)\n",
        "\n",
        "def plt_train_eq_dist(X_train,y_train,classes, X_cv,   y_cv, centers, std):\n",
        "    css = np.unique(y_train)\n",
        "    fig,ax = plt.subplots(1,2,figsize=(8,4))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "    plt_mc_data(ax[0], X_train,y_train,classes, map=dkcolors_map, legend=True, size=50)\n",
        "    plt_mc_data(ax[0], X_cv,   y_cv,   classes, map=ltcolors_map, legend=True, m=\"<\")\n",
        "    ax[0].set_title(\"Training, CV Data\")\n",
        "    for c in css:\n",
        "        circ = plt.Circle(centers[c], 2*std, color=dkcolors_map(c), clip_on=False, fill=False, lw=0.5)\n",
        "        ax[0].add_patch(circ)\n",
        "\n",
        "\n",
        "    #make a model for plotting routines to call\n",
        "    cat_predict = lambda pt: recat(pt.reshape(1,2), centers)\n",
        "    plot_cat_decision_boundary(ax[1], X_train, cat_predict,  vector=False, color = dlc[\"dlmagenta\"], lw=0.75)\n",
        "    ax[1].set_title(\"ideal performance\", fontsize=14)\n",
        "\n",
        "    #add the original data to the decison boundary\n",
        "    plt_mc_data(ax[1], X_train,y_train, classes, map=dkcolors_map, legend=True, size=50)\n",
        "    ax[1].set_xlabel('x0') ; ax[1].set_ylabel(\"x1\");\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plt_nn(model_predict,X_train,y_train, classes, X_cv, y_cv, suptitle=\"\"):\n",
        "    #plot the decison boundary.\n",
        "    fig,ax = plt.subplots(1,2, figsize=(8,4))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "    plot_cat_decision_boundary(ax[0], X_train, model_predict,  vector=True)\n",
        "    ax[0].set_title(\"training data\", fontsize=14)\n",
        "\n",
        "    #add the original data to the decison boundary\n",
        "    plt_mc_data(ax[0], X_train,y_train, classes, map=dkcolors_map, legend=True, size=75)\n",
        "    ax[0].set_xlabel('x0') ; ax[0].set_ylabel(\"x1\");\n",
        "\n",
        "    plot_cat_decision_boundary(ax[1], X_train, model_predict,  vector=True)\n",
        "    ax[1].set_title(\"cross-validation data\", fontsize=14)\n",
        "    plt_mc_data(ax[1], X_cv,y_cv, classes,\n",
        "                map=ltcolors_map, legend=True, size=100, m='<')\n",
        "    ax[1].set_xlabel('x0') ; ax[1].set_ylabel(\"x1\");\n",
        "    fig.suptitle(suptitle,fontsize = 12)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def eval_cat_err(y, yhat):\n",
        "    \"\"\"\n",
        "    Calculate the categorization error\n",
        "    Args:\n",
        "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
        "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
        "    Returns:|\n",
        "      err: (scalar)\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    incorrect = 0\n",
        "    for i in range(m):\n",
        "        if yhat[i] != y[i]:\n",
        "            incorrect += 1\n",
        "    err = incorrect/m\n",
        "    return(err)\n",
        "\n",
        "def plot_iterate(lambdas, models, X_train, y_train, X_cv, y_cv):\n",
        "    err_train = np.zeros(len(lambdas))\n",
        "    err_cv = np.zeros(len(lambdas))\n",
        "    for i in range(len(models)):\n",
        "        err_train[i] = eval_cat_err(y_train,np.argmax( models[i](X_train), axis=1))\n",
        "        err_cv[i] = eval_cat_err(y_cv, np.argmax( models[i](X_cv), axis=1))\n",
        "\n",
        "    fig, ax = plt.subplots(1,1,figsize=(6,4))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "    ax.set_title(\"error vs regularization\",fontsize = 12)\n",
        "    ax.plot(lambdas, err_train, marker='o', label=\"train error\", color = dlc[\"dlblue\"])\n",
        "    ax.plot(lambdas, err_cv,    marker='o', label=\"cv error\",    color = dlc[\"dlorange\"])\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_ylim(*ax.get_ylim())\n",
        "    ax.set_xlabel(\"Regularization (lambda)\",fontsize = 14)\n",
        "    ax.set_ylabel(\"Error\",fontsize = 14)\n",
        "    ax.legend()\n",
        "    fig.suptitle(\"Tuning Regularization\",fontsize = 14)\n",
        "    ax.text(0.05,0.14,\"Training Error\\nlower than CV\",fontsize=12, ha='left',transform=ax.transAxes,color = dlc[\"dlblue\"])\n",
        "    ax.text(0.95,0.14,\"Similar\\nTraining, CV\",    fontsize=12, ha='right',transform=ax.transAxes,color = dlc[\"dlblue\"])\n",
        "    plt.show()\n",
        "\n",
        "# not used but will calculate the erro assuming an equal distance\n",
        "def err_all_equal(X_train,X_cv,X_test, y_train,y_cv,y_test, centers):\n",
        "    X_all = np.concatenate((X_train,X_cv,X_test), axis=0)\n",
        "    y_all = np.concatenate((y_train,y_cv,y_test), axis=0)\n",
        "    m = len(X_all)\n",
        "    y_eq  = np.zeros(m)\n",
        "    for i in range(m):\n",
        "        y_eq[i] = recat(X_all[i], centers)\n",
        "    err_all = eval_cat_err(y_all, y_eq)\n",
        "    return(err_all)\n",
        "\n",
        "def plt_compare(X,y, classes, simple, regularized, centers):\n",
        "    plt.close(\"all\")\n",
        "    fig,ax = plt.subplots(1,3, figsize=(8,3))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "\n",
        "  #plt simple\n",
        "    plot_cat_decision_boundary(ax[0], X, simple,  vector=True)\n",
        "    ax[0].set_title(\"Simple Model\", fontsize=14)\n",
        "    plt_mc_data(ax[0], X,y, classes, map=dkcolors_map, legend=True, size=75)\n",
        "    ax[0].set_xlabel('x0') ; ax[0].set_ylabel(\"x1\");\n",
        "\n",
        "  #plt regularized\n",
        "    plot_cat_decision_boundary(ax[1], X, regularized,  vector=True)\n",
        "    ax[1].set_title(\"Regularized Model\", fontsize=14)\n",
        "    plt_mc_data(ax[1], X,y, classes, map=dkcolors_map, legend=True, size=75)\n",
        "    ax[1].set_xlabel('x0') ; ax[0].set_ylabel(\"x1\");\n",
        "\n",
        "  #plt ideal\n",
        "    cat_predict = lambda pt: recat(pt.reshape(1,2), centers)\n",
        "    plot_cat_decision_boundary(ax[2], X, cat_predict,  vector=False)\n",
        "    ax[2].set_title(\"Ideal Model\", fontsize=14)\n",
        "    plt_mc_data(ax[2], X,y, classes, map=dkcolors_map, legend=True, size=75)\n",
        "    ax[2].set_xlabel('x0') ; ax[0].set_ylabel(\"x1\");\n",
        "\n",
        "    err_s = eval_cat_err(y, simple(X))\n",
        "    err_r = eval_cat_err(y, regularized(X))\n",
        "    ax[0].text(-2.75,3,f\"err_test={err_s:0.2f}\", fontsize=11)\n",
        "    ax[1].text(-2.75,3,f\"err_test={err_r:0.2f}\", fontsize=11)\n",
        "    m = len(X)\n",
        "    y_eq  = np.zeros(m)\n",
        "    for i in range(m):\n",
        "        y_eq[i] = recat(X[i], centers)\n",
        "    err_eq = eval_cat_err(y, y_eq)\n",
        "    ax[2].text(-2.75,3,f\"err_test={err_eq:0.2f}\", fontsize=11)\n",
        "    plt.show()\n",
        "\n",
        "# --- End Assignment ----------------------------------------\n"
      ],
      "metadata": {
        "id": "aSyyd161qldw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import relu,linear\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def test_eval_mse(target):\n",
        "    y_hat = np.array([2.4, 4.2])\n",
        "    y_tmp = np.array([2.3, 4.1])\n",
        "    result = target(y_hat, y_tmp)\n",
        "\n",
        "    assert np.isclose(result, 0.005, atol=1e-6), f\"Wrong value. Expected 0.005, got {result}\"\n",
        "\n",
        "    y_hat = np.array([3.] * 10)\n",
        "    y_tmp = np.array([3.] * 10)\n",
        "    result = target(y_hat, y_tmp)\n",
        "    assert np.isclose(result, 0.), f\"Wrong value. Expected 0.0 when y_hat == t_tmp, but got {result}\"\n",
        "\n",
        "    y_hat = np.array([3.])\n",
        "    y_tmp = np.array([0.])\n",
        "    result = target(y_hat, y_tmp)\n",
        "    assert np.isclose(result, 4.5), f\"Wrong value. Expected 4.5, but got {result}. Remember the square termn\"\n",
        "\n",
        "    y_hat = np.array([3.] * 5)\n",
        "    y_tmp = np.array([2.] * 5)\n",
        "    result = target(y_hat, y_tmp)\n",
        "    assert np.isclose(result, 0.5), f\"Wrong value. Expected 0.5, but got {result}. Remember to divide by (2*m)\"\n",
        "\n",
        "    print(\"\\033[92m All tests passed.\")\n",
        "\n",
        "def test_eval_cat_err(target):\n",
        "    y_hat = np.array([1, 0, 1, 1, 1, 0])\n",
        "    y_tmp = np.array([0, 1, 0, 0, 0, 1])\n",
        "    result = target(y_hat, y_tmp)\n",
        "    assert not np.isclose(result, 6.), f\"Wrong value. Expected 1, but got {result}. Did you divided by m?\"\n",
        "\n",
        "    y_hat = np.array([1, 2, 0])\n",
        "    y_tmp = np.array([1, 2, 3])\n",
        "    result = target(y_hat, y_tmp)\n",
        "    assert np.isclose(result, 1./3., atol=1e-6), f\"Wrong value. Expected 0.333, but got {result}\"\n",
        "\n",
        "    y_hat = np.array([1, 0, 1, 1, 1, 0])\n",
        "    y_tmp = np.array([1, 1, 1, 0, 0, 0])\n",
        "    result = target(y_hat, y_tmp)\n",
        "    assert np.isclose(result, 3./6., atol=1e-6), f\"Wrong value. Expected 0.5, but got {result}\"\n",
        "\n",
        "    y_hat = np.array([[1], [2], [0], [3]])\n",
        "    y_tmp = np.array([[1], [2], [1], [3]])\n",
        "    res_tmp =  target(y_hat, y_tmp)\n",
        "    assert type(res_tmp) != np.ndarray, f\"The output must be an scalar but got {type(res_tmp)}\"\n",
        "\n",
        "    print(\"\\033[92m All tests passed.\")\n",
        "\n",
        "def model_test(target, classes, input_size):\n",
        "    target.build(input_shape=(None,input_size))\n",
        "    expected_lr = 0.01\n",
        "\n",
        "    assert len(target.layers) == 3, \\\n",
        "        f\"Wrong number of layers. Expected 3 but got {len(target.layers)}\"\n",
        "    assert target.input.shape.as_list() == [None, input_size], \\\n",
        "        f\"Wrong input shape. Expected [None,  {input_size}] but got {target.input.shape.as_list()}\"\n",
        "    i = 0\n",
        "    expected = [[Dense, [None, 120], relu],\n",
        "                [Dense, [None, 40], relu],\n",
        "                [Dense, [None, classes], linear]]\n",
        "\n",
        "    for layer in target.layers:\n",
        "        assert type(layer) == expected[i][0], \\\n",
        "            f\"Wrong type in layer {i}. Expected {expected[i][0]} but got {type(layer)}\"\n",
        "        assert layer.output.shape.as_list() == expected[i][1], \\\n",
        "            f\"Wrong number of units in layer {i}. Expected {expected[i][1]} but got {layer.output.shape.as_list()}\"\n",
        "        assert layer.activation == expected[i][2], \\\n",
        "            f\"Wrong activation in layer {i}. Expected {expected[i][2]} but got {layer.activation}\"\n",
        "        assert layer.kernel_regularizer == None, \"You must not specify any regularizer for any layer\"\n",
        "        i = i + 1\n",
        "\n",
        "    assert type(target.loss)==SparseCategoricalCrossentropy, f\"Wrong loss function. Expected {SparseCategoricalCrossentropy}, but got {target.loss}\"\n",
        "    assert type(target.optimizer)==Adam, f\"Wrong loss function. Expected {Adam}, but got {target.optimizer}\"\n",
        "    lr = target.optimizer.learning_rate.numpy()\n",
        "    assert np.isclose(lr, expected_lr, atol=1e-8), f\"Wrong learning rate. Expected {expected_lr}, but got {lr}\"\n",
        "    assert target.loss.get_config()['from_logits'], f\"Set from_logits=True in loss function\"\n",
        "\n",
        "    print(\"\\033[92mAll tests passed!\")\n",
        "\n",
        "def model_s_test(target, classes, input_size):\n",
        "    target.build(input_shape=(None,input_size))\n",
        "    expected_lr = 0.01\n",
        "\n",
        "    assert len(target.layers) == 2, \\\n",
        "        f\"Wrong number of layers. Expected 3 but got {len(target.layers)}\"\n",
        "    assert target.input.shape.as_list() == [None, input_size], \\\n",
        "        f\"Wrong input shape. Expected [None,  {input_size}] but got {target.input.shape.as_list()}\"\n",
        "    i = 0\n",
        "    expected = [[Dense, [None, 6], relu],\n",
        "                [Dense, [None, classes], linear]]\n",
        "\n",
        "    for layer in target.layers:\n",
        "        assert type(layer) == expected[i][0], \\\n",
        "            f\"Wrong type in layer {i}. Expected {expected[i][0]} but got {type(layer)}\"\n",
        "        assert layer.output.shape.as_list() == expected[i][1], \\\n",
        "            f\"Wrong number of units in layer {i}. Expected {expected[i][1]} but got {layer.output.shape.as_list()}\"\n",
        "        assert layer.activation == expected[i][2], \\\n",
        "            f\"Wrong activation in layer {i}. Expected {expected[i][2]} but got {layer.activation}\"\n",
        "        assert layer.kernel_regularizer == None, \"You must not specify any regularizer any layer\"\n",
        "        i = i + 1\n",
        "\n",
        "    assert type(target.loss)==SparseCategoricalCrossentropy, f\"Wrong loss function. Expected {SparseCategoricalCrossentropy}, but got {target.loss}\"\n",
        "    assert type(target.optimizer)==Adam, f\"Wrong loss function. Expected {Adam}, but got {target.optimizer}\"\n",
        "    lr = target.optimizer.learning_rate.numpy()\n",
        "    assert np.isclose(lr, expected_lr, atol=1e-8), f\"Wrong learning rate. Expected {expected_lr}, but got {lr}\"\n",
        "    assert target.loss.get_config()['from_logits'], f\"Set from_logits=True in loss function\"\n",
        "\n",
        "    print(\"\\033[92mAll tests passed!\")\n",
        "\n",
        "def model_r_test(target, classes, input_size):\n",
        "    target.build(input_shape=(None,input_size))\n",
        "    expected_lr = 0.01\n",
        "    print(\"ddd\")\n",
        "    assert len(target.layers) == 3, \\\n",
        "        f\"Wrong number of layers. Expected 3 but got {len(target.layers)}\"\n",
        "    assert target.input.shape.as_list() == [None, input_size], \\\n",
        "        f\"Wrong input shape. Expected [None,  {input_size}] but got {target.input.shape.as_list()}\"\n",
        "    i = 0\n",
        "    expected = [[Dense, [None, 120], relu, (tf.keras.regularizers.l2, 0.1)],\n",
        "                [Dense, [None, 40], relu, (tf.keras.regularizers.l2, 0.1)],\n",
        "                [Dense, [None, classes], linear, None]]\n",
        "\n",
        "    for layer in target.layers:\n",
        "        assert type(layer) == expected[i][0], \\\n",
        "            f\"Wrong type in layer {i}. Expected {expected[i][0]} but got {type(layer)}\"\n",
        "        assert layer.output.shape.as_list() == expected[i][1], \\\n",
        "            f\"Wrong number of units in layer {i}. Expected {expected[i][1]} but got {layer.output.shape.as_list()}\"\n",
        "        assert layer.activation == expected[i][2], \\\n",
        "            f\"Wrong activation in layer {i}. Expected {expected[i][2]} but got {layer.activation}\"\n",
        "        if not (expected[i][3] == None):\n",
        "            assert type(layer.kernel_regularizer) == expected[i][3][0], f\"Wrong regularizer. Expected L2 regularizer but got {type(layer.kernel_regularizer)}\"\n",
        "            assert np.isclose(layer.kernel_regularizer.l2,  expected[i][3][1]), f\"Wrong regularization factor. Expected {expected[i][3][1]}, but got {layer.kernel_regularizer.l2}\"\n",
        "        else:\n",
        "            assert layer.kernel_regularizer == None, \"You must not specify any regularizer for the 3th layer\"\n",
        "        i = i + 1\n",
        "\n",
        "    assert type(target.loss)==SparseCategoricalCrossentropy, f\"Wrong loss function. Expected {SparseCategoricalCrossentropy}, but got {target.loss}\"\n",
        "    assert type(target.optimizer)==Adam, f\"Wrong loss function. Expected {Adam}, but got {target.optimizer}\"\n",
        "    lr = target.optimizer.learning_rate.numpy()\n",
        "    assert np.isclose(lr, expected_lr, atol=1e-8), f\"Wrong learning rate. Expected {expected_lr}, but got {lr}\"\n",
        "    assert target.loss.get_config()['from_logits'], f\"Set from_logits=True in loss function\"\n",
        "\n",
        "    print(\"\\033[92mAll tests passed!\")\n"
      ],
      "metadata": {
        "id": "jgDnRWrNq1gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W4_Lab10 (Trees Ensemble)"
      ],
      "metadata": {
        "id": "u3IkjNRjmZ8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you will:\n",
        "\n",
        " - Use Pandas to perform one-hot encoding of a dataset\n",
        " - Use scikit-learn to implement a Decision Tree, Random Forest and XGBoost models"
      ],
      "metadata": {
        "id": "EPC__1HYmpO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.style.use('./deeplearning.mplstyle')\n",
        "\n",
        "RANDOM_STATE = 55 ## We will pass it to every sklearn call so we ensure reproducibility"
      ],
      "metadata": {
        "id": "1tndN7iHm7tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Introduction\n",
        "\n",
        "#### Datatset\n",
        "- This dataset is obtained from Kaggle: [Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)\n",
        "\n",
        "#### Context\n",
        "- Cardiovascular disease (CVDs) is the number one cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of five CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs.\n",
        "- People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management.  \n",
        "- This dataset contains 11 features that can be used to predict possible heart disease.\n",
        "- Let's train a machine learning model to assist with diagnosing this disease.\n",
        "\n",
        "#### Attribute Information\n",
        "- Age: age of the patient [years]\n",
        "- Sex: sex of the patient [M: Male, F: Female]\n",
        "- ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
        "- RestingBP: resting blood pressure [mm Hg]\n",
        "- Cholesterol: serum cholesterol [mm/dl]\n",
        "- FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
        "- RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
        "- MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n",
        "- ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n",
        "- Oldpeak: oldpeak = ST [Numeric value measured in depression]\n",
        "- ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
        "- HeartDisease: output class [1: heart disease, 0: Normal]"
      ],
      "metadata": {
        "id": "vyhz5HFSopqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now load the dataset. As we can see above, the variables:\n",
        "\n",
        "- Sex\n",
        "- ChestPainType\n",
        "- RestingECG\n",
        "- ExerciseAngina\n",
        "- ST_Slope\n",
        "\n",
        "Are *categorical*, so we must one-hot encode them."
      ],
      "metadata": {
        "id": "nkiVgGyTpVRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset using pandas\n",
        "\n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/HomayounfarM/machine-learning-andrew-ng/main/Data/heart.csv\"\n",
        "df = pd.read_csv(url)\n",
        "print(df.shape)\n",
        "print(df.info())\n",
        "df.head(5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uGSi6ffvpZ1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must perform some data engineering before working with the models. There are 5 categorical features, so we will use Pandas to one-hot encode them."
      ],
      "metadata": {
        "id": "M_KZsfVVsH0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. One-hot encoding using Pandas\n",
        "\n",
        "First we will remove the binary variables, because one-hot encoding them would do nothing to them. To achieve this we will just count how many different values there are in each categorical variable and consider only the variables with 3 or more values."
      ],
      "metadata": {
        "id": "1ppFm9r1tALg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_variables = ['Sex',\n",
        "'ChestPainType',\n",
        "'RestingECG',\n",
        "'ExerciseAngina',\n",
        "'ST_Slope'\n",
        "]"
      ],
      "metadata": {
        "id": "mYULWpUrtDJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a reminder, one-hot encoding aims to transform a categorical variable with `n` outputs into `n` binary variables.\n",
        "\n",
        "Pandas has a built-in method to one-hot encode variables, it is the function `pd.get_dummies`. There are several arguments to this function, but here we will use only a few. They are:\n",
        "\n",
        " - data: DataFrame to be used\n",
        " - prefix: A list with prefixes, so we know which value we are dealing with\n",
        " - columns: the list of columns that will be one-hot encoded. 'prefix' and 'columns' must have the same length.\n",
        "\n",
        "For more information, you can always type `help(pd.get_dummies)` to read the function's full documentation."
      ],
      "metadata": {
        "id": "F2Xx7qgqtmuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will replace the columns with the one-hot encoded ones and keep the columns outside 'columns' argument as it is.\n",
        "df2 = pd.get_dummies(data = df,\n",
        "                         prefix = cat_variables,\n",
        "                         columns = cat_variables)"
      ],
      "metadata": {
        "id": "jWWZnkAJtmH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "id": "i8Puyly2upgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [x for x in df2.columns if x != \"HeartDisease\"]"
      ],
      "metadata": {
        "id": "YzcTRqVKsJDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting the Dataset\n",
        "\n",
        "In this section, we will split our dataset into train and test datasets. We will use the function `train_test_split` from Scikit-learn. Let's just check its arguments."
      ],
      "metadata": {
        "id": "x5d2rf6gx7kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(df2[features], df2['HeartDisease'], train_size = 0.8, random_state = RANDOM_STATE)\n",
        "\n",
        "# We will keep the shuffle = True since our dataset has not any time dependency."
      ],
      "metadata": {
        "id": "X9-fV8_ayCbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train samples: {len(X_train)}')\n",
        "print(f'validation samples: {len(X_val)}')\n",
        "print(f'target proportion: {sum(y_train)/len(y_train):.4f}')"
      ],
      "metadata": {
        "id": "XxcDQoNGyL7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Decision Tree\n",
        "\n",
        "In this section, let's work with the Decision Tree we previously learned, but now using the [Scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
        "\n",
        "There are several hyperparameters in the Decision Tree object from Scikit-learn. We will use only some of them and also we will not perform feature selection nor hyperparameter tuning in this lab (but you are encouraged to do so and compare the results ð )\n",
        "\n",
        "The hyperparameters we will use and investigate here are:\n",
        "\n",
        " - min_samples_split: The minimum number of samples required to split an internal node.\n",
        "   - Choosing a higher min_samples_split can reduce the number of splits and may help to reduce overfitting.\n",
        " - max_depth: The maximum depth of the tree.\n",
        "   - Choosing a lower max_depth can reduce the number of splits and may help to reduce overfitting."
      ],
      "metadata": {
        "id": "pSM0tg0qym5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_samples_split_list = [2,10, 30, 50, 100, 200, 300, 700] ## If the number is an integer, then it is the actual quantity of samples,\n",
        "max_depth_list = [1,2, 3, 4, 8, 16, 32, 64, None] # None means that there is no depth limit."
      ],
      "metadata": {
        "id": "BVVYShCVyrI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list_train = []\n",
        "accuracy_list_val = []\n",
        "for min_samples_split in min_samples_split_list:\n",
        "    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
        "    model = DecisionTreeClassifier(min_samples_split = min_samples_split,\n",
        "                                   random_state = RANDOM_STATE).fit(X_train,y_train)\n",
        "    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n",
        "    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n",
        "    accuracy_train = accuracy_score(predictions_train,y_train)\n",
        "    accuracy_val = accuracy_score(predictions_val,y_val)\n",
        "    accuracy_list_train.append(accuracy_train)\n",
        "    accuracy_list_val.append(accuracy_val)\n",
        "\n",
        "plt.title('Train x Validation metrics')\n",
        "plt.xlabel('min_samples_split')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xticks(ticks = range(len(min_samples_split_list )),labels=min_samples_split_list)\n",
        "plt.plot(accuracy_list_train)\n",
        "plt.plot(accuracy_list_val)\n",
        "plt.legend(['Train','Validation'])"
      ],
      "metadata": {
        "id": "4MbG_ubPzBC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how increasing the the number of `min_samples_split` reduces overfitting.\n",
        "- Increasing `min_samples_split` from 10 to 30, and from 30 to 50, even though it does not improve the validation accuracy, it brings the training accuracy closer to it, showing a reduction in overfitting.\n",
        "\n",
        "Let's do the same experiment with `max_depth`."
      ],
      "metadata": {
        "id": "igB4e3vozoM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list_train = []\n",
        "accuracy_list_val = []\n",
        "for max_depth in max_depth_list:\n",
        "    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
        "    model = DecisionTreeClassifier(max_depth = max_depth,\n",
        "                                   random_state = RANDOM_STATE).fit(X_train,y_train)\n",
        "    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n",
        "    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n",
        "    accuracy_train = accuracy_score(predictions_train,y_train)\n",
        "    accuracy_val = accuracy_score(predictions_val,y_val)\n",
        "    accuracy_list_train.append(accuracy_train)\n",
        "    accuracy_list_val.append(accuracy_val)\n",
        "\n",
        "plt.title('Train x Validation metrics')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xticks(ticks = range(len(max_depth_list )),labels=max_depth_list)\n",
        "plt.plot(accuracy_list_train)\n",
        "plt.plot(accuracy_list_val)\n",
        "plt.legend(['Train','Validation'])"
      ],
      "metadata": {
        "id": "aEXvPnyjzra_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that in general, reducing `max_depth` can help to reduce overfitting.\n",
        "- Reducing `max_depth` from 8 to 4 increases validation accuracy closer to training accuracy, while significantly reducing training accuracy.\n",
        "- The validation accuracy reaches the highest at tree_depth=4.\n",
        "- When the `max_depth` is smaller than 3, both training and validation accuracy decreases.  The tree cannot make enough splits to distinguish positives from negatives (the model is underfitting the training set).\n",
        "- When the `max_depth` is too high ( >= 5), validation accuracy decreases while training accuracy increases, indicating that the model is overfitting to the training set.\n",
        "\n",
        "So we can choose the best values for these two hyper-parameters for our model to be:\n",
        "- `max_depth = 4`\n",
        "- `min_samples_split = 50`"
      ],
      "metadata": {
        "id": "jL-7PRXvz3lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree_model = DecisionTreeClassifier(min_samples_split = 50,\n",
        "                                             max_depth = 3,\n",
        "                                             random_state = RANDOM_STATE).fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "8V4pEPWqz893"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(decision_tree_model.predict(X_train),y_train):.4f}\")\n",
        "print(f\"Metrics validation:\\n\\tAccuracy score: {accuracy_score(decision_tree_model.predict(X_val),y_val):.4f}\")"
      ],
      "metadata": {
        "id": "i11sykG_z_Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Random Forest\n",
        "\n",
        "Now let's try the Random Forest algorithm also, using the Scikit-learn implementation.\n",
        "- All of the hyperparameters found in the decision tree model will also exist in this algorithm, since a random forest is an ensemble of many Decision Trees.\n",
        "- One additional hyperparameter for Random Forest is called `n_estimators` which is the number of Decision Trees that make up the Random Forest.\n",
        "\n",
        "Remember that for a Random Forest, we randomly choose a subset of the features AND randomly choose a subset of the training examples to train each individual tree.\n",
        "- Following the lectures, if $n$ is the number of features, we will randomly select $\\sqrt{n}$ of these features to train each individual tree.\n",
        "- Note that you can modify this by setting the `max_features` parameter.\n",
        "\n",
        "You can also speed up your training jobs with another parameter, `n_jobs`.\n",
        "- Since the fitting of each tree is independent of each other, it is possible fit more than one tree in parallel.\n",
        "- So setting `n_jobs` higher will increase how many CPU cores it will use. Note that the numbers very close to the maximum cores of your CPU may impact on the overall performance of your PC and even lead to freezes.\n",
        "- Changing this parameter does not impact on the final result but can reduce the training time.\n",
        "\n",
        "We will run the same script again, but with another parameter, `n_estimators`, where we will choose between 10, 50, and 100. The default is 100."
      ],
      "metadata": {
        "id": "t7FZLB1W0HZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_samples_split_list = [2,10, 30, 50, 100, 200, 300, 700]  ## If the number is an integer, then it is the actual quantity of samples,\n",
        "                                             ## If it is a float, then it is the percentage of the dataset\n",
        "max_depth_list = [2, 4, 8, 16, 32, 64, None]\n",
        "n_estimators_list = [10,50,100,500]"
      ],
      "metadata": {
        "id": "v_ctQBVx0koh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list_train = []\n",
        "accuracy_list_val = []\n",
        "for min_samples_split in min_samples_split_list:\n",
        "    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
        "    model = RandomForestClassifier(min_samples_split = min_samples_split,\n",
        "                                   random_state = RANDOM_STATE).fit(X_train,y_train)\n",
        "    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n",
        "    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n",
        "    accuracy_train = accuracy_score(predictions_train,y_train)\n",
        "    accuracy_val = accuracy_score(predictions_val,y_val)\n",
        "    accuracy_list_train.append(accuracy_train)\n",
        "    accuracy_list_val.append(accuracy_val)\n",
        "\n",
        "plt.title('Train x Validation metrics')\n",
        "plt.xlabel('min_samples_split')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xticks(ticks = range(len(min_samples_split_list )),labels=min_samples_split_list)\n",
        "plt.plot(accuracy_list_train)\n",
        "plt.plot(accuracy_list_val)\n",
        "plt.legend(['Train','Validation'])"
      ],
      "metadata": {
        "id": "cGKFYg-m0oO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that, even though the validation accuraty reaches is the same both at `min_samples_split = 2` and `min_samples_split = 10`, in the latter the difference in training and validation set reduces, showing less overfitting."
      ],
      "metadata": {
        "id": "aWUoOTxf0xgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list_train = []\n",
        "accuracy_list_val = []\n",
        "for max_depth in max_depth_list:\n",
        "    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
        "    model = RandomForestClassifier(max_depth = max_depth,\n",
        "                                   random_state = RANDOM_STATE).fit(X_train,y_train)\n",
        "    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n",
        "    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n",
        "    accuracy_train = accuracy_score(predictions_train,y_train)\n",
        "    accuracy_val = accuracy_score(predictions_val,y_val)\n",
        "    accuracy_list_train.append(accuracy_train)\n",
        "    accuracy_list_val.append(accuracy_val)\n",
        "\n",
        "plt.title('Train x Validation metrics')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xticks(ticks = range(len(max_depth_list )),labels=max_depth_list)\n",
        "plt.plot(accuracy_list_train)\n",
        "plt.plot(accuracy_list_val)\n",
        "plt.legend(['Train','Validation'])"
      ],
      "metadata": {
        "id": "iidiXKZG00XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list_train = []\n",
        "accuracy_list_val = []\n",
        "for n_estimators in n_estimators_list:\n",
        "    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
        "    model = RandomForestClassifier(n_estimators = n_estimators,\n",
        "                                   random_state = RANDOM_STATE).fit(X_train,y_train)\n",
        "    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n",
        "    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n",
        "    accuracy_train = accuracy_score(predictions_train,y_train)\n",
        "    accuracy_val = accuracy_score(predictions_val,y_val)\n",
        "    accuracy_list_train.append(accuracy_train)\n",
        "    accuracy_list_val.append(accuracy_val)\n",
        "\n",
        "plt.title('Train x Validation metrics')\n",
        "plt.xlabel('n_estimators')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xticks(ticks = range(len(n_estimators_list )),labels=n_estimators_list)\n",
        "plt.plot(accuracy_list_train)\n",
        "plt.plot(accuracy_list_val)\n",
        "plt.legend(['Train','Validation'])"
      ],
      "metadata": {
        "id": "LT6EwDEI04y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's then fit a random forest with the following parameters:\n",
        "\n",
        " - max_depth: 16\n",
        " - min_samples_split: 10\n",
        " - n_estimators: 100"
      ],
      "metadata": {
        "id": "umSuNyLw08yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest_model = RandomForestClassifier(n_estimators = 100,\n",
        "                                             max_depth = 16,\n",
        "                                             min_samples_split = 10).fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "zVpL6l-l0_Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(random_forest_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(random_forest_model.predict(X_val),y_val):.4f}\")"
      ],
      "metadata": {
        "id": "Lap-gMxh1Bf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we are searching for the best value one hyperparameter while leaving the other hyperparameters at their default values.\n",
        "- Ideally, we would want to check every combination of values for every hyperparameter that we are tuning.\n",
        "- If we have 3 hyperparameters, and each hyperparameter has 4 values to try out, we should have a total of 4 x 4 x 4 = 64 combinations to try.\n",
        "- When we only modify one hyperparameter while leaving the rest as their default value, we are trying 4 + 4 + 4 = 12 results.\n",
        "- To try out all combinations, we can use a sklearn implementation called GridSearchCV. GridSearchCV has a refit parameter that will automatically refit a model on the best combination so we will not need to program it explicitly. For more on GridSearchCV, please refer to its [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
      ],
      "metadata": {
        "id": "V46r1KZb1FF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 XGBoost\n",
        "\n",
        "Next is the Gradient Boosting model, called XGBoost. The boosting methods train several trees, but instead of them being uncorrelated to each other, now the trees are fit one after the other in order to minimize the error.\n",
        "\n",
        "The model has the same parameters as a decision tree, plus the learning rate.\n",
        "- The learning rate is the size of the step on the Gradient Descent method that the XGBoost uses internally to minimize the error on each train step.\n",
        "\n",
        "One interesting thing about the XGBoost is that during fitting, it can take in an evaluation dataset of the form `(X_val,y_val)`.\n",
        "- On each iteration, it measures the cost (or evaluation metric) on the evaluation datasets.\n",
        "- Once the cost (or metric) stops decreasing for a number of rounds (called early_stopping_rounds), the training will stop.\n",
        "- More iterations lead to more estimators, and more estimators can result in overfitting.  \n",
        "- By stopping once the validation metric no longer improves, we can limit the number of estimators created, and reduce overfitting.\n",
        "\n",
        "First, let's define a subset of our training set (we should not use the test set here)."
      ],
      "metadata": {
        "id": "aB5fopss1ZNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(len(X_train)*0.8) ## Let's use 80% to train and 20% to eval"
      ],
      "metadata": {
        "id": "fWaZ1b6f152m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_fit, X_train_eval, y_train_fit, y_train_eval = X_train[:n], X_train[n:], y_train[:n], y_train[n:]"
      ],
      "metadata": {
        "id": "-jcXJltF18IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then set a large number of estimators, because we can stop if the cost function stops decreasing."
      ],
      "metadata": {
        "id": "oJtxRr9d1_eD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note some of the `.fit()` parameters:\n",
        "- `eval_set = [(X_train_eval,y_train_eval)]`:Here we must pass a list to the eval_set, because you can have several different tuples ov eval sets.\n",
        "- `early_stopping_rounds`: This parameter helps to stop the model training if its evaluation metric is no longer improving on the validation set. It's set to 10.\n",
        "  - The model keeps track of the round with the best performance (lowest evaluation metric).  For example, let's say round 16 has the lowest evaluation metric so far.\n",
        "  - Each successive round's evaluation metric is compared to the best metric.  If the model goes 10 rounds where none have a better metric than the best one, then the model stops training.\n",
        "  - The model is returned at its last state when training terminated, not its state during the best round.  For example, if the model stops at round 26, but the best round was 16, the model's training state at round 26 is returned, not round 16.\n",
        "  - Note that this is different from returning the model's \"best\" state (from when the evaluation metric was the lowest)."
      ],
      "metadata": {
        "id": "U5M0A09A2EaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\n",
        "xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)], early_stopping_rounds = 10)"
      ],
      "metadata": {
        "id": "TFErM3MI2SRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though we initialized the model to allow up to 500 estimators, the algorithm only fit 26 estimators (over 26 rounds of training).\n",
        "\n",
        "To see why, let's look for the round of training that had the best performance (lowest evaluation metric).  You can either view the validation log loss metrics that were output above, or view the model's `.best_iteration` attribute:"
      ],
      "metadata": {
        "id": "_bmge0Ps2WFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model.best_iteration"
      ],
      "metadata": {
        "id": "otkkSQCQ2YVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best round of training was round 16, with a log loss of 4.3948.  \n",
        "- For 10 rounds of training after that (from round 17 to 26), the log loss was higher than this.\n",
        "- Since we set `early_stopping_rounds` to 10, then by the 10th round where the log loss doesn't improve upon the best one, training stops.\n",
        "- You can try out different values of `early_stopping_rounds` to verify this.  If you set it to 20, for instance, the model stops training at round 36 (16 + 20)."
      ],
      "metadata": {
        "id": "xQCfG3l-2bWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_val),y_val):.4f}\")"
      ],
      "metadata": {
        "id": "Npi1K9df2dqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, both Random Forest and XGBoost had similar performance (test accuracy).  \n",
        "\n",
        "Congratulations, you have learned how to use Decision Tree, Random Forest from the scikit-learn library and XGBoost!"
      ],
      "metadata": {
        "id": "0cVskLxx2hJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "h-0aj0uuk7TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.array([[1,2]])\n",
        "Y = np.array([[2,3]]).T\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print('*'*10)\n",
        "Z = np.ones([1,2])\n",
        "print(Z*X)\n",
        "print(Z*Y)\n",
        "print(Z@Y)\n",
        "np.dot(Z,Y)\n"
      ],
      "metadata": {
        "id": "gbodwHvs83T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1,2]])\n",
        "w = np.array([[1,2,3], [4,5,6]])\n",
        "b = np.array([1,2,3])\n",
        "x @ w + b    # np.matmul(x,w)+b does the same"
      ],
      "metadata": {
        "id": "BrXkhyOqlFr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Implementation of matplotlib function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "dx, dy = 0.015, 0.05\n",
        "y, x = np.mgrid[slice(-4, 4 + dy, dy),\n",
        "                slice(-4, 4 + dx, dx)]\n",
        "z = (1 - x / 3. + x ** 5 + y ** 5) * np.exp(-x ** 2 - y ** 2)\n",
        "z = z[:-1, :-1]\n",
        "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
        "\n",
        "plt.imshow(y, cmap ='Reds')\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XA5QKjFcm3UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'a':list(np.random.randint(0,10,100))})\n",
        "arr = df.to_numpy()\n",
        "arr.reshape((5,20))"
      ],
      "metadata": {
        "id": "JzYBwDk-gaqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'a':list(np.random.randint(0,255,4))})\n",
        "arr = df.to_numpy()\n",
        "arr = arr.reshape((2,2))\n",
        "plt.imshow(arr, cmap = 'Greys')"
      ],
      "metadata": {
        "id": "zvvkdy9xcbYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(np.random.randint(0,255,400)/100)"
      ],
      "metadata": {
        "id": "6JwBsfvPBBKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "givenString = \"TutorialsPoint\"\n",
        "my_list = []\n",
        "your_list = []\n",
        "\n",
        "for i in range(0, 20):\n",
        "  my_list.append(random.randint(0, 9))\n",
        "\n",
        "print(my_list)\n",
        "\n",
        "for item in my_list:\n",
        "  if item == 0 or item == 1:\n",
        "    print(\"yes\")\n",
        "  elif item == 2 or item == 3:\n",
        "    print(\"No\")\n",
        "  else:\n",
        "    print(\"NA\")"
      ],
      "metadata": {
        "id": "hNm2VHC2IN8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# input string\n",
        "givenString = \"ABCDEFGHIJ\"\n",
        "\n",
        "# generating 3 random items from the String using random.sample() method\n",
        "randomItems = random.sample(givenString, 10)\n",
        "print(randomItems)\n",
        "\n",
        "for item in randomItems:\n",
        "  if item == 'A' or item == 'B':\n",
        "    print(\"yes\")\n",
        "  elif item == 'C' or item == 'D':\n",
        "    print(\"No\")\n",
        "  else:\n",
        "    print(\"NA\")"
      ],
      "metadata": {
        "id": "8z5hWXHEPrxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saple_id = {'a1b':1, 'b1b':2, 'c1b':3}\n",
        "saple_id\n",
        "\n",
        "my_list_2 = [1, 2, 3, 4, 3, 2, 1]\n",
        "\n",
        "my_list_1 = ['A', 'B', 'C']\n",
        "\n",
        "unique(my_list_2)"
      ],
      "metadata": {
        "id": "HYFzOP7Md5Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "a = np.array(['a', 'b', 'c', 'a'])\n",
        "\n",
        "\n",
        "print(a == 'a')\n",
        "\n",
        "a[a == 'a'] = 'AAA'\n",
        "\n",
        "a\n",
        "\n"
      ],
      "metadata": {
        "id": "gdXLKuz4jvY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}