{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mq53ARLRHHfj",
        "pupmi3GAHWJ1",
        "UZaUyq91Kn4u"
      ],
      "authorship_tag": "ABX9TyOMPXVljX5bpKOm6II7cFr4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HomayounfarM/machine-learning-andrew-ng/blob/main/C2_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W1_Lab01_TF"
      ],
      "metadata": {
        "id": "mq53ARLRHHfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Tensorflow and Keras\n",
        "Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by FranÃ§ois Chollet that creates a simple, layer-centric interface to Tensorflow. \n",
        "The following example shows to use tersorflow and Keras to create a layer. The first layer includes three cells (units). The second layer infludes one cell. \n",
        "\n",
        "In the following example, we have two layers: the first layer includes three unit (cell) and the second layer includes one unit (cell). \n",
        "Our x can be an array with any length. then, w will be an array with the same length and the inner production of these two array plus a 'b' wil be assigned to the first cell of that layer."
      ],
      "metadata": {
        "id": "Eg9FXGzsBqLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7eVKTN4__8Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "#from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
        "#from tensorflow.keras.activations import sigmoid\n",
        "#import matplotlib.colors as colors\n",
        "\n",
        "x = np.array([[200, 1, 4, 6, 7, 8]], dtype = float)\n",
        "\n",
        "print(type(x))\n",
        "print(x.shape)\n",
        "print(x.dtype)\n",
        "\n",
        "layer_1 = tf.keras.layers.Dense(units = 3, activation = 'sigmoid')\n",
        "a1 = layer_1(x)\n",
        "\n",
        "print(a1)\n",
        "\n",
        "layer_2 = Dense(units = 1, activation = 'sigmoid')\n",
        "a2 = layer_2(a1)\n",
        "\n",
        "print(a2.numpy())    # use .numpy() to convert the tensorflow to an np.array\n",
        "\n",
        "if a2 > 0.5:\n",
        "    yhat = 1\n",
        "else:\n",
        "    yhat = 0\n",
        "    \n",
        "print(yhat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "X_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\n",
        "Y_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\n",
        "\n",
        "pos = Y_train == 1\n",
        "neg = Y_train == 0\n",
        "X_train[pos]\n",
        "\n",
        "pos = Y_train == 1\n",
        "neg = Y_train == 0\n",
        "\n",
        "\n",
        "dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC')\n",
        "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
        "ax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "ax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n",
        "              edgecolors=dlc[\"dlmagenta\"],lw=3)\n",
        "\n",
        "ax.set_ylim(-0.08,1.1)\n",
        "ax.set_ylabel('y', fontsize=12)\n",
        "ax.set_xlabel('x', fontsize=12)\n",
        "ax.set_title('one variable plot')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hDy7DrsbCIUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Neuron\n",
        "\n",
        "We can implement a 'logistic neuron' by adding a sigmoid activation. The function of the neuron is then described by (1).  \n",
        "\n",
        "$$ f_{\\mathbf{w},b}(x^{(i)}) = g(\\mathbf{w}x^{(i)} + b) \\tag{1}$$\n",
        "where $$g(x) = sigmoid(x)$$ \n",
        "\n",
        "Let's set $w$ and $b$ to some known values and check the model.\n",
        "\n",
        "This section will create a Tensorflow Model that contains our logistic layer to demonstrate an alternate method of creating models. Tensorflow is most often used to create multi-layer models. The [Sequential](https://keras.io/guides/sequential_model/) model is a convenient means of constructing these models.\n",
        "We can implement a 'logistic'\n",
        "\n"
      ],
      "metadata": {
        "id": "8eajVEDoDH6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Rvy3-4v_Dhdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model.summary()` shows the layers and number of parameters in the model. There is only one layer in this model and that layer has only one unit. The unit has two parameters, $w$ and $b$.\n",
        "The first term in the 'Sequential function' determine the number of cells (units) in a layer. The second term, 'input_dim', is to determine the dimention of input to this layer.  "
      ],
      "metadata": {
        "id": "FS_AQ-2yD863"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "KvHWvdOsECw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W and b are randomly assigned. We need to train the model to find the best values for  and b"
      ],
      "metadata": {
        "id": "Yboq3i8WwBmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_layer = model.get_layer('L1')\n",
        "w,b = logistic_layer.get_weights()\n",
        "print(w,b)\n",
        "print(w.shape,b.shape)"
      ],
      "metadata": {
        "id": "EnXcZg4wED5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set the weight and bias to some known values."
      ],
      "metadata": {
        "id": "-7LVRJoREM2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I1uR3XLTEM1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_w = np.array([[2]])\n",
        "set_b = np.array([-4.5])\n",
        "# set_weights takes a list of numpy arrays\n",
        "logistic_layer.set_weights([set_w, set_b])\n",
        "print(logistic_layer.get_weights())"
      ],
      "metadata": {
        "id": "wME2GEXFEL92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare equation (2) to the layer output."
      ],
      "metadata": {
        "id": "GBh9fB-tEVYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = model.predict(X_train[0].reshape(1,1))\n",
        "print(a1)\n"
      ],
      "metadata": {
        "id": "oWU6UugGEWHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The followings are the functions used above"
      ],
      "metadata": {
        "id": "raaKGXU8FPOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoidnp(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    z : array_like\n",
        "        A scalar or numpy array of any size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "     g : array_like\n",
        "         sigmoid(z)\n",
        "    \"\"\"\n",
        "    z = np.clip( z, -500, 500 )           # protect against overflow\n",
        "    g = 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "VEqu_euWEsM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_logistic(X_train, Y_train, model, set_w, set_b, pos, neg):\n",
        "    fig,ax = plt.subplots(1,2,figsize=(16,4))\n",
        "\n",
        "    layerf= lambda x : model.predict(x)\n",
        "    plt_prob_1d(ax[0], layerf)\n",
        "\n",
        "    ax[0].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "    ax[0].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n",
        "                  edgecolors=dlc[\"dlblue\"],lw=3)\n",
        "\n",
        "    ax[0].set_ylim(-0.08,1.1)\n",
        "    ax[0].set_xlim(-0.5,5.5)\n",
        "    ax[0].set_ylabel('y', fontsize=16)\n",
        "    ax[0].set_xlabel('x', fontsize=16)\n",
        "    ax[0].set_title('Tensorflow Model', fontsize=20)\n",
        "    ax[0].legend(fontsize=16)\n",
        "\n",
        "    layerf= lambda x : sigmoidnp(np.dot(set_w,x.reshape(1,1)) + set_b)\n",
        "    plt_prob_1d(ax[1], layerf)\n",
        "\n",
        "    ax[1].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "    ax[1].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n",
        "                  edgecolors=dlc[\"dlblue\"],lw=3)\n",
        "\n",
        "    ax[1].set_ylim(-0.08,1.1)\n",
        "    ax[1].set_xlim(-0.5,5.5)\n",
        "    ax[1].set_ylabel('y', fontsize=16)\n",
        "    ax[1].set_xlabel('x', fontsize=16)\n",
        "    ax[1].set_title('Numpy Model', fontsize=20)\n",
        "    ax[1].legend(fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BuSuJLwYEyoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_prob_1d(ax,fwb):\n",
        "    \"\"\" plots a decision boundary but include shading to indicate the probability \"\"\"\n",
        "    #setup useful ranges and common linspaces\n",
        "    x_space  = np.linspace(0, 5 , 50)\n",
        "    y_space  = np.linspace(0, 1 , 50)\n",
        "\n",
        "    # get probability for x range, extend to y\n",
        "    z = np.zeros((len(x_space),len(y_space)))\n",
        "    for i in range(len(x_space)):\n",
        "        x = np.array([[x_space[i]]])\n",
        "        z[:,i] = fwb(x)\n",
        "\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "    new_cmap = truncate_colormap(cmap, 0.0, 0.5)\n",
        "    pcm = ax.pcolormesh(x_space, y_space, z,\n",
        "                   norm=cm.colors.Normalize(vmin=0, vmax=1),\n",
        "                   cmap=new_cmap, shading='nearest', alpha = 0.9)\n",
        "    ax.figure.colorbar(pcm, ax=ax)\n",
        "    \n",
        "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
        "    \"\"\" truncates color map \"\"\"\n",
        "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
        "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
        "        cmap(np.linspace(minval, maxval, n)))\n",
        "    return new_cmap"
      ],
      "metadata": {
        "id": "e4Cxz4odE1fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W1_Lab02_TF"
      ],
      "metadata": {
        "id": "pupmi3GAHWJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataSet"
      ],
      "metadata": {
        "id": "UZaUyq91Kn4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = load_coffee_data();\n",
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "id": "1UDAe7B8HkrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize Data\n",
        "Fitting the weights to the data (back-propagation, covered in next week's lectures) will proceed more quickly if the data is normalized. This is the same procedure you used in Course 1 where features in the data are each normalized to have a similar range. \n",
        "The procedure below uses a Keras [normalization layer](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/). It has the following steps:\n",
        "- create a \"Normalization Layer\". Note, as applied here, this is not a layer in your model.\n",
        "- 'adapt' the data. This learns the mean and variance of the data set and saves the values internally.\n",
        "- normalize the data.  \n",
        "It is important to apply normalization to any future data that utilizes the learned model."
      ],
      "metadata": {
        "id": "FF3ACvgLMpX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\")\n",
        "print(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\")\n",
        "norm_l = tf.keras.layers.Normalization(axis=-1)    # create a \"Normalization Layer\"\n",
        "norm_l.adapt(X)  # learns mean, variance\n",
        "Xn = norm_l(X)    # normalize the data.\n",
        "print(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\")\n",
        "print(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\")"
      ],
      "metadata": {
        "id": "Vrn_POzZL61V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tile/copy our data to increase the training set size and reduce the number of training epochs."
      ],
      "metadata": {
        "id": "sDvnh2OsM93l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xt = np.tile(Xn,(1000,1))\n",
        "Yt= np.tile(Y,(1000,1))   \n",
        "print(Xt.shape, Yt.shape)"
      ],
      "metadata": {
        "id": "oOEQ1ke7NCg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Model"
      ],
      "metadata": {
        "id": "hqUMM7S-PpBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build the \"Coffee Roasting Network\". There are two layers with sigmoid activations as shown below:"
      ],
      "metadata": {
        "id": "l7QYMVyKPvjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(2,)),\n",
        "        Dense(3, activation='sigmoid', name = 'layer1'),\n",
        "        Dense(1, activation='sigmoid', name = 'layer2')\n",
        "     ]\n",
        ")"
      ],
      "metadata": {
        "id": "fvo2onrTP0sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer has six w and three b. The second layer has three w and one b.\n"
      ],
      "metadata": {
        "id": "74x2AdOCQcR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Note 1:** The `tf.keras.Input(shape=(2,)),` specifies the expected shape of the input. This allows Tensorflow to size the weights and bias parameters at this point.  This is useful when exploring Tensorflow models. This statement can be omitted in practice and Tensorflow will size the network parameters when the input data is specified in the `model.fit` statement.  \n",
        ">**Note 2:** Including the sigmoid activation in the final layer is not considered best practice. It would instead be accounted for in the loss which improves numerical stability. This will be described in more detail in a later lab.\n",
        "\n",
        "The `model.summary()` provides a description of the network:"
      ],
      "metadata": {
        "id": "Sc_hdtFKRAtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Cjs9GECeQDVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine the weights and biases Tensorflow has instantiated.  The weights $W$ should be of size (number of features in input, number of units in the layer) while the bias $b$ size should match the number of units in the layer:\n",
        "- In the first layer with 3 units, we expect W to have a size of (2,3) and $b$ should have 3 elements.\n",
        "- In the second layer with 1 unit, we expect W to have a size of (3,1) and $b$ should have 1 element."
      ],
      "metadata": {
        "id": "kFQr__rfS1ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
        "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)"
      ],
      "metadata": {
        "id": "p8OftCbsS8oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following statements will be described in detail later. For now:\n",
        "- The `model.compile` statement defines a loss function and specifies a compile optimization.\n",
        "- The `model.fit` statement runs gradient descent and fits the weights to the data."
      ],
      "metadata": {
        "id": "0z8-t2emTDui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    Xt,Yt,            \n",
        "    epochs=10,\n",
        ")"
      ],
      "metadata": {
        "id": "gNa8v4EdS8Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Updated Weights\n",
        "After fitting, the weights have been updated: "
      ],
      "metadata": {
        "id": "zBsAmqw2VMdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
        "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
      ],
      "metadata": {
        "id": "0jFNEyHYVPFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by creating input data. The model is expecting one or more examples where examples are in the rows of matrix. In this case, we have two features so the matrix will be (m,2) where m is the number of examples.\n",
        "Recall, we have normalized the input features so we must normalize our test data as well.   \n",
        "To make a prediction, you apply the `predict` method."
      ],
      "metadata": {
        "id": "pdICQ-bZXhmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array([\n",
        "    [200,13.9],  # postive example\n",
        "    [200,17]])   # negative example\n",
        "X_testn = norm_l(X_test)\n",
        "predictions = model.predict(X_testn)\n",
        "print(\"predictions = \\n\", predictions)"
      ],
      "metadata": {
        "id": "k1ao7C3CXknh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Epochs and batches\n",
        "In the `compile` statement above, the number of `epochs` was set to 10. This specifies that the entire data set should be applied during training 10 times.  During training, you see output describing the progress of training that looks like this:\n",
        "```\n",
        "Epoch 1/10\n",
        "6250/6250 [==============================] - 6s 910us/step - loss: 0.1782\n",
        "```\n",
        "The first line, `Epoch 1/10`, describes which epoch the model is currently running. For efficiency, the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line `6250/6250 [====` is describing which batch has been executed."
      ],
      "metadata": {
        "id": "OtR3H_CEX61v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert the probabilities to a decision, we apply a threshold:"
      ],
      "metadata": {
        "id": "atV6VDDWYAgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = np.zeros_like(predictions)\n",
        "for i in range(len(predictions)):\n",
        "    if predictions[i] >= 0.5:\n",
        "        yhat[i] = 1\n",
        "    else:\n",
        "        yhat[i] = 0\n",
        "print(f\"decisions = \\n{yhat}\")"
      ],
      "metadata": {
        "id": "fTtz2kPRYHin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be accomplished more succinctly:"
      ],
      "metadata": {
        "id": "IfavVteuYK9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = (predictions >= 0.5).astype(int)\n",
        "print(f\"decisions = \\n{yhat}\")"
      ],
      "metadata": {
        "id": "z2kbGpBkYQ1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###These are the functions we use above:"
      ],
      "metadata": {
        "id": "RbqgWk6mI76Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_coffee_data():\n",
        "    \"\"\" Creates a coffee roasting data set.\n",
        "        roasting duration: 12-15 minutes is best\n",
        "        temperature range: 175-260C is best\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(2)\n",
        "    X = rng.random(400).reshape(-1,2)\n",
        "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
        "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
        "    Y = np.zeros(len(X))\n",
        "    \n",
        "    i=0\n",
        "    for t,d in X:\n",
        "        y = -3/(260-175)*t + 21\n",
        "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
        "            Y[i] = 1\n",
        "        else:\n",
        "            Y[i] = 0\n",
        "        i += 1\n",
        "\n",
        "    return (X, Y.reshape(-1,1))"
      ],
      "metadata": {
        "id": "Vi_j45jrI7iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Y0hpWLwI6iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "h-0aj0uuk7TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1,2]])\n",
        "w = np.array([[1,2,3], [4,5,6]])\n",
        "b = np.array([1,2,3])\n",
        "x @ w + b    # np.matmul(x,w)+b does the same"
      ],
      "metadata": {
        "id": "BrXkhyOqlFr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Implementation of matplotlib function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import LogNorm\n",
        "      \n",
        "dx, dy = 0.015, 0.05\n",
        "y, x = np.mgrid[slice(-4, 4 + dy, dy),\n",
        "                slice(-4, 4 + dx, dx)]\n",
        "z = (1 - x / 3. + x ** 5 + y ** 5) * np.exp(-x ** 2 - y ** 2)\n",
        "z = z[:-1, :-1]\n",
        "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
        "  \n",
        "plt.imshow(y, cmap ='Reds')\n",
        "\n",
        "  \n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XA5QKjFcm3UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.min()"
      ],
      "metadata": {
        "id": "JzYBwDk-gaqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0:3]"
      ],
      "metadata": {
        "id": "zvvkdy9xcbYD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}