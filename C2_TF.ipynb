{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mq53ARLRHHfj",
        "pupmi3GAHWJ1",
        "UZaUyq91Kn4u",
        "iIyAtOK90QDj",
        "KVIsV85L0wUj",
        "NLWZPcD-16rl",
        "Oz_KHhZ-DetU",
        "uFbIY43n4HUD",
        "fNHHRj4GjzZT",
        "TPrFTRGcCXk3",
        "h-0aj0uuk7TV"
      ],
      "authorship_tag": "ABX9TyPaks/I2Kv4ElqrsHF16Osn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HomayounfarM/machine-learning-andrew-ng/blob/main/C2_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W1_Lab01_TF"
      ],
      "metadata": {
        "id": "mq53ARLRHHfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Tensorflow and Keras\n",
        "Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by FranÃ§ois Chollet that creates a simple, layer-centric interface to Tensorflow.\n",
        "The following example shows to use tersorflow and Keras to create a layer. The first layer includes three cells (units). The second layer infludes one cell.\n",
        "\n",
        "In the following example, we have two layers: the first layer includes three unit (cell) and the second layer includes one unit (cell).\n",
        "Our x can be an array with any length. then, w will be an array with the same length and the inner production of these two array plus a 'b' wil be assigned to the first cell of that layer."
      ],
      "metadata": {
        "id": "Eg9FXGzsBqLh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7eVKTN4__8Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "#from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
        "#from tensorflow.keras.activations import sigmoid\n",
        "#import matplotlib.colors as colors\n",
        "\n",
        "x = np.array([[200, 1, 4, 6, 7, 8]], dtype = float)\n",
        "\n",
        "print(type(x))\n",
        "print(x.shape)\n",
        "print(x.dtype)\n",
        "\n",
        "layer_1 = tf.keras.layers.Dense(units = 3, activation = 'sigmoid')\n",
        "a1 = layer_1(x)\n",
        "print(a1.numpy())\n",
        "print(a1)\n",
        "\n",
        "layer_2 = Dense(units = 1, activation = 'sigmoid')\n",
        "a2 = layer_2(a1)\n",
        "\n",
        "print(a2.numpy())    # use .numpy() to convert the tensorflow to an np.array\n",
        "\n",
        "if a2 > 0.5:\n",
        "    yhat = 1\n",
        "else:\n",
        "    yhat = 0\n",
        "\n",
        "print(yhat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "X_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\n",
        "Y_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\n",
        "\n",
        "pos = Y_train == 1\n",
        "neg = Y_train == 0\n",
        "X_train[pos]\n",
        "\n",
        "pos = Y_train == 1\n",
        "neg = Y_train == 0\n",
        "\n",
        "\n",
        "dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC')\n",
        "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
        "ax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "ax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',\n",
        "              edgecolors=dlc[\"dlmagenta\"],lw=3)\n",
        "\n",
        "ax.set_ylim(-0.08,1.1)\n",
        "ax.set_ylabel('y', fontsize=12)\n",
        "ax.set_xlabel('x', fontsize=12)\n",
        "ax.set_title('one variable plot')\n",
        "ax.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hDy7DrsbCIUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Neuron\n",
        "\n",
        "We can implement a 'logistic neuron' by adding a sigmoid activation. The function of the neuron is then described by (1).  \n",
        "\n",
        "$$ f_{\\mathbf{w},b}(x^{(i)}) = g(\\mathbf{w}x^{(i)} + b) \\tag{1}$$\n",
        "where $$g(x) = sigmoid(x)$$\n",
        "\n",
        "Let's set $w$ and $b$ to some known values and check the model.\n",
        "\n",
        "This section will create a Tensorflow Model that contains our logistic layer to demonstrate an alternate method of creating models. Tensorflow is most often used to create multi-layer models. The [Sequential](https://keras.io/guides/sequential_model/) model is a convenient means of constructing these models.\n",
        "We can implement a 'logistic'\n",
        "\n"
      ],
      "metadata": {
        "id": "8eajVEDoDH6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Rvy3-4v_Dhdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model.summary()` shows the layers and number of parameters in the model. There is only one layer in this model and that layer has only one unit. The unit has two parameters, $w$ and $b$.\n",
        "The first term in the 'Sequential function' determine the number of cells (units) in a layer. The second term, 'input_dim', is to determine the dimention of input to this layer.  "
      ],
      "metadata": {
        "id": "FS_AQ-2yD863"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "KvHWvdOsECw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W and b are randomly assigned. We need to train the model to find the best values for  and b"
      ],
      "metadata": {
        "id": "Yboq3i8WwBmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_layer = model.get_layer('L1')\n",
        "w,b = logistic_layer.get_weights()\n",
        "print(w,b)\n",
        "print(w.shape,b.shape)"
      ],
      "metadata": {
        "id": "EnXcZg4wED5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set the weight and bias to some known values."
      ],
      "metadata": {
        "id": "-7LVRJoREM2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I1uR3XLTEM1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_w = np.array([[2]])\n",
        "set_b = np.array([-4.5])\n",
        "# set_weights takes a list of numpy arrays\n",
        "logistic_layer.set_weights([set_w, set_b])\n",
        "print(logistic_layer.get_weights())"
      ],
      "metadata": {
        "id": "wME2GEXFEL92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare equation (2) to the layer output."
      ],
      "metadata": {
        "id": "GBh9fB-tEVYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = model.predict(X_train[0].reshape(1,1))\n",
        "print(a1)\n"
      ],
      "metadata": {
        "id": "oWU6UugGEWHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The followings are the functions used above"
      ],
      "metadata": {
        "id": "raaKGXU8FPOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoidnp(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    z : array_like\n",
        "        A scalar or numpy array of any size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "     g : array_like\n",
        "         sigmoid(z)\n",
        "    \"\"\"\n",
        "    z = np.clip( z, -500, 500 )           # protect against overflow\n",
        "    g = 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "VEqu_euWEsM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_logistic(X_train, Y_train, model, set_w, set_b, pos, neg):\n",
        "    fig,ax = plt.subplots(1,2,figsize=(16,4))\n",
        "\n",
        "    layerf= lambda x : model.predict(x)\n",
        "    plt_prob_1d(ax[0], layerf)\n",
        "\n",
        "    ax[0].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "    ax[0].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',\n",
        "                  edgecolors=dlc[\"dlblue\"],lw=3)\n",
        "\n",
        "    ax[0].set_ylim(-0.08,1.1)\n",
        "    ax[0].set_xlim(-0.5,5.5)\n",
        "    ax[0].set_ylabel('y', fontsize=16)\n",
        "    ax[0].set_xlabel('x', fontsize=16)\n",
        "    ax[0].set_title('Tensorflow Model', fontsize=20)\n",
        "    ax[0].legend(fontsize=16)\n",
        "\n",
        "    layerf= lambda x : sigmoidnp(np.dot(set_w,x.reshape(1,1)) + set_b)\n",
        "    plt_prob_1d(ax[1], layerf)\n",
        "\n",
        "    ax[1].scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "    ax[1].scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none',\n",
        "                  edgecolors=dlc[\"dlblue\"],lw=3)\n",
        "\n",
        "    ax[1].set_ylim(-0.08,1.1)\n",
        "    ax[1].set_xlim(-0.5,5.5)\n",
        "    ax[1].set_ylabel('y', fontsize=16)\n",
        "    ax[1].set_xlabel('x', fontsize=16)\n",
        "    ax[1].set_title('Numpy Model', fontsize=20)\n",
        "    ax[1].legend(fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BuSuJLwYEyoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_prob_1d(ax,fwb):\n",
        "    \"\"\" plots a decision boundary but include shading to indicate the probability \"\"\"\n",
        "    #setup useful ranges and common linspaces\n",
        "    x_space  = np.linspace(0, 5 , 50)\n",
        "    y_space  = np.linspace(0, 1 , 50)\n",
        "\n",
        "    # get probability for x range, extend to y\n",
        "    z = np.zeros((len(x_space),len(y_space)))\n",
        "    for i in range(len(x_space)):\n",
        "        x = np.array([[x_space[i]]])\n",
        "        z[:,i] = fwb(x)\n",
        "\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "    new_cmap = truncate_colormap(cmap, 0.0, 0.5)\n",
        "    pcm = ax.pcolormesh(x_space, y_space, z,\n",
        "                   norm=cm.colors.Normalize(vmin=0, vmax=1),\n",
        "                   cmap=new_cmap, shading='nearest', alpha = 0.9)\n",
        "    ax.figure.colorbar(pcm, ax=ax)\n",
        "\n",
        "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
        "    \"\"\" truncates color map \"\"\"\n",
        "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
        "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
        "        cmap(np.linspace(minval, maxval, n)))\n",
        "    return new_cmap"
      ],
      "metadata": {
        "id": "e4Cxz4odE1fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W1_Lab02_TF"
      ],
      "metadata": {
        "id": "pupmi3GAHWJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataSet"
      ],
      "metadata": {
        "id": "UZaUyq91Kn4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = load_coffee_data();\n",
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "id": "1UDAe7B8HkrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize Data\n",
        "Fitting the weights to the data (back-propagation, covered in next week's lectures) will proceed more quickly if the data is normalized. This is the same procedure you used in Course 1 where features in the data are each normalized to have a similar range.\n",
        "The procedure below uses a Keras [normalization layer](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/). It has the following steps:\n",
        "- create a \"Normalization Layer\". Note, as applied here, this is not a layer in your model.\n",
        "- 'adapt' the data. This learns the mean and variance of the data set and saves the values internally.\n",
        "- normalize the data.  \n",
        "It is important to apply normalization to any future data that utilizes the learned model."
      ],
      "metadata": {
        "id": "FF3ACvgLMpX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\")\n",
        "print(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\")\n",
        "norm_l = tf.keras.layers.Normalization(axis=-1)    # create a \"Normalization Layer\"\n",
        "norm_l.adapt(X)  # learns mean, variance\n",
        "Xn = norm_l(X)    # normalize the data.\n",
        "print(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\")\n",
        "print(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\")"
      ],
      "metadata": {
        "id": "Vrn_POzZL61V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tile/copy our data to increase the training set size and reduce the number of training epochs."
      ],
      "metadata": {
        "id": "sDvnh2OsM93l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xt = np.tile(Xn,(1000,1))\n",
        "Yt= np.tile(Y,(1000,1))\n",
        "print(Xt.shape, Yt.shape)"
      ],
      "metadata": {
        "id": "oOEQ1ke7NCg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Model"
      ],
      "metadata": {
        "id": "hqUMM7S-PpBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build the \"Coffee Roasting Network\". There are two layers with sigmoid activations as shown below:"
      ],
      "metadata": {
        "id": "l7QYMVyKPvjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(2,)),\n",
        "        Dense(3, activation='sigmoid', name = 'layer1'),\n",
        "        Dense(1, activation='sigmoid', name = 'layer2')\n",
        "     ]\n",
        ")"
      ],
      "metadata": {
        "id": "fvo2onrTP0sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer has six w and three b. The second layer has three w and one b.\n"
      ],
      "metadata": {
        "id": "74x2AdOCQcR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Note 1:** The `tf.keras.Input(shape=(2,)),` specifies the expected shape of the input. This allows Tensorflow to size the weights and bias parameters at this point.  This is useful when exploring Tensorflow models. This statement can be omitted in practice and Tensorflow will size the network parameters when the input data is specified in the `model.fit` statement.  \n",
        ">**Note 2:** Including the sigmoid activation in the final layer is not considered best practice. It would instead be accounted for in the loss which improves numerical stability. This will be described in more detail in a later lab.\n",
        "\n",
        "The `model.summary()` provides a description of the network:"
      ],
      "metadata": {
        "id": "Sc_hdtFKRAtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Cjs9GECeQDVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine the weights and biases Tensorflow has instantiated.  The weights $W$ should be of size (number of features in input, number of units in the layer) while the bias $b$ size should match the number of units in the layer:\n",
        "- In the first layer with 3 units, we expect W to have a size of (2,3) and $b$ should have 3 elements.\n",
        "- In the second layer with 1 unit, we expect W to have a size of (3,1) and $b$ should have 1 element."
      ],
      "metadata": {
        "id": "kFQr__rfS1ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
        "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)"
      ],
      "metadata": {
        "id": "p8OftCbsS8oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following statements will be described in detail later. For now:\n",
        "- The `model.compile` statement defines a loss function and specifies a compile optimization.\n",
        "- The `model.fit` statement runs gradient descent and fits the weights to the data."
      ],
      "metadata": {
        "id": "0z8-t2emTDui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    Xt,Yt,\n",
        "    epochs=10,\n",
        ")"
      ],
      "metadata": {
        "id": "gNa8v4EdS8Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Updated Weights\n",
        "After fitting, the weights have been updated:"
      ],
      "metadata": {
        "id": "zBsAmqw2VMdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
        "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
        "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
        "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
      ],
      "metadata": {
        "id": "0jFNEyHYVPFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by creating input data. The model is expecting one or more examples where examples are in the rows of matrix. In this case, we have two features so the matrix will be (m,2) where m is the number of examples.\n",
        "Recall, we have normalized the input features so we must normalize our test data as well.   \n",
        "To make a prediction, you apply the `predict` method."
      ],
      "metadata": {
        "id": "pdICQ-bZXhmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array([\n",
        "    [200,13.9],  # postive example\n",
        "    [200,17]])   # negative example\n",
        "X_testn = norm_l(X_test)\n",
        "predictions = model.predict(X_testn)\n",
        "print(\"predictions = \\n\", predictions)"
      ],
      "metadata": {
        "id": "k1ao7C3CXknh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Epochs and batches\n",
        "In the `compile` statement above, the number of `epochs` was set to 10. This specifies that the entire data set should be applied during training 10 times.  During training, you see output describing the progress of training that looks like this:\n",
        "```\n",
        "Epoch 1/10\n",
        "6250/6250 [==============================] - 6s 910us/step - loss: 0.1782\n",
        "```\n",
        "The first line, `Epoch 1/10`, describes which epoch the model is currently running. For efficiency, the training data set is broken into 'batches'. The default size of a batch in Tensorflow is 32. There are 200000 examples in our expanded data set or 6250 batches. The notation on the 2nd line `6250/6250 [====` is describing which batch has been executed."
      ],
      "metadata": {
        "id": "OtR3H_CEX61v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert the probabilities to a decision, we apply a threshold:"
      ],
      "metadata": {
        "id": "atV6VDDWYAgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = np.zeros_like(predictions)\n",
        "for i in range(len(predictions)):\n",
        "    if predictions[i] >= 0.5:\n",
        "        yhat[i] = 1\n",
        "    else:\n",
        "        yhat[i] = 0\n",
        "print(f\"decisions = \\n{yhat}\")"
      ],
      "metadata": {
        "id": "fTtz2kPRYHin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be accomplished more succinctly:"
      ],
      "metadata": {
        "id": "IfavVteuYK9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = (predictions >= 0.5).astype(int)\n",
        "print(f\"decisions = \\n{yhat}\")"
      ],
      "metadata": {
        "id": "z2kbGpBkYQ1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###These are the functions we use above:"
      ],
      "metadata": {
        "id": "RbqgWk6mI76Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_coffee_data():\n",
        "    \"\"\" Creates a coffee roasting data set.\n",
        "        roasting duration: 12-15 minutes is best\n",
        "        temperature range: 175-260C is best\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(2)\n",
        "    X = rng.random(400).reshape(-1,2)\n",
        "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
        "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
        "    Y = np.zeros(len(X))\n",
        "\n",
        "    i=0\n",
        "    for t,d in X:\n",
        "        y = -3/(260-175)*t + 21\n",
        "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
        "            Y[i] = 1\n",
        "        else:\n",
        "            Y[i] = 0\n",
        "        i += 1\n",
        "\n",
        "    return (X, Y.reshape(-1,1))"
      ],
      "metadata": {
        "id": "Vi_j45jrI7iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Y0hpWLwI6iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W1_Lab03_TF"
      ],
      "metadata": {
        "id": "iIyAtOK90QDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lab: Neural Networks for Handwritten Digit Recognition, BinaryÂ¶\n",
        "In this exercise, you will use a neural network to recognize the hand-written digits zero and one."
      ],
      "metadata": {
        "id": "KVIsV85L0wUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from autils import *\n",
        "#%matplotlib inline\n",
        "\n",
        "#import logging\n",
        "#logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "#tf.autograph.set_verbosity(0)"
      ],
      "metadata": {
        "id": "CDd2Pblm0iEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "UD7IDFqZ_4Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem Statement\n",
        "In this exercise, you will use a neural network to recognize two handwritten digits, zero and one. This is a binary classification task. Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks. You will extend this network to recognize all 10 digits (0-9) in a future assignment.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "You will start by loading the dataset for this task.\n",
        "- The `load_data()` function shown below loads the data into variables `X` and `y`\n",
        "\n",
        "\n",
        "- The data set contains 1000 training examples of handwritten digits $^1$, here limited to zero and one.  \n",
        "\n",
        "    - Each training example is a 20-pixel x 20-pixel grayscale image of the digit.\n",
        "        - Each pixel is represented by a floating-point number indicating the grayscale intensity at that location.\n",
        "        - The 20 by 20 grid of pixels is âunrolledâ into a 400-dimensional vector.\n",
        "        - Each training example becomes a single row in our data matrix `X`.\n",
        "        - This gives us a 1000 x 400 matrix `X` where every row is a training example of a handwritten digit image.\n",
        "\n",
        "$$X =\n",
        "\\left(\\begin{array}{cc}\n",
        "--- (x^{(1)}) --- \\\\\n",
        "--- (x^{(2)}) --- \\\\\n",
        "\\vdots \\\\\n",
        "--- (x^{(m)}) ---\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "- The second part of the training set is a 1000 x 1 dimensional vector `y` that contains labels for the training set\n",
        "    - `y = 0` if the image is of the digit `0`, `y = 1` if the image is of the digit `1`.\n",
        "\n",
        "$^1$<sub> This is a subset of the MNIST handwritten digit dataset (http://yann.lecun.com/exdb/mnist/)</sub>"
      ],
      "metadata": {
        "id": "NLWZPcD-16rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "link = 'https://drive.google.com/file/d/1iR3DzNzotfp6y4MWjr2MJJWIvaE527xT/view?usp=share_link'\n",
        "link1 = 'https://drive.google.com/file/d/1PTtDlUkJ3TatNpaSiIb2t98uIs3B4_Da/view?usp=share_link'\n",
        "\n",
        "# to get the id part of the file\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('data.csv')\n",
        "df = pd.read_csv('data.csv', index_col=None)\n",
        "df = df.drop('Unnamed: 0', axis=1)\n",
        "X = df.to_numpy()\n",
        "\n",
        "id1 = link1.split(\"/\")[-2]\n",
        "downloaded1 = drive.CreateFile({'id':id1})\n",
        "downloaded1.GetContentFile('data1.csv')\n",
        "df1 = pd.read_csv('data1.csv', index_col=None)\n",
        "df1 = df1.drop('Unnamed: 0', axis=1)\n",
        "y = df1.to_numpy()"
      ],
      "metadata": {
        "id": "Rg-Du-Ro0iBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing the Data\n",
        "\n",
        "You will begin by visualizing a subset of the training set.\n",
        "- In the cell below, the code randomly selects 64 rows from `X`, maps each row back to a 20 pixel by 20 pixel grayscale image and displays the images together.\n",
        "- The label for each image is displayed above the image"
      ],
      "metadata": {
        "id": "HwyQLc50_tS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "# You do not need to modify anything in this cell\n",
        "\n",
        "m, n = X.shape\n",
        "\n",
        "fig, axes = plt.subplots(8,8, figsize=(8,8))\n",
        "fig.tight_layout(pad=0.1)\n",
        "\n",
        "for i,ax in enumerate(axes.flat):\n",
        "    # Select random indices\n",
        "    random_index = np.random.randint(m)\n",
        "\n",
        "    # Select rows corresponding to the random indices and\n",
        "    # reshape the image\n",
        "    X_random_reshaped = X[random_index].reshape((20,20)).T\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(X_random_reshaped, cmap='Greys')\n",
        "\n",
        "    # Display the label above the image\n",
        "    ax.set_title(y[random_index,0])\n",
        "    ax.set_axis_off()"
      ],
      "metadata": {
        "id": "8g3mFpfy_C4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model representation\n",
        "\n",
        "The neural network you will use in this assignment is shown in the figure below.\n",
        "- This has three dense layers with sigmoid activations.\n",
        "    - Recall that our inputs are pixel values of digit images.\n",
        "    - Since the images are of size 20 Ã 20, this gives us 400 inputs  \n",
        "    \n",
        "![picture](https://drive.google.com/uc?export=view&id=1UfjXiLEWsYCl8fvizkflk3SXxey2y7Le)\n",
        "\n"
      ],
      "metadata": {
        "id": "Oz_KHhZ-DetU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The parameters have dimensions that are sized for a neural network with $25$ units in layer 1, $15$ units in layer 2 and $1$ output unit in layer 3.\n",
        "\n",
        "    - Recall that the dimensions of these parameters are determined as follows:\n",
        "        - If network has $s_{in}$ units in a layer and $s_{out}$ units in the next layer, then\n",
        "            - $W$ will be of dimension $s_{in} \\times s_{out}$.\n",
        "            - $b$ will a vector with $s_{out}$ elements\n",
        "  \n",
        "    - Therefore, the shapes of `W`, and `b`,  are\n",
        "        - layer1: The shape of `W1` is (400, 25) and the shape of `b1` is (25,)\n",
        "        - layer2: The shape of `W2` is (25, 15) and the shape of `b2` is: (15,)\n",
        "        - layer3: The shape of `W3` is (15, 1) and the shape of `b3` is: (1,)\n",
        ">**Note:** The bias vector `b` could be represented as a 1-D (n,) or 2-D (1,n) array. Tensorflow utilizes a 1-D representation and this lab will maintain that convention."
      ],
      "metadata": {
        "id": "UZ_GmVxuIQi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Model Implementation"
      ],
      "metadata": {
        "id": "CsFI4KmIKUXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow models are built layer by layer. A layer's input dimensions ($s_{in}$ above) are calculated for you. You specify a layer's *output dimensions* and this determines the next layer's input dimension. The input dimension of the first layer is derived from the size of the input data specified in the `model.fit` statement below.\n",
        ">**Note:** It is also possible to add an input layer that specifies the input dimension of the first layer. For example:  \n",
        "`tf.keras.Input(shape=(400,)),    #specify input shape`  \n",
        "We will include that here to illuminate some model sizing."
      ],
      "metadata": {
        "id": "0ax8vqcLKY92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(400,)),    #specify input size\n",
        "        ### START CODE HERE ###\n",
        "        tf.keras.layers.Dense(25, input_dim=400,  activation = 'sigmoid', name='L1'),\n",
        "        tf.keras.layers.Dense(15, input_dim=25,  activation = 'sigmoid', name='L2'),\n",
        "        tf.keras.layers.Dense(1, input_dim=15,  activation = 'sigmoid', name='L3')\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "    ], name = \"my_model\"\n",
        ")"
      ],
      "metadata": {
        "id": "qYzzYbkL4JhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "YIReF-Ol4JeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can examine details of the model by first extracting the layers with model.layers and then extracting the weights with layerx.get_weights() as shown below."
      ],
      "metadata": {
        "id": "O-3ikM3W_CWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[layer1, layer2, layer3] = model.layers\n",
        "#### Examine Weights shapes\n",
        "W1,b1 = layer1.get_weights()\n",
        "W2,b2 = layer2.get_weights()\n",
        "W3,b3 = layer3.get_weights()\n",
        "print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
        "print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\n",
        "print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")"
      ],
      "metadata": {
        "id": "DlxLGAcA4JbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "xx.get_weights returns a NumPy array. One can also access the weights directly in their tensor form. Note the shape of the tensors in the final layer."
      ],
      "metadata": {
        "id": "vitR-MlKArJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.layers[2].weights)"
      ],
      "metadata": {
        "id": "0PMOzPkE4JYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will define a loss function and run gradient descent to fit the weights of the model to the training data. This will be explained in more detail later."
      ],
      "metadata": {
        "id": "8kkilKO5A-ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X,y,\n",
        "    epochs=20\n",
        ")"
      ],
      "metadata": {
        "id": "mk4AmlQX4JV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the coefficients after trainig thwe model\n",
        "print(model.layers[2].weights)"
      ],
      "metadata": {
        "id": "1FnhlFVvBrim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the model on an example to make a prediction, use Keras predict. The input to predict is an array so the single example is reshaped to be two dimensional."
      ],
      "metadata": {
        "id": "eDOUAIatCQ8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(X[0].reshape(1,400))  # a zero\n",
        "print(f\" predicting a zero: {prediction}\")\n",
        "prediction = model.predict(X[500].reshape(1,400))  # a one\n",
        "print(f\" predicting a one:  {prediction}\")"
      ],
      "metadata": {
        "id": "WsR0us93Brfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the model is interpreted as a probability. In the first example above, the input is a zero. The model predicts the probability that the input is a one is nearly zero. In the second example, the input is a one. The model predicts the probability that the input is a one is nearly one. As in the case of logistic regression, the probability is compared to a threshold to make a final prediction."
      ],
      "metadata": {
        "id": "967HllniCarv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if prediction >= 0.5:\n",
        "    yhat = 1\n",
        "else:\n",
        "    yhat = 0\n",
        "print(f\"prediction after threshold: {yhat}\")"
      ],
      "metadata": {
        "id": "dpWQlRB4Brcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the predictions vs the labels for a random sample of 64 digits. This takes a moment to run."
      ],
      "metadata": {
        "id": "Fyt8q6ynC5ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "# You do not need to modify anything in this cell\n",
        "\n",
        "m, n = X.shape\n",
        "\n",
        "fig, axes = plt.subplots(8,8, figsize=(8,8))\n",
        "fig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]\n",
        "\n",
        "for i,ax in enumerate(axes.flat):\n",
        "    # Select random indices\n",
        "    random_index = np.random.randint(m)\n",
        "\n",
        "    # Select rows corresponding to the random indices and\n",
        "    # reshape the image\n",
        "    X_random_reshaped = X[random_index].reshape((20,20)).T\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(X_random_reshaped, cmap='gray')\n",
        "\n",
        "    # Predict using the Neural Network\n",
        "    prediction = model.predict(X[random_index].reshape(1,400))\n",
        "    if prediction >= 0.5:\n",
        "        yhat = 1\n",
        "    else:\n",
        "        yhat = 0\n",
        "\n",
        "    # Display the label above the image\n",
        "    ax.set_title(f\"{y[random_index,0]},{yhat}\")\n",
        "    ax.set_axis_off()\n",
        "fig.suptitle(\"Label, yhat\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hWb2iF4NBrZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLaEyzJXBrWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kg97oXlpBrT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###These are the functions we use above:"
      ],
      "metadata": {
        "id": "uFbIY43n4HUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "    X = np.load(\"data/X.npy\")\n",
        "    y = np.load(\"data/y.npy\")\n",
        "    X = X[0:1000]\n",
        "    y = y[0:1000]\n",
        "    return X, y\n",
        "\n",
        "def load_weights():\n",
        "    w1 = np.load(\"data/w1.npy\")\n",
        "    b1 = np.load(\"data/b1.npy\")\n",
        "    w2 = np.load(\"data/w2.npy\")\n",
        "    b2 = np.load(\"data/b2.npy\")\n",
        "    return w1, b1, w2, b2\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1. / (1. + np.exp(-x))"
      ],
      "metadata": {
        "id": "q9JOmyu70hwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W2_Lab04_TF (Multiclass)"
      ],
      "metadata": {
        "id": "fNHHRj4GjzZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Lab - Multi-class Classification"
      ],
      "metadata": {
        "id": "uSWbifRxkzuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib widget\n",
        "from sklearn.datasets import make_blobs\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n"
      ],
      "metadata": {
        "id": "CC-DPvaukq8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare and visualize our data\n",
        "We will use Scikit-Learn make_blobs function to make a training data set with 4 categories as shown in the plot below."
      ],
      "metadata": {
        "id": "lLiEQyZIlKSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make 4-class dataset for classification\n",
        "classes = 4\n",
        "m = 100\n",
        "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
        "std = 1.0\n",
        "X_train, y_train = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=30)"
      ],
      "metadata": {
        "id": "DjTM9IyilJxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_mc(X_train,y_train,classes, centers, std=std)"
      ],
      "metadata": {
        "id": "fnrk3vs8lY8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each dot represents a training example. The axis (x0,x1) are the inputs and the color represents the class the example is associated with. Once trained, the model will be presented with a new example, (x0,x1), and will predict the class.\n",
        "\n",
        "While generated, this data set is representative of many real-world classification problems. There are several input features (x0,...,xn) and several output categories. The model is trained to use the input features to predict the correct output category."
      ],
      "metadata": {
        "id": "aVpaYnmynw9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "This lab will use a 2-layer network as shown. Unlike the binary classification networks, this network has four outputs, one for each class. Given an input example, the output with the highest value is the predicted class of the input.\n",
        "Below is an example of how to construct this network in Tensorflow. Notice the output layer uses a linear rather than a softmax activation. While it is possible to include the softmax in the output layer, it is more numerically stable if linear outputs are passed to the loss function during training. If the model is used to predict probabilities, the softmax can be applied at that point.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=12PG4uzZM4dr40M7WQ2jt1gehJhjzAccz)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F1S2mEuqTteM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(2, activation = 'relu',   name = \"L1\"),\n",
        "        Dense(4, activation = 'linear', name = \"L2\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "6qYPtSkjl131"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statements below compile and train the network. Setting from_logits=True as an argument to the loss function specifies that the output activation was linear rather than a softmax."
      ],
      "metadata": {
        "id": "MnsCBHNFVWuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(0.01),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=200\n",
        ")"
      ],
      "metadata": {
        "id": "LZCms9Xkl109"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can see that we have two X's, so, in the first layer, we have two w's and one b for each unit. We have two unit in the first layer. So, we have 6 parameters there. In the secon layer, we have 4 units, therefor, we have 12 parameters."
      ],
      "metadata": {
        "id": "sOZM6ZDhT_wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "aQD3UN-cT-nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the model trained, we can see how the model has classified the training data."
      ],
      "metadata": {
        "id": "oYq3A1tN5SSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt_cat_mc(X_train, y_train, model, classes)"
      ],
      "metadata": {
        "id": "cTYUoQdyl1yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, the decision boundaries show how the model has partitioned the input space. This very simple model has had no trouble classifying the training data. How did it accomplish this? Let's look at the network in more detail.\n",
        "\n",
        "Below, we will pull the trained weights from the model and use that to plot the function of each of the network units. Further down, there is a more detailed explanation of the results. You don't need to know these details to successfully use neural networks, but it may be helpful to gain more intuition about how the layers combine to solve a classification problem."
      ],
      "metadata": {
        "id": "qaEiS5mSVg6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gather the trained parameters from the first layer\n",
        "l1 = model.get_layer(\"L1\")\n",
        "W1,b1 = l1.get_weights()"
      ],
      "metadata": {
        "id": "HfupL5FNl1vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the function of the first layer\n",
        "plt_layer_relu(X_train, y_train.reshape(-1,), W1, b1, classes)"
      ],
      "metadata": {
        "id": "w9OWbbBEVqVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "Layer 1\n",
        "These plots show the function of Units 0 and 1 in the first layer of the network. The inputs are ( ð¥0,ð¥1\n",
        " ) on the axis. The output of the unit is represented by the color of the background. This is indicated by the color bar on the right of each graph. Notice that since these units are using a ReLu, the outputs do not necessarily fall between 0 and 1 and in this case are greater than 20 at their peaks. The contour lines in this graph show the transition point between the output,  ð[1]ð\n",
        "  being zero and non-zero. Recall the graph for a ReLu : The contour line in the graph is the inflection point in the ReLu.\n",
        "\n",
        "Unit 0 has separated classes 0 and 1 from classes 2 and 3. Points to the left of the line (classes 0 and 1) will output zero, while points to the right will output a value greater than zero.\n",
        "Unit 1 has separated classes 0 and 2 from classes 1 and 3. Points above the line (classes 0 and 2 ) will output a zero, while points below will output a value greater than zero. Let's see how this works out in the next layer!"
      ],
      "metadata": {
        "id": "6mkghqoAPLEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gather the trained parameters from the output layer\n",
        "l2 = model.get_layer(\"L2\")\n",
        "W2, b2 = l2.get_weights()\n",
        "# create the 'new features', the training examples after L1 transformation\n",
        "Xl2 = np.maximum(0, np.dot(X_train,W1) + b1)\n",
        "\n",
        "plt_output_layer_linear(Xl2, y_train.reshape(-1,), W2, b2, classes,\n",
        "                        x0_rng = (-0.25,np.amax(Xl2[:,0])), x1_rng = (-0.25,np.amax(Xl2[:,1])))"
      ],
      "metadata": {
        "id": "MoWyDpHUXyYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer 2, the output layer\n",
        "The dots in these graphs are the training examples translated by the first layer. One way to think of this is the first layer has created a new set of features for evaluation by the 2nd layer. The axes in these plots are the outputs of the previous layer  ð[1]0\n",
        "  and  ð[1]1\n",
        " . As predicted above, classes 0 and 1 (blue and green) have  ð[1]0=0\n",
        "  while classes 0 and 2 (blue and orange) have  ð[1]1=0\n",
        " .\n",
        "Once again, the intensity of the background color indicates the highest values.\n",
        "Unit 0 will produce its maximum value for values near (0,0), where class 0 (blue) has been mapped.\n",
        "Unit 1 produces its highest values in the upper left corner selecting class 1 (green).\n",
        "Unit 2 targets the lower right corner where class 2 (orange) resides.\n",
        "Unit 3 produces its highest values in the upper right selecting our final class (purple).\n",
        "\n",
        "One other aspect that is not obvious from the graphs is that the values have been coordinated between the units. It is not sufficient for a unit to produce a maximum value for the class it is selecting for, it must also be the highest value of all the units for points in that class. This is done by the implied softmax function that is part of the loss function (SparseCategoricalCrossEntropy). Unlike other activation functions, the softmax works across all the outputs."
      ],
      "metadata": {
        "id": "p55UHkkEPRWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the functions we use above:"
      ],
      "metadata": {
        "id": "VsY0G-9vl0tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import warnings\n",
        "from matplotlib import cm\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
        "import matplotlib.colors as colors\n",
        "from matplotlib import cm\n",
        "\n",
        "dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0', dldarkblue =  '#0D5BDC')\n",
        "dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; dldarkblue =  '#0D5BDC'\n",
        "dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\n",
        "\n",
        "dkcolors = plt.cm.Paired((1,3,7,9,5,11))\n",
        "ltcolors = plt.cm.Paired((0,2,6,8,4,10))\n",
        "dkcolors_map = mpl.colors.ListedColormap(dkcolors)\n",
        "ltcolors_map = mpl.colors.ListedColormap(ltcolors)\n",
        "\n",
        "def plt_mc(X_train,y_train,classes, centers, std):\n",
        "    css = np.unique(y_train)\n",
        "    fig,ax = plt.subplots(1,1,figsize=(3,3))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "    plt_mc_data(ax, X_train,y_train,classes, map=dkcolors_map, legend=True, size=50, equal_xy = False)\n",
        "    ax.set_title(\"Multiclass Data\")\n",
        "    ax.set_xlabel(\"x0\")\n",
        "    ax.set_ylabel(\"x1\")\n",
        "    #for c in css:\n",
        "    #    circ = plt.Circle(centers[c], 2*std, color=dkcolors_map(c), clip_on=False, fill=False, lw=0.5)\n",
        "    #    ax.add_patch(circ)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plt_mc_data(ax, X, y, classes,  class_labels=None, map=plt.cm.Paired,\n",
        "                legend=False, size=50, m='o', equal_xy = False):\n",
        "    \"\"\" Plot multiclass data. Note, if equal_xy is True, setting ylim on the plot may not work \"\"\"\n",
        "    for i in range(classes):\n",
        "        idx = np.where(y == i)\n",
        "        col = len(idx[0])*[i]\n",
        "        label = class_labels[i] if class_labels else \"c{}\".format(i)\n",
        "        # this didn't work on coursera but did in local version\n",
        "        #ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n",
        "        #            c=col, vmin=0, vmax=map.N, cmap=map,\n",
        "        #            s=size, label=label)\n",
        "        ax.scatter(X[idx, 0], X[idx, 1],  marker=m,\n",
        "                    color=map(col), vmin=0, vmax=map.N,\n",
        "                    s=size, label=label)\n",
        "    if legend: ax.legend()\n",
        "    if equal_xy: ax.axis(\"equal\")\n",
        "\n",
        "def plt_cat_mc(X_train, y_train, model, classes):\n",
        "    #make a model for plotting routines to call\n",
        "    model_predict = lambda Xl: np.argmax(model.predict(Xl),axis=1)\n",
        "\n",
        "    fig,ax = plt.subplots(1,1, figsize=(3,3))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "\n",
        "    #add the original data to the decison boundary\n",
        "    plt_mc_data(ax, X_train,y_train, classes, map=dkcolors_map, legend=True)\n",
        "    #plot the decison boundary.\n",
        "    plot_cat_decision_boundary_mc(ax, X_train, model_predict, vector=True)\n",
        "    ax.set_title(\"model decision boundary\")\n",
        "\n",
        "    plt.xlabel(r'$x_0$');\n",
        "    plt.ylabel(r\"$x_1$\");\n",
        "    plt.show()\n",
        "\n",
        "def plot_cat_decision_boundary_mc(ax, X, predict , class_labels=None, legend=False, vector=True):\n",
        "\n",
        "    # create a mesh to points to plot\n",
        "    x_min, x_max = X[:, 0].min()- 0.5, X[:, 0].max()+0.5\n",
        "    y_min, y_max = X[:, 1].min()- 0.5, X[:, 1].max()+0.5\n",
        "    h = max(x_max-x_min, y_max-y_min)/100\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    #print(\"points\", points.shape)\n",
        "    #print(\"xx.shape\", xx.shape)\n",
        "\n",
        "    #make predictions for each point in mesh\n",
        "    if vector:\n",
        "        Z = predict(points)\n",
        "    else:\n",
        "        Z = np.zeros((len(points),))\n",
        "        for i in range(len(points)):\n",
        "            Z[i] = predict(points[i].reshape(1,2))\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    #contour plot highlights boundaries between values - classes in this case\n",
        "    ax.contour(xx, yy, Z, linewidths=1)\n",
        "    #ax.axis('tight')\n",
        "\n",
        "def plt_layer_relu(X, Y, W1, b1, classes):\n",
        "    nunits = (W1.shape[1])\n",
        "    Y = Y.reshape(-1,)\n",
        "    fig,ax = plt.subplots(1,W1.shape[1], figsize=(7,2.5))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "\n",
        "    for i in range(nunits):\n",
        "        layerf= lambda x : np.maximum(0,(np.dot(x,W1[:,i]) + b1[i]))\n",
        "        plt_prob_z(ax[i], layerf)\n",
        "        plt_mc_data(ax[i], X, Y, classes, map=dkcolors_map,legend=True, size=50, m='o')\n",
        "        ax[i].set_title(f\"Layer 1 Unit {i}\")\n",
        "        ax[i].set_ylabel(r\"$x_1$\",size=10)\n",
        "        ax[i].set_xlabel(r\"$x_0$\",size=10)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plt_prob_z(ax,fwb, x0_rng=(-8,8), x1_rng=(-5,4)):\n",
        "    \"\"\" plots a decision boundary but include shading to indicate the probability\n",
        "        and adds a conouter to show where z=0\n",
        "    \"\"\"\n",
        "    #setup useful ranges and common linspaces\n",
        "    x0_space  = np.linspace(x0_rng[0], x0_rng[1], 40)\n",
        "    x1_space  = np.linspace(x1_rng[0], x1_rng[1], 40)\n",
        "\n",
        "    # get probability for x0,x1 ranges\n",
        "    tmp_x0,tmp_x1 = np.meshgrid(x0_space,x1_space)\n",
        "    z = np.zeros_like(tmp_x0)\n",
        "    c = np.zeros_like(tmp_x0)\n",
        "    for i in range(tmp_x0.shape[0]):\n",
        "        for j in range(tmp_x1.shape[1]):\n",
        "            x = np.array([[tmp_x0[i,j],tmp_x1[i,j]]])\n",
        "            z[i,j] = fwb(x)\n",
        "            c[i,j] = 0. if z[i,j] == 0 else 1.\n",
        "    with warnings.catch_warnings():  # suppress no contour warning\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        #ax.contour(tmp_x0, tmp_x1, c, colors='b', linewidths=1)\n",
        "        ax.contour(tmp_x0, tmp_x1, c, linewidths=1)\n",
        "\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "    new_cmap = truncate_colormap(cmap, 0.0, 0.7)\n",
        "\n",
        "    pcm = ax.pcolormesh(tmp_x0, tmp_x1, z,\n",
        "                   norm=cm.colors.Normalize(vmin=np.amin(z), vmax=np.amax(z)),\n",
        "                   cmap=new_cmap, shading='nearest', alpha = 0.9)\n",
        "    ax.figure.colorbar(pcm, ax=ax)\n",
        "\n",
        "def plt_output_layer_linear(X, Y, W, b, classes, x0_rng=None, x1_rng=None):\n",
        "    nunits = (W.shape[1])\n",
        "    Y = Y.reshape(-1,)\n",
        "    fig,ax = plt.subplots(2,int(nunits/2), figsize=(7,5))\n",
        "    fig.canvas.toolbar_visible = False\n",
        "    fig.canvas.header_visible = False\n",
        "    fig.canvas.footer_visible = False\n",
        "    for i,axi in enumerate(ax.flat):\n",
        "        layerf = lambda x : np.dot(x,W[:,i]) + b[i]\n",
        "        plt_prob_z(axi, layerf, x0_rng=x0_rng, x1_rng=x1_rng)\n",
        "        plt_mc_data(axi, X, Y, classes, map=dkcolors_map,legend=True, size=50, m='o')\n",
        "        axi.set_ylabel(r\"$a^{[1]}_1$\",size=9)\n",
        "        axi.set_xlabel(r\"$a^{[1]}_0$\",size=9)\n",
        "        axi.set_xlim(x0_rng)\n",
        "        axi.set_ylim(x1_rng)\n",
        "        axi.set_title(f\"Linear Output Unit {i}\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
        "    \"\"\" truncates color map \"\"\"\n",
        "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
        "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
        "        cmap(np.linspace(minval, maxval, n)))\n",
        "    return new_cmap"
      ],
      "metadata": {
        "id": "LXiSvLiqmNEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W2_Lab05_TF (Derivatives)\n"
      ],
      "metadata": {
        "id": "iJ1KOHMd-gDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Lab - Derivatives\n",
        "This lab will give you a more intuitive understanding of derivatives. It will show you a simple way of calculating derivatives arithmetically. It will also introduce you to a handy Python library that allows you to calculate derivatives symbolically."
      ],
      "metadata": {
        "id": "oVTl_zMO_CDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import symbols, diff"
      ],
      "metadata": {
        "id": "chPV75y1_FGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Informal definition of derivatives\n",
        "The formal definition of derivatives can be a bit daunting with limits and values 'going to zero'. The idea is really much simpler.\n",
        "\n",
        "The derivative of a function describes how the output of a function changes when there is a small change in an input variable.\n",
        "\n",
        "Let's use the cost function $J(w)$ as an example. The cost $J$ is the output and $w$ is the input variable.  \n",
        "Let's give a 'small change' a name *epsilon* or $\\epsilon$. We use these Greek letters because it is traditional in mathematics to use *epsilon*($\\epsilon$) or *delta* ($\\Delta$) to represent a small value. You can think of it as representing 0.001 or some other small value.  \n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\text{if } w \\uparrow \\epsilon \\text{ causes }J(w) \\uparrow \\text{by }k \\times \\epsilon \\text{ then}  \\\\\n",
        "\\frac{\\partial J(w)}{\\partial w} = k \\tag{1}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "This just says if you change the input to the function $J(w)$ by a little bit and the output changes by $k$ times that little bit, then the derivative of $J(w)$ is equal to $k$.\n",
        "\n",
        "Let's try this out.  Let's look at the derivative of the function $J(w) = w^2$ at the point $w=3$ and $\\epsilon = 0.001$"
      ],
      "metadata": {
        "id": "VdviazUA_OEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J = (3)**2\n",
        "J_epsilon = (3 + 0.001)**2\n",
        "k = (J_epsilon - J)/0.001    # difference divided by epsilon\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k:0.6f} \")"
      ],
      "metadata": {
        "id": "6spHLMt7_uAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have increased the input value a little bit (0.001), causing the output to change from 9 to 9.006001, an increase of 6 times the input increase. Referencing (1) above, this says that $k=6$, so $\\frac{\\partial J(w)}{\\partial w} \\approx 6$. If you are familiar with calculus, you know, written symbolically,  $\\frac{\\partial J(w)}{\\partial w} = 2 w$. With $w=3$ this is 6. Our calculation above is not exactly 6 because to be exactly correct $\\epsilon$ would need to be [infinitesimally small](https://www.dictionary.com/browse/infinitesimally) or really, really small. That is why we use the symbols $\\approx$ or ~= rather than =. Let's see what happens if we make $\\epsilon$ smaller."
      ],
      "metadata": {
        "id": "uQM0ZbTz_5Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J = (3)**2\n",
        "J_epsilon = (3 + 0.000000001)**2\n",
        "k = (J_epsilon - J)/0.000000001\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")"
      ],
      "metadata": {
        "id": "8saN6_5U_8BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value gets close to exactly 6 as we reduce the size of $\\epsilon$. Feel free to try reducing the value further."
      ],
      "metadata": {
        "id": "-Wn4wGslAD-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding symbolic derivatives\n",
        "In backprop it is useful to know the derivative of simple functions at any input value. Put another way, we would like to know the 'symbolic' derivative rather than the 'arithmetic' derivative. An example of a symbolic derivative is,  $\\frac{\\partial J(w)}{\\partial w} = 2 w$, the derivative of $J(w) = w^2$ above.  With the symbolic derivative you can find the value of the derivative at any input value $w$.  \n",
        "\n",
        "If you have taken a calculus course, you are familiar with the many [differentiation rules](https://en.wikipedia.org/wiki/Differentiation_rules#Power_laws,_polynomials,_quotients,_and_reciprocals) that mathematicians have developed to solve for a derivative given an expression. Well, it turns out this process has been automated with symbolic differentiation programs. An example of this in python is the [SymPy](https://www.sympy.org/en/index.html) library. Let's take a look at how to use this."
      ],
      "metadata": {
        "id": "IxMSSvViAV28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $J = w^2$\n",
        "Define the python variables and their symbolic names."
      ],
      "metadata": {
        "id": "sNPbbMDgAY5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J, w = symbols('J, w')"
      ],
      "metadata": {
        "id": "GNFuwH25AgAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define and print the expression. Note SymPy produces a latex string which generates a nicely readable equation."
      ],
      "metadata": {
        "id": "kE1YgFDdAjOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use SymPy's `diff` to differentiate the expression for $J$ with respect to $w$. Note the result matches our earlier example."
      ],
      "metadata": {
        "id": "SzoAWaOqAq4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "J=w**2\n",
        "J"
      ],
      "metadata": {
        "id": "aGNqNuoBAl_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw = diff(J,w)\n",
        "dJ_dw"
      ],
      "metadata": {
        "id": "zHXUjgr3AyaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the derivative at a few points by 'substituting' numeric values for the symbolic values. In the first example, $w$ is replaced by $2$."
      ],
      "metadata": {
        "id": "-Q-rJXmaAwpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw.subs([(w,2)])    # derivative at the point w = 2"
      ],
      "metadata": {
        "id": "DGg36QmFBAO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw.subs([(w,3)])    # derivative at the point w = 3"
      ],
      "metadata": {
        "id": "HCRwo5ClBB2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw.subs([(w,-3)])    # derivative at the point w = -3"
      ],
      "metadata": {
        "id": "sF-RAckWBIDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
        "    \n",
        "```python\n",
        "J= 1/w**2\n",
        "dJ_dw = diff(J,w)\n",
        "dJ_dw.subs([(w,4)])\n",
        "```\n",
        "  \n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "IZy8tOxEBu5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W2_Lab06_TF (Backprop)\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "TPrFTRGcCXk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Lab: Back propagation using a computation graph\n",
        "Working through this lab will give you insight into a key algorithm used by most machine learning frameworks. Gradient descent requires the derivative of the cost with respect to each parameter in the network.  Neural networks can have millions or even billions of parameters. The *back propagation* algorithm is used to compute those derivatives. *Computation graphs* are used to simplify the operation. Let's dig into this below."
      ],
      "metadata": {
        "id": "dikdGfBGCYNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import *\n",
        "import numpy as np\n",
        "import re\n",
        "#%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.widgets import TextBox\n",
        "from matplotlib.widgets import Button\n",
        "import ipywidgets as widgets\n",
        "#from lab_utils_backprop import *"
      ],
      "metadata": {
        "id": "S2lhkBcQCpaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computation Graph\n",
        "A computation graph simplifies the computation of complex derivatives by breaking them into smaller steps. Let's see how this works.\n",
        "\n",
        "Let's calculate the derivative of this slightly complex expression, $J = (2+3w)^2$. We would like to find the derivative of $J$ with respect to $w$ or $\\frac{\\partial J}{\\partial w}$."
      ],
      "metadata": {
        "id": "32AecCcEDBBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Propagation   \n",
        "Let's calculate the values in the forward direction.\n",
        "\n",
        ">Just a note about this section. It uses global variables and reuses them as the calculation progresses. If you run cells out of order, you may get funny results. If you do, go back to this point and run them in order."
      ],
      "metadata": {
        "id": "rG7AGPZUINUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = 3\n",
        "a = 2+3*w\n",
        "J = a**2\n",
        "print(f\"a = {a}, J = {J}\")"
      ],
      "metadata": {
        "id": "2BIDdOV3IP-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backprop\n",
        "<img align=\"left\" src=\"./images/C2_W2_BP_network0_j.PNG\"     style=\" width:100px; padding: 10px 20px; \" > Backprop is the algorithm we use to calculate derivatives. As described in the lectures, backprop starts at the right and moves to the left. The first node to consider is $J = a^2 $ and the first step is to find $\\frac{\\partial J}{\\partial a}$\n"
      ],
      "metadata": {
        "id": "QOesZqwgIVcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\frac{\\partial J}{\\partial a}$\n",
        "#### Arithmetically\n",
        "Find $\\frac{\\partial J}{\\partial a}$ by finding how $J$ changes as a result of a little change in $a$. This is described in detail in the derivatives optional lab."
      ],
      "metadata": {
        "id": "txOVSwUnIbqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_epsilon = a + 0.001       # a epsilon\n",
        "J_epsilon = a_epsilon**2    # J_epsilon\n",
        "k = (J_epsilon - J)/0.001   # difference divided by epsilon\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_da ~= k = {k} \")"
      ],
      "metadata": {
        "id": "OOphDG0pIhNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\frac{\\partial J}{\\partial a}$ is 22 which is $2\\times a$. Our result is not exactly $2 \\times a$ because our epsilon value is not infinitesimally small.\n",
        "#### Symbolically\n",
        "Now, let's use SymPy to calculate derivatives symbolically as we did in the derivatives optional lab. We will prefix the name of the variable with an 's' to indicate this is a *symbolic* variable."
      ],
      "metadata": {
        "id": "me3h0qhCIxjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sw,sJ,sa = symbols('w,J,a')\n",
        "sJ = sa**2\n",
        "sJ"
      ],
      "metadata": {
        "id": "cH0lLJj2Iwxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sJ.subs([(sa,a)])"
      ],
      "metadata": {
        "id": "EKCuxg_KI3Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_da = diff(sJ, sa)\n",
        "dJ_da"
      ],
      "metadata": {
        "id": "jI4ZULWZI5Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, $\\frac{\\partial J}{\\partial a} = 2a$. When $a=11$, $\\frac{\\partial J}{\\partial a} = 22$. This matches our arithmetic calculation above.\n",
        "If you have not already done so, you can go back to the diagram above and fill in the value for $\\frac{\\partial J}{\\partial a}$."
      ],
      "metadata": {
        "id": "pvqd3_vNJAOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\frac{\\partial J}{\\partial w}$\n",
        "<img align=\"left\" src=\"./images/C2_W2_BP_network0_a.PNG\"     style=\" width:100px; padding: 10px 20px; \" >  Moving from right to left, the next value we would like to compute is $\\frac{\\partial J}{\\partial w}$. To do this, we first need to calculate $\\frac{\\partial a}{\\partial w}$ which describes how the output of this node, $a=2+3w$, changes when the input $w$ changes a little bit."
      ],
      "metadata": {
        "id": "flGxQBulJExW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Arithmetically\n",
        "Find $\\frac{\\partial a}{\\partial w}$ by finding how $a$ changes as a result of a little change in $w$."
      ],
      "metadata": {
        "id": "06E5SRxkJ4bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_epsilon = w + 0.001       # a  plus a small value, epsilon\n",
        "a_epsilon = 2 + 3*w_epsilon\n",
        "k = (a_epsilon - a)/0.001   # difference divided by epsilon\n",
        "print(f\"a = {a}, a_epsilon = {a_epsilon}, da_dw ~= k = {k} \")"
      ],
      "metadata": {
        "id": "gSNoQXohJ6q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculated arithmetically,  $\\frac{\\partial a}{\\partial w} \\approx 3$. Let's try it with SymPy."
      ],
      "metadata": {
        "id": "XUEBp1cKJ-Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sa = 2 + 3*sw\n",
        "sa"
      ],
      "metadata": {
        "id": "JWbpZkbpKBMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "da_dw = diff(sa,sw)\n",
        "da_dw"
      ],
      "metadata": {
        "id": "TZ2jbxtVKDHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">The next step is the interesting part:\n",
        "> - We know that a small change in $w$ will cause $a$ to change by 3 times that amount.\n",
        "> - We know that a small change in $a$ will cause $J$ to change by $2\\times a$ times that amount. (a=11 in this example)    \n",
        " so, putting these together,\n",
        "> - We  know that a small change in $w$ will cause $J$ to change by $3 \\times 2\\times a$ times that amount.\n",
        ">\n",
        "> These cascading changes go by the name of *the chain rule*.  It can be written like this:\n",
        " $$\\frac{\\partial J}{\\partial w} = \\frac{\\partial a}{\\partial w} \\frac{\\partial J}{\\partial a} $$\n",
        "\n",
        "It's worth spending some time thinking this through if it is not clear. This is a key take-away.\n",
        "\n",
        " Let's try calculating it:"
      ],
      "metadata": {
        "id": "CjUGgM1BJwyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dJ_dw = da_dw * dJ_da\n",
        "dJ_dw"
      ],
      "metadata": {
        "id": "j5HCh5vbKQ5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And  a  is 11 in this example so  âJâw=66 . We can check this arithmetically:"
      ],
      "metadata": {
        "id": "nTeFFa3EKbab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_epsilon = w + 0.001\n",
        "a_epsilon = 2 + 3*w_epsilon\n",
        "J_epsilon = a_epsilon**2\n",
        "k = (J_epsilon - J)/0.001   # difference divided by epsilon\n",
        "print(f\"J = {J}, J_epsilon = {J_epsilon}, dJ_dw ~= k = {k} \")"
      ],
      "metadata": {
        "id": "j5mNGPczKXZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK! You can now fill the values for  $\\frac{\\partial a}{\\partial w}$ and $\\frac{\\partial J}{\\partial w}$ in  the diagram if you have not already done so."
      ],
      "metadata": {
        "id": "clRDtK6ZKfpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R2lQ0J7ukivP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C2_W3_Lab07 (Model_Evaluation_and_Selection)\n",
        "\n",
        "Quantifying a learning algorithm's performance and comparing different models are some of the common tasks when applying machine learning to real world applications. In this lab, you will practice doing these using the tips shared in class. Specifically, you will:\n",
        "\n",
        "* split datasets into training, cross validation, and test sets\n",
        "* evaluate regression and classification models\n",
        "* add polynomial features to improve the performance of a linear regression model\n",
        "* compare several neural network architectures\n"
      ],
      "metadata": {
        "id": "STSmhw4qkyD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Lab Setup\n",
        "\n",
        "First, you will import the packages needed for the tasks in this lab. We also included some commands to make the outputs later more readable by reducing verbosity and suppressing non-critical warnings."
      ],
      "metadata": {
        "id": "rvH07KwNeQ7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Lab Setup"
      ],
      "metadata": {
        "id": "gcKqtN38lc9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for array computations and loading data\n",
        "import numpy as np\n",
        "\n",
        "# for building linear regression models and preparing data\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# for building and training neural networks\n",
        "import tensorflow as tf\n",
        "\n",
        "# custom functions\n",
        "#import utils\n",
        "\n",
        "# reduce display precision on numpy arrays\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# suppress warnings\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.autograph.set_verbosity(0)"
      ],
      "metadata": {
        "id": "9gfIxFCHkxNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression\n",
        "\n",
        "First, you will be tasked to develop a model for a regression problem. You are given the dataset below consisting of 50 examples of an input feature `x` and its corresponding target `y`."
      ],
      "metadata": {
        "id": "OpY81Qozmhf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Load the dataset from the text file\n",
        "data = np.loadtxt('./Colab Notebooks/data/data_w3_ex1.csv', delimiter=',')\n",
        "\n",
        "# Split the inputs and outputs into separate arrays\n",
        "x = data[:,0]\n",
        "y = data[:,1]\n",
        "\n",
        "# Convert 1-D arrays into 2-D because the commands later will require it\n",
        "x = np.expand_dims(x, axis=1)\n",
        "y = np.expand_dims(y, axis=1)\n",
        "\n",
        "print(f\"the shape of the inputs x is: {x.shape}\")\n",
        "print(f\"the shape of the targets y is: {y.shape}\")"
      ],
      "metadata": {
        "id": "jfl0vDbvmyA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the functions we use above:"
      ],
      "metadata": {
        "id": "bG-7WxiMl8hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "#plt.style.use('./deeplearning.mplstyle')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def plot_dataset(x, y, title):\n",
        "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
        "    plt.rcParams[\"lines.markersize\"] = 12\n",
        "    plt.scatter(x, y, marker='x', c='r');\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "    plt.show()\n",
        "\n",
        "def plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title):\n",
        "    plt.scatter(x_train, y_train, marker='x', c='r', label='training');\n",
        "    plt.scatter(x_cv, y_cv, marker='o', c='b', label='cross validation');\n",
        "    plt.scatter(x_test, y_test, marker='^', c='g', label='test');\n",
        "    plt.title(\"input vs. target\")\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_train_cv_mses(degrees, train_mses, cv_mses, title):\n",
        "    degrees = range(1,11)\n",
        "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_bc_dataset(x, y, title):\n",
        "    for i in range(len(y)):\n",
        "        marker = 'x' if y[i] == 1 else 'o'\n",
        "        c = 'r' if y[i] == 1 else 'b'\n",
        "        plt.scatter(x[i,0], x[i,1], marker=marker, c=c);\n",
        "    plt.title(\"x1 vs x2\")\n",
        "    plt.xlabel(\"x1\");\n",
        "    plt.ylabel(\"x2\");\n",
        "    y_0 = mlines.Line2D([], [], color='r', marker='x', markersize=12, linestyle='None', label='y=1')\n",
        "    y_1 = mlines.Line2D([], [], color='b', marker='o', markersize=12, linestyle='None', label='y=0')\n",
        "    plt.title(title)\n",
        "    plt.legend(handles=[y_0, y_1])\n",
        "    plt.show()\n",
        "\n",
        "def build_models():\n",
        "\n",
        "    tf.random.set_seed(20)\n",
        "\n",
        "    model_1 = Sequential(\n",
        "        [\n",
        "            Dense(25, activation = 'relu'),\n",
        "            Dense(15, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_1'\n",
        "    )\n",
        "\n",
        "    model_2 = Sequential(\n",
        "        [\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_2'\n",
        "    )\n",
        "\n",
        "    model_3 = Sequential(\n",
        "        [\n",
        "            Dense(32, activation = 'relu'),\n",
        "            Dense(16, activation = 'relu'),\n",
        "            Dense(8, activation = 'relu'),\n",
        "            Dense(4, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(1, activation = 'linear')\n",
        "        ],\n",
        "        name='model_3'\n",
        "    )\n",
        "\n",
        "    model_list = [model_1, model_2, model_3]\n",
        "\n",
        "    return model_list\n",
        "\n",
        "\n",
        "# Not used in the lab. You can call this for the binary\n",
        "# classification problem if you set `from_logits=False`\n",
        "# when declaring the loss. With this, you will also not need\n",
        "# to call the `tf.math.sigmoid()` function in the loop\n",
        "# because the model output is already a probability\n",
        "def build_bc_models():\n",
        "\n",
        "    tf.random.set_seed(20)\n",
        "\n",
        "    model_1_bc = Sequential(\n",
        "        [\n",
        "            Dense(25, activation = 'relu'),\n",
        "            Dense(15, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_1_bc'\n",
        "    )\n",
        "\n",
        "    model_2_bc = Sequential(\n",
        "        [\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(20, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_2_bc'\n",
        "    )\n",
        "\n",
        "    model_3_bc = Sequential(\n",
        "        [\n",
        "            Dense(32, activation = 'relu'),\n",
        "            Dense(16, activation = 'relu'),\n",
        "            Dense(8, activation = 'relu'),\n",
        "            Dense(4, activation = 'relu'),\n",
        "            Dense(12, activation = 'relu'),\n",
        "            Dense(1, activation = 'sigmoid')\n",
        "        ],\n",
        "        name='model_3_bc'\n",
        "    )\n",
        "\n",
        "    models_bc = [model_1_bc, model_2_bc, model_3_bc]\n",
        "\n",
        "    return models_bc\n",
        "\n",
        "\n",
        "def prepare_dataset(filename):\n",
        "\n",
        "    data = np.loadtxt(filename, delimiter=\",\")\n",
        "\n",
        "    x = data[:,:-1]\n",
        "    y = data[:,-1]\n",
        "\n",
        "    # Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
        "    x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=80)\n",
        "\n",
        "    # Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
        "    x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=80)\n",
        "\n",
        "    return x_train, y_train, x_cv, y_cv, x_test, y_test\n",
        "\n",
        "def train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "    degrees = range(1,max_degree+1)\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for degree in degrees:\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model.fit(X_train_mapped_scaled, y_train )\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv)\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
        "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
        "    plt.xticks(degrees)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for reg_param in reg_params:\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model = Ridge(alpha=reg_param)\n",
        "        model.fit(X_train_mapped_scaled, y_train)\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv)\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    reg_params = [str(x) for x in reg_params]\n",
        "    plt.plot(reg_params, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(reg_params, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(reg_params, np.repeat(baseline, len(reg_params)), linestyle='--', label='baseline')\n",
        "    plt.title(\"lambda vs. train and CV MSEs\")\n",
        "    plt.xlabel(\"lambda\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_diff_datasets(model, files, max_degree=10, baseline=None):\n",
        "\n",
        "    for file in files:\n",
        "\n",
        "        x_train, y_train, x_cv, y_cv, x_test, y_test = prepare_dataset(file['filename'])\n",
        "\n",
        "        train_mses = []\n",
        "        cv_mses = []\n",
        "        models = []\n",
        "        scalers = []\n",
        "        degrees = range(1,max_degree+1)\n",
        "\n",
        "        # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "        for degree in degrees:\n",
        "\n",
        "            # Add polynomial features to the training set\n",
        "            poly = PolynomialFeatures(degree, include_bias=False)\n",
        "            X_train_mapped = poly.fit_transform(x_train)\n",
        "\n",
        "            # Scale the training set\n",
        "            scaler_poly = StandardScaler()\n",
        "            X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "            scalers.append(scaler_poly)\n",
        "\n",
        "            # Create and train the model\n",
        "            model.fit(X_train_mapped_scaled, y_train )\n",
        "            models.append(model)\n",
        "\n",
        "            # Compute the training MSE\n",
        "            yhat = model.predict(X_train_mapped_scaled)\n",
        "            train_mse = mean_squared_error(y_train, yhat) / 2\n",
        "            train_mses.append(train_mse)\n",
        "\n",
        "            # Add polynomial features and scale the cross-validation set\n",
        "            poly = PolynomialFeatures(degree, include_bias=False)\n",
        "            X_cv_mapped = poly.fit_transform(x_cv)\n",
        "            X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "            # Compute the cross-validation MSE\n",
        "            yhat = model.predict(X_cv_mapped_scaled)\n",
        "            cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
        "            cv_mses.append(cv_mse)\n",
        "\n",
        "        # Plot the results\n",
        "        plt.plot(degrees, train_mses, marker='o', c='r', linestyle=file['linestyle'], label=f\"{file['label']} training MSEs\");\n",
        "        plt.plot(degrees, cv_mses, marker='o', c='b', linestyle=file['linestyle'], label=f\"{file['label']} CV MSEs\");\n",
        "\n",
        "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
        "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
        "    plt.xticks(degrees)\n",
        "    plt.xlabel(\"degree\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def train_plot_learning_curve(model, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
        "\n",
        "    train_mses = []\n",
        "    cv_mses = []\n",
        "    models = []\n",
        "    scalers = []\n",
        "    num_samples_train_and_cv = []\n",
        "    percents = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
        "    for percent in percents:\n",
        "\n",
        "        num_samples_train = round(len(x_train) * (percent/100.0))\n",
        "        num_samples_cv = round(len(x_cv) * (percent/100.0))\n",
        "        num_samples_train_and_cv.append(num_samples_train + num_samples_cv)\n",
        "\n",
        "        x_train_sub = x_train[:num_samples_train]\n",
        "        y_train_sub = y_train[:num_samples_train]\n",
        "        x_cv_sub = x_cv[:num_samples_cv]\n",
        "        y_cv_sub = y_cv[:num_samples_cv]\n",
        "\n",
        "        # Add polynomial features to the training set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_train_mapped = poly.fit_transform(x_train_sub)\n",
        "\n",
        "        # Scale the training set\n",
        "        scaler_poly = StandardScaler()\n",
        "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
        "        scalers.append(scaler_poly)\n",
        "\n",
        "        # Create and train the model\n",
        "        model.fit(X_train_mapped_scaled, y_train_sub)\n",
        "        models.append(model)\n",
        "\n",
        "        # Compute the training MSE\n",
        "        yhat = model.predict(X_train_mapped_scaled)\n",
        "        train_mse = mean_squared_error(y_train_sub, yhat) / 2\n",
        "        train_mses.append(train_mse)\n",
        "\n",
        "        # Add polynomial features and scale the cross-validation set\n",
        "        poly = PolynomialFeatures(degree, include_bias=False)\n",
        "        X_cv_mapped = poly.fit_transform(x_cv_sub)\n",
        "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
        "\n",
        "        # Compute the cross-validation MSE\n",
        "        yhat = model.predict(X_cv_mapped_scaled)\n",
        "        cv_mse = mean_squared_error(y_cv_sub, yhat) / 2\n",
        "        cv_mses.append(cv_mse)\n",
        "\n",
        "    # Plot the results\n",
        "    plt.plot(num_samples_train_and_cv, train_mses, marker='o', c='r', label='training MSEs');\n",
        "    plt.plot(num_samples_train_and_cv, cv_mses, marker='o', c='b', label='CV MSEs');\n",
        "    plt.plot(num_samples_train_and_cv, np.repeat(baseline, len(percents)), linestyle='--', label='baseline')\n",
        "    plt.title(\"number of examples vs. train and CV MSEs\")\n",
        "    plt.xlabel(\"total number of training and cv examples\");\n",
        "    plt.ylabel(\"MSE\");\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6nFAV60Nl9Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "h-0aj0uuk7TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.array([[1,2]])\n",
        "Y = np.array([[2,3]]).T\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print('*'*10)\n",
        "Z = np.ones([1,2])\n",
        "print(Z*X)\n",
        "print(Z*Y)\n",
        "print(Z@Y)\n",
        "np.dot(Z,Y)\n"
      ],
      "metadata": {
        "id": "gbodwHvs83T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1,2]])\n",
        "w = np.array([[1,2,3], [4,5,6]])\n",
        "b = np.array([1,2,3])\n",
        "x @ w + b    # np.matmul(x,w)+b does the same"
      ],
      "metadata": {
        "id": "BrXkhyOqlFr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Implementation of matplotlib function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "dx, dy = 0.015, 0.05\n",
        "y, x = np.mgrid[slice(-4, 4 + dy, dy),\n",
        "                slice(-4, 4 + dx, dx)]\n",
        "z = (1 - x / 3. + x ** 5 + y ** 5) * np.exp(-x ** 2 - y ** 2)\n",
        "z = z[:-1, :-1]\n",
        "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
        "\n",
        "plt.imshow(y, cmap ='Reds')\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XA5QKjFcm3UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'a':list(np.random.randint(0,10,100))})\n",
        "arr = df.to_numpy()\n",
        "arr.reshape((5,20))"
      ],
      "metadata": {
        "id": "JzYBwDk-gaqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'a':list(np.random.randint(0,255,4))})\n",
        "arr = df.to_numpy()\n",
        "arr = arr.reshape((2,2))\n",
        "plt.imshow(arr, cmap = 'Greys')"
      ],
      "metadata": {
        "id": "zvvkdy9xcbYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(np.random.randint(0,255,400)/100)"
      ],
      "metadata": {
        "id": "6JwBsfvPBBKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "givenString = \"TutorialsPoint\"\n",
        "my_list = []\n",
        "your_list = []\n",
        "\n",
        "for i in range(0, 20):\n",
        "  my_list.append(random.randint(0, 9))\n",
        "\n",
        "print(my_list)\n",
        "\n",
        "for item in my_list:\n",
        "  if item == 0 or item == 1:\n",
        "    print(\"yes\")\n",
        "  elif item == 2 or item == 3:\n",
        "    print(\"No\")\n",
        "  else:\n",
        "    print(\"NA\")"
      ],
      "metadata": {
        "id": "hNm2VHC2IN8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# input string\n",
        "givenString = \"ABCDEFGHIJ\"\n",
        "\n",
        "# generating 3 random items from the String using random.sample() method\n",
        "randomItems = random.sample(givenString, 10)\n",
        "print(randomItems)\n",
        "\n",
        "for item in randomItems:\n",
        "  if item == 'A' or item == 'B':\n",
        "    print(\"yes\")\n",
        "  elif item == 'C' or item == 'D':\n",
        "    print(\"No\")\n",
        "  else:\n",
        "    print(\"NA\")"
      ],
      "metadata": {
        "id": "8z5hWXHEPrxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saple_id = {'a1b':1, 'b1b':2, 'c1b':3}\n",
        "saple_id\n",
        "\n",
        "my_list_2 = [1, 2, 3, 4, 3, 2, 1]\n",
        "\n",
        "my_list_1 = ['A', 'B', 'C']\n",
        "\n",
        "unique(my_list_2)"
      ],
      "metadata": {
        "id": "HYFzOP7Md5Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "a = np.array(['a', 'b', 'c', 'a'])\n",
        "\n",
        "\n",
        "print(a == 'a')\n",
        "\n",
        "a[a == 'a'] = 'AAA'\n",
        "\n",
        "a\n",
        "\n"
      ],
      "metadata": {
        "id": "gdXLKuz4jvY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}